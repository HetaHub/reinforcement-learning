{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc62ddc",
   "metadata": {},
   "source": [
    "- ptan.agen.BaseAgent is the conceptual class, which is basic class. It receives batch observation and return the batch action. Batch action will have a higher efficiency than processing each action at once.\n",
    "- ptan.agent.DQNAgent transfer the batch observation value to action value, it need to fill in the action selector as the 2nd parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ee2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589363f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(env.observation_space.shape[0], 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, env.action_space.n)\n",
    ")\n",
    "\n",
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1)\n",
    "agent = ptan.agent.DQNAgent(net, action_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9feee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1], dtype=int64), [None])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we send the observation to the agent and ask what action we should perform\n",
    "#first result is the action and 2nd can be ignored\n",
    "obs = np.array([env.reset()], dtype=np.float32)\n",
    "agent(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a072ac",
   "metadata": {},
   "source": [
    "ptan.experience.ExperienceSourceFirstLast accept environment and agent with the experience stream, it will auto process the end episode situation, when ended, it will set the last element as None and reset the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3932eafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object ExperienceSourceFirstLast.__iter__ at 0x000001C4FD780F10>\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=1)\n",
    "it = iter(exp_source)\n",
    "print(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aab88f",
   "metadata": {},
   "source": [
    "ptan.common.wrappers.wrap_dqn(env) is modified from OpenAI baselines wrapper github.com/openai/baselines , it wrap up the environment like previous chapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
