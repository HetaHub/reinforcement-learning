{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06b99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from lib import dqn_model, common_dqn_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796c66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These 2 tags used to activate screenshot saving for debug, but it will lower the training speed\n",
    "#For every 10000 frames we activate the first tag to save the first 200 states in the buffer to get the distribution\n",
    "#2nd tag to save non-zero reward or last episode distribution projection, this is used for debug and visualize.\n",
    "SAVE_STATES_IMG = False\n",
    "SAVE_TRANSITIONS_IMG = False\n",
    "\n",
    "#we switch matplotlib as headless mode. Headless mode: when ploting is not neded, code has debug tag for saving probability\n",
    "#distribution, so we can use visual method for debug and training.\n",
    "if SAVE_STATES_IMG or SAVE_TRANSITIONS_IMG:\n",
    "    import matplotlib as mpl\n",
    "    mpl.use(\"Agg\")\n",
    "    import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "Vmax = 10\n",
    "Vmin = -10\n",
    "#number of atoms\n",
    "N_ATOMS = 51\n",
    "#each atom width\n",
    "DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)\n",
    "\n",
    "#The number of states to keep in buffer. This is to calculate mean value and the frequency to update the mean value.\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_FRAME = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e7ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn construction function has a n_actions * n_atoms array as output, it contains each action probabilities, and batch \n",
    "#dimension, therefore the output will be 3d.\n",
    "class DistributionalDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DistributionalDQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions * N_ATOMS)\n",
    "        )\n",
    "        #we use atom value to register torch tensor to use it later.\n",
    "        self.register_buffer(\"supports\", torch.arange(Vmin, Vmax + DELTA_Z, DELTA_Z))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    #forward() function basically the same, just the output need to fit the output shape.\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(batch_size, -1)\n",
    "        fc_out = self.fc(conv_out)\n",
    "        return fc_out.view(batch_size, -1, N_ATOMS)\n",
    "    \n",
    "    #both() will return the original distribution and q value. Q-value will be used to choose the action\n",
    "    #although using distribution can let us have different action to choose, but if we use greedy method to the q-value\n",
    "    #it will be same as the basic dqn version\n",
    "    #to get q-value, we calculate the sum of normal distribution and atom value, \n",
    "    #result is the expectation of the distribution\n",
    "    def both(self, x):\n",
    "        cat_out = self(x)\n",
    "        probs = self.apply_softmax(cat_out)\n",
    "        weights = probs * self.supports\n",
    "        res = weights.sum(dim=2)\n",
    "        return cat_out, res\n",
    "    \n",
    "    #tool functions, qvals() to calculate q value, 2nd apply softmax to the output tensor, to let it have proper shape.\n",
    "    def qvals(self, x):\n",
    "        return self.both(x)[1]\n",
    "    \n",
    "    def apply_softmax(self, t):\n",
    "        return self.softmax(t.view(-1, N_ATOMS)).view(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfcb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_values_of_states(states, net, device=\"cpu\"):\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_values_v = net.qvals(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return np.mean(mean_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f1a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state_images(frame_idx, states, net, device=\"cpu\", max_states=200):\n",
    "    ofs = 0\n",
    "    p = np.arange(Vmin, Vmax + DELTA_Z, DELTA_Z)\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_prob = net.apply_softmax(net(states_v)).data.cpu().numpy()\n",
    "        batch_size, num_actions, _ = action_prob.shape\n",
    "        for batch_idx in range(batch_size):\n",
    "            plt.clf()\n",
    "            for action_idx in range(num_actions):\n",
    "                plt.subplot(num_actions, 1, action_idx+1)\n",
    "                plt.bar(p, action_prob[batch_idx, action_idx], width=0.5)\n",
    "            plt.savefig(\"states/%05d_%08d.png\" % (ofs + batch_idx, frame_idx))\n",
    "        ofs += batch_size\n",
    "        if ofs >= max_states:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8e7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transition_images(batch_size, predicted, projected, next_distr, dones, rewards, save_prefix):\n",
    "    for batch_idx in range(batch_size):\n",
    "        is_done = dones[batch_idx]\n",
    "        reward = rewards[batch_idx]\n",
    "        plt.clf()\n",
    "        p = np.arange(Vmin, Vmax + DELTA_Z, DELTA_Z)\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.bar(p, predicted[batch_idx], width=0.5)\n",
    "        plt.title(\"Predicted\")\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.bar(p, projected[batch_idx], width=0.5)\n",
    "        plt.title(\"Projected\")\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.bar(p, next_distr[batch_idx], width=0.5)\n",
    "        plt.title(\"Next state\")\n",
    "        suffix = \"\"\n",
    "        if reward != 0.0:\n",
    "            suffix = suffix + \"_%.0f\" % reward\n",
    "        if is_done:\n",
    "            suffix = suffix + \"_done\"\n",
    "        plt.savefig(\"%s_%02d%s.png\" % (save_prefix, batch_idx, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6553a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\", save_prefix=None):\n",
    "    #we unzip batch and change it to tensor\n",
    "    states, actions, rewards, dones, next_states = common_dqn_distrib.unpack_batch(batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    \n",
    "    #we get next state distribution and q values, so we call both() and get the best action, then we apply softmax to\n",
    "    #the distribution to transform it as array\n",
    "    next_distr_v, next_qvals_v = tgt_net.both(next_states_v)\n",
    "    next_actions = next_qvals_v.max(1)[1].data.cpu().numpy()\n",
    "    next_distr = tgt_net.apply_softmax(next_distr_v).data.cpu().numpy()\n",
    "    \n",
    "    #then we use Bellman equation to extract best action distribution and activate the projection\n",
    "    #the result is how we hope the output of the target distribution will look like\n",
    "    next_best_distr = next_distr[range(batch_size), next_actions]\n",
    "    dones = dones.astype(np.bool)\n",
    "    proj_distr = common_dqn_distrib.distr_projection(next_best_distr, rewards, dones, Vmin, Vmax, N_ATOMS, gamma)\n",
    "    \n",
    "    #we calculate network output, also the loss from projection output and the network output from the chosen action,\n",
    "    #this is the Kullback Leibler divergence. We use log_softmax function to calculate log function\n",
    "    distr_v = net(states_v)\n",
    "    state_action_values = distr_v[range(batch_size), actions_v.data]\n",
    "    state_log_sm_v = F.log_softmax(state_action_values, dim=1)\n",
    "    proj_distr_v = torch.tensor(proj_distr).to(device)\n",
    "    \n",
    "    if save_prefix is not None:\n",
    "        pred = F.softmax(state_action_values, dim=1).data.cpu().numpy()\n",
    "        save_transition_images(batch_size, pred, proj_distr, next_best_distr, dones, rewards, save_prefix)\n",
    "        \n",
    "        \n",
    "    loss_v = -state_log_sm_v * proj_distr_v\n",
    "    return loss_v.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ad9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918: done 1 games, mean reward -20.000, speed 202.35 f/s, eps 0.99\n",
      "1678: done 2 games, mean reward -20.500, speed 232.78 f/s, eps 0.98\n",
      "2557: done 3 games, mean reward -20.667, speed 224.73 f/s, eps 0.97\n",
      "3437: done 4 games, mean reward -20.750, speed 186.29 f/s, eps 0.97\n",
      "4453: done 5 games, mean reward -20.600, speed 166.15 f/s, eps 0.96\n",
      "5239: done 6 games, mean reward -20.667, speed 159.58 f/s, eps 0.95\n",
      "6298: done 7 games, mean reward -20.714, speed 158.13 f/s, eps 0.94\n",
      "7331: done 8 games, mean reward -20.625, speed 160.45 f/s, eps 0.93\n",
      "8508: done 9 games, mean reward -20.111, speed 156.74 f/s, eps 0.91\n",
      "9353: done 10 games, mean reward -20.200, speed 159.89 f/s, eps 0.91\n",
      "10238: done 11 games, mean reward -20.273, speed 90.77 f/s, eps 0.90\n",
      "10995: done 12 games, mean reward -20.333, speed 47.33 f/s, eps 0.89\n",
      "12107: done 13 games, mean reward -20.308, speed 42.34 f/s, eps 0.88\n",
      "13016: done 14 games, mean reward -20.357, speed 43.96 f/s, eps 0.87\n",
      "13900: done 15 games, mean reward -20.400, speed 45.02 f/s, eps 0.86\n",
      "15069: done 16 games, mean reward -20.375, speed 43.69 f/s, eps 0.85\n",
      "15917: done 17 games, mean reward -20.412, speed 44.23 f/s, eps 0.84\n",
      "16910: done 18 games, mean reward -20.333, speed 42.72 f/s, eps 0.83\n",
      "17733: done 19 games, mean reward -20.368, speed 44.36 f/s, eps 0.82\n",
      "18638: done 20 games, mean reward -20.400, speed 44.53 f/s, eps 0.81\n",
      "19612: done 21 games, mean reward -20.381, speed 44.37 f/s, eps 0.80\n",
      "20394: done 22 games, mean reward -20.409, speed 42.26 f/s, eps 0.80\n",
      "21262: done 23 games, mean reward -20.391, speed 42.75 f/s, eps 0.79\n",
      "22172: done 24 games, mean reward -20.417, speed 44.62 f/s, eps 0.78\n",
      "23248: done 25 games, mean reward -20.400, speed 44.36 f/s, eps 0.77\n",
      "24086: done 26 games, mean reward -20.385, speed 44.87 f/s, eps 0.76\n",
      "25061: done 27 games, mean reward -20.333, speed 44.32 f/s, eps 0.75\n",
      "26151: done 28 games, mean reward -20.357, speed 44.99 f/s, eps 0.74\n",
      "27239: done 29 games, mean reward -20.276, speed 44.81 f/s, eps 0.73\n",
      "28209: done 30 games, mean reward -20.233, speed 44.95 f/s, eps 0.72\n",
      "29122: done 31 games, mean reward -20.258, speed 44.82 f/s, eps 0.71\n",
      "29900: done 32 games, mean reward -20.281, speed 43.85 f/s, eps 0.70\n",
      "30753: done 33 games, mean reward -20.303, speed 44.20 f/s, eps 0.69\n",
      "31582: done 34 games, mean reward -20.294, speed 43.84 f/s, eps 0.68\n",
      "32432: done 35 games, mean reward -20.314, speed 43.63 f/s, eps 0.68\n",
      "33352: done 36 games, mean reward -20.306, speed 44.64 f/s, eps 0.67\n",
      "34262: done 37 games, mean reward -20.324, speed 44.17 f/s, eps 0.66\n",
      "35227: done 38 games, mean reward -20.342, speed 43.70 f/s, eps 0.65\n",
      "36216: done 39 games, mean reward -20.333, speed 44.12 f/s, eps 0.64\n",
      "37113: done 40 games, mean reward -20.325, speed 43.90 f/s, eps 0.63\n",
      "37890: done 41 games, mean reward -20.341, speed 44.17 f/s, eps 0.62\n",
      "38781: done 42 games, mean reward -20.333, speed 43.46 f/s, eps 0.61\n",
      "39617: done 43 games, mean reward -20.326, speed 44.07 f/s, eps 0.60\n",
      "40506: done 44 games, mean reward -20.318, speed 44.15 f/s, eps 0.59\n",
      "41506: done 45 games, mean reward -20.333, speed 44.37 f/s, eps 0.58\n",
      "42460: done 46 games, mean reward -20.326, speed 43.84 f/s, eps 0.58\n",
      "43527: done 47 games, mean reward -20.298, speed 43.61 f/s, eps 0.56\n",
      "44403: done 48 games, mean reward -20.312, speed 43.86 f/s, eps 0.56\n",
      "45221: done 49 games, mean reward -20.327, speed 44.24 f/s, eps 0.55\n",
      "46136: done 50 games, mean reward -20.320, speed 44.12 f/s, eps 0.54\n",
      "46974: done 51 games, mean reward -20.333, speed 43.68 f/s, eps 0.53\n",
      "47957: done 52 games, mean reward -20.346, speed 44.04 f/s, eps 0.52\n",
      "48780: done 53 games, mean reward -20.358, speed 41.88 f/s, eps 0.51\n",
      "49720: done 54 games, mean reward -20.370, speed 43.18 f/s, eps 0.50\n",
      "50819: done 55 games, mean reward -20.364, speed 44.05 f/s, eps 0.49\n",
      "51919: done 56 games, mean reward -20.357, speed 44.43 f/s, eps 0.48\n",
      "52766: done 57 games, mean reward -20.368, speed 44.37 f/s, eps 0.47\n",
      "53528: done 58 games, mean reward -20.379, speed 44.33 f/s, eps 0.46\n",
      "54781: done 59 games, mean reward -20.339, speed 44.59 f/s, eps 0.45\n",
      "55696: done 60 games, mean reward -20.333, speed 44.29 f/s, eps 0.44\n",
      "56935: done 61 games, mean reward -20.311, speed 43.92 f/s, eps 0.43\n",
      "57772: done 62 games, mean reward -20.306, speed 44.33 f/s, eps 0.42\n",
      "58950: done 63 games, mean reward -20.286, speed 43.84 f/s, eps 0.41\n",
      "59786: done 64 games, mean reward -20.281, speed 43.48 f/s, eps 0.40\n",
      "60827: done 65 games, mean reward -20.292, speed 44.14 f/s, eps 0.39\n",
      "61615: done 66 games, mean reward -20.303, speed 43.93 f/s, eps 0.38\n",
      "62525: done 67 games, mean reward -20.299, speed 44.60 f/s, eps 0.37\n",
      "63582: done 68 games, mean reward -20.309, speed 44.01 f/s, eps 0.36\n",
      "64678: done 69 games, mean reward -20.319, speed 44.13 f/s, eps 0.35\n",
      "65834: done 70 games, mean reward -20.314, speed 44.08 f/s, eps 0.34\n",
      "66857: done 71 games, mean reward -20.310, speed 44.07 f/s, eps 0.33\n",
      "67705: done 72 games, mean reward -20.319, speed 44.16 f/s, eps 0.32\n",
      "68894: done 73 games, mean reward -20.301, speed 43.92 f/s, eps 0.31\n",
      "69999: done 74 games, mean reward -20.297, speed 44.04 f/s, eps 0.30\n",
      "71004: done 75 games, mean reward -20.307, speed 44.17 f/s, eps 0.29\n",
      "72414: done 76 games, mean reward -20.316, speed 44.22 f/s, eps 0.28\n",
      "73770: done 77 games, mean reward -20.286, speed 44.25 f/s, eps 0.26\n",
      "74727: done 78 games, mean reward -20.269, speed 43.93 f/s, eps 0.25\n",
      "75801: done 79 games, mean reward -20.266, speed 44.06 f/s, eps 0.24\n",
      "76965: done 80 games, mean reward -20.250, speed 44.46 f/s, eps 0.23\n",
      "78125: done 81 games, mean reward -20.247, speed 44.15 f/s, eps 0.22\n",
      "79356: done 82 games, mean reward -20.256, speed 44.09 f/s, eps 0.21\n",
      "80421: done 83 games, mean reward -20.265, speed 43.68 f/s, eps 0.20\n",
      "82014: done 84 games, mean reward -20.226, speed 44.20 f/s, eps 0.18\n",
      "83328: done 85 games, mean reward -20.224, speed 44.18 f/s, eps 0.17\n",
      "84996: done 86 games, mean reward -20.221, speed 43.50 f/s, eps 0.15\n",
      "86414: done 87 games, mean reward -20.207, speed 43.98 f/s, eps 0.14\n",
      "87615: done 88 games, mean reward -20.205, speed 44.01 f/s, eps 0.12\n",
      "88951: done 89 games, mean reward -20.180, speed 44.13 f/s, eps 0.11\n",
      "90489: done 90 games, mean reward -20.144, speed 43.94 f/s, eps 0.10\n",
      "92252: done 91 games, mean reward -20.143, speed 44.10 f/s, eps 0.08\n",
      "93663: done 92 games, mean reward -20.130, speed 44.08 f/s, eps 0.06\n",
      "95306: done 93 games, mean reward -20.097, speed 43.93 f/s, eps 0.05\n",
      "96726: done 94 games, mean reward -20.106, speed 43.47 f/s, eps 0.03\n",
      "98794: done 95 games, mean reward -20.053, speed 43.72 f/s, eps 0.02\n",
      "100307: done 96 games, mean reward -20.042, speed 44.03 f/s, eps 0.02\n",
      "102136: done 97 games, mean reward -20.021, speed 43.74 f/s, eps 0.02\n",
      "104134: done 98 games, mean reward -20.000, speed 44.01 f/s, eps 0.02\n",
      "105550: done 99 games, mean reward -20.000, speed 43.69 f/s, eps 0.02\n",
      "107227: done 100 games, mean reward -19.980, speed 43.22 f/s, eps 0.02\n",
      "108901: done 101 games, mean reward -19.980, speed 43.80 f/s, eps 0.02\n",
      "110463: done 102 games, mean reward -19.960, speed 44.05 f/s, eps 0.02\n",
      "112137: done 103 games, mean reward -19.940, speed 44.25 f/s, eps 0.02\n",
      "113855: done 104 games, mean reward -19.920, speed 43.88 f/s, eps 0.02\n",
      "115925: done 105 games, mean reward -19.900, speed 44.03 f/s, eps 0.02\n",
      "117677: done 106 games, mean reward -19.860, speed 43.98 f/s, eps 0.02\n",
      "119309: done 107 games, mean reward -19.860, speed 43.66 f/s, eps 0.02\n",
      "121113: done 108 games, mean reward -19.850, speed 43.54 f/s, eps 0.02\n",
      "122728: done 109 games, mean reward -19.890, speed 43.59 f/s, eps 0.02\n",
      "125045: done 110 games, mean reward -19.810, speed 43.81 f/s, eps 0.02\n",
      "126831: done 111 games, mean reward -19.780, speed 44.12 f/s, eps 0.02\n",
      "128707: done 112 games, mean reward -19.760, speed 43.80 f/s, eps 0.02\n",
      "130741: done 113 games, mean reward -19.700, speed 43.83 f/s, eps 0.02\n",
      "132732: done 114 games, mean reward -19.650, speed 43.79 f/s, eps 0.02\n",
      "134573: done 115 games, mean reward -19.610, speed 43.87 f/s, eps 0.02\n",
      "136685: done 116 games, mean reward -19.560, speed 43.81 f/s, eps 0.02\n",
      "138523: done 117 games, mean reward -19.510, speed 44.56 f/s, eps 0.02\n",
      "140896: done 118 games, mean reward -19.460, speed 44.75 f/s, eps 0.02\n",
      "143092: done 119 games, mean reward -19.410, speed 44.26 f/s, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144563: done 120 games, mean reward -19.400, speed 44.46 f/s, eps 0.02\n",
      "146359: done 121 games, mean reward -19.390, speed 44.20 f/s, eps 0.02\n",
      "148739: done 122 games, mean reward -19.310, speed 44.62 f/s, eps 0.02\n",
      "150210: done 123 games, mean reward -19.290, speed 43.61 f/s, eps 0.02\n",
      "151521: done 124 games, mean reward -19.280, speed 45.30 f/s, eps 0.02\n",
      "153791: done 125 games, mean reward -19.220, speed 45.30 f/s, eps 0.02\n",
      "155055: done 126 games, mean reward -19.210, speed 45.35 f/s, eps 0.02\n",
      "156522: done 127 games, mean reward -19.220, speed 45.60 f/s, eps 0.02\n",
      "157892: done 128 games, mean reward -19.190, speed 45.69 f/s, eps 0.02\n",
      "159138: done 129 games, mean reward -19.210, speed 45.50 f/s, eps 0.02\n",
      "160629: done 130 games, mean reward -19.210, speed 45.70 f/s, eps 0.02\n",
      "161959: done 131 games, mean reward -19.200, speed 45.67 f/s, eps 0.02\n",
      "164278: done 132 games, mean reward -19.050, speed 45.65 f/s, eps 0.02\n",
      "165836: done 133 games, mean reward -19.030, speed 45.71 f/s, eps 0.02\n",
      "167340: done 134 games, mean reward -19.030, speed 45.38 f/s, eps 0.02\n",
      "168910: done 135 games, mean reward -19.020, speed 45.67 f/s, eps 0.02\n",
      "170855: done 136 games, mean reward -18.940, speed 45.81 f/s, eps 0.02\n",
      "173179: done 137 games, mean reward -18.810, speed 45.73 f/s, eps 0.02\n",
      "175054: done 138 games, mean reward -18.740, speed 44.09 f/s, eps 0.02\n",
      "176603: done 139 games, mean reward -18.720, speed 44.02 f/s, eps 0.02\n",
      "179429: done 140 games, mean reward -18.600, speed 44.52 f/s, eps 0.02\n",
      "181321: done 141 games, mean reward -18.510, speed 42.77 f/s, eps 0.02\n",
      "183211: done 142 games, mean reward -18.440, speed 45.74 f/s, eps 0.02\n",
      "185245: done 143 games, mean reward -18.400, speed 45.50 f/s, eps 0.02\n",
      "187712: done 144 games, mean reward -18.330, speed 45.88 f/s, eps 0.02\n",
      "190745: done 145 games, mean reward -18.210, speed 45.51 f/s, eps 0.02\n",
      "192864: done 146 games, mean reward -18.130, speed 45.69 f/s, eps 0.02\n",
      "194956: done 147 games, mean reward -18.070, speed 45.46 f/s, eps 0.02\n",
      "197094: done 148 games, mean reward -17.970, speed 45.68 f/s, eps 0.02\n",
      "199702: done 149 games, mean reward -17.810, speed 45.49 f/s, eps 0.02\n",
      "202410: done 150 games, mean reward -17.680, speed 45.49 f/s, eps 0.02\n",
      "204705: done 151 games, mean reward -17.560, speed 45.33 f/s, eps 0.02\n",
      "206757: done 152 games, mean reward -17.470, speed 45.43 f/s, eps 0.02\n",
      "209101: done 153 games, mean reward -17.330, speed 45.71 f/s, eps 0.02\n",
      "211556: done 154 games, mean reward -17.180, speed 45.27 f/s, eps 0.02\n",
      "213519: done 155 games, mean reward -17.090, speed 45.37 f/s, eps 0.02\n",
      "216293: done 156 games, mean reward -16.940, speed 45.79 f/s, eps 0.02\n",
      "218773: done 157 games, mean reward -16.800, speed 45.61 f/s, eps 0.02\n",
      "221862: done 158 games, mean reward -16.610, speed 45.69 f/s, eps 0.02\n",
      "224230: done 159 games, mean reward -16.510, speed 45.67 f/s, eps 0.02\n",
      "226486: done 160 games, mean reward -16.400, speed 45.80 f/s, eps 0.02\n",
      "229433: done 161 games, mean reward -16.180, speed 45.84 f/s, eps 0.02\n",
      "232178: done 162 games, mean reward -16.040, speed 45.58 f/s, eps 0.02\n",
      "234949: done 163 games, mean reward -15.900, speed 45.75 f/s, eps 0.02\n",
      "237547: done 164 games, mean reward -15.800, speed 44.96 f/s, eps 0.02\n",
      "240260: done 165 games, mean reward -15.670, speed 45.37 f/s, eps 0.02\n",
      "242931: done 166 games, mean reward -15.530, speed 45.91 f/s, eps 0.02\n",
      "245563: done 167 games, mean reward -15.440, speed 45.49 f/s, eps 0.02\n",
      "248519: done 168 games, mean reward -15.300, speed 45.44 f/s, eps 0.02\n",
      "251692: done 169 games, mean reward -15.150, speed 45.61 f/s, eps 0.02\n",
      "254546: done 170 games, mean reward -15.020, speed 45.50 f/s, eps 0.02\n",
      "257804: done 171 games, mean reward -14.800, speed 45.61 f/s, eps 0.02\n",
      "260881: done 172 games, mean reward -14.660, speed 45.29 f/s, eps 0.02\n",
      "263874: done 173 games, mean reward -14.500, speed 45.62 f/s, eps 0.02\n",
      "267122: done 174 games, mean reward -14.320, speed 45.67 f/s, eps 0.02\n",
      "270375: done 175 games, mean reward -14.070, speed 45.74 f/s, eps 0.02\n",
      "273769: done 176 games, mean reward -13.890, speed 45.67 f/s, eps 0.02\n",
      "276453: done 177 games, mean reward -13.800, speed 45.67 f/s, eps 0.02\n",
      "279648: done 178 games, mean reward -13.640, speed 45.73 f/s, eps 0.02\n",
      "281998: done 179 games, mean reward -13.570, speed 45.76 f/s, eps 0.02\n",
      "285434: done 180 games, mean reward -13.400, speed 44.81 f/s, eps 0.02\n",
      "287976: done 181 games, mean reward -13.290, speed 44.58 f/s, eps 0.02\n",
      "290589: done 182 games, mean reward -13.180, speed 45.12 f/s, eps 0.02\n",
      "293615: done 183 games, mean reward -13.000, speed 45.20 f/s, eps 0.02\n",
      "297172: done 184 games, mean reward -12.820, speed 45.73 f/s, eps 0.02\n",
      "299996: done 185 games, mean reward -12.690, speed 45.63 f/s, eps 0.02\n",
      "302995: done 186 games, mean reward -12.560, speed 45.68 f/s, eps 0.02\n",
      "306197: done 187 games, mean reward -12.420, speed 45.82 f/s, eps 0.02\n",
      "309832: done 188 games, mean reward -12.270, speed 45.23 f/s, eps 0.02\n",
      "313769: done 189 games, mean reward -12.150, speed 45.60 f/s, eps 0.02\n",
      "317463: done 190 games, mean reward -11.970, speed 45.28 f/s, eps 0.02\n",
      "321587: done 191 games, mean reward -11.760, speed 45.60 f/s, eps 0.02\n",
      "325107: done 192 games, mean reward -11.610, speed 45.33 f/s, eps 0.02\n",
      "328626: done 193 games, mean reward -11.510, speed 45.76 f/s, eps 0.02\n",
      "331597: done 194 games, mean reward -11.240, speed 45.91 f/s, eps 0.02\n",
      "334768: done 195 games, mean reward -11.020, speed 45.59 f/s, eps 0.02\n",
      "338363: done 196 games, mean reward -10.820, speed 45.87 f/s, eps 0.02\n",
      "341054: done 197 games, mean reward -10.540, speed 45.42 f/s, eps 0.02\n",
      "343538: done 198 games, mean reward -10.250, speed 45.43 f/s, eps 0.02\n",
      "346558: done 199 games, mean reward -10.000, speed 45.65 f/s, eps 0.02\n",
      "349773: done 200 games, mean reward -9.760, speed 45.57 f/s, eps 0.02\n",
      "353004: done 201 games, mean reward -9.590, speed 45.30 f/s, eps 0.02\n",
      "356426: done 202 games, mean reward -9.350, speed 45.60 f/s, eps 0.02\n",
      "359579: done 203 games, mean reward -9.080, speed 45.56 f/s, eps 0.02\n",
      "362371: done 204 games, mean reward -8.790, speed 45.63 f/s, eps 0.02\n",
      "364769: done 205 games, mean reward -8.460, speed 45.59 f/s, eps 0.02\n",
      "368348: done 206 games, mean reward -8.310, speed 45.65 f/s, eps 0.02\n",
      "372308: done 207 games, mean reward -8.070, speed 45.53 f/s, eps 0.02\n",
      "374956: done 208 games, mean reward -7.780, speed 45.26 f/s, eps 0.02\n",
      "378332: done 209 games, mean reward -7.530, speed 45.73 f/s, eps 0.02\n",
      "381458: done 210 games, mean reward -7.350, speed 45.76 f/s, eps 0.02\n",
      "383767: done 211 games, mean reward -7.030, speed 45.81 f/s, eps 0.02\n",
      "386706: done 212 games, mean reward -6.790, speed 45.90 f/s, eps 0.02\n",
      "389868: done 213 games, mean reward -6.680, speed 45.38 f/s, eps 0.02\n",
      "392840: done 214 games, mean reward -6.470, speed 45.74 f/s, eps 0.02\n",
      "395000: done 215 games, mean reward -6.130, speed 45.82 f/s, eps 0.02\n",
      "397791: done 216 games, mean reward -5.870, speed 45.75 f/s, eps 0.02\n",
      "400645: done 217 games, mean reward -5.580, speed 45.74 f/s, eps 0.02\n",
      "404764: done 218 games, mean reward -5.460, speed 45.98 f/s, eps 0.02\n",
      "406697: done 219 games, mean reward -5.110, speed 45.66 f/s, eps 0.02\n",
      "409830: done 220 games, mean reward -4.820, speed 45.89 f/s, eps 0.02\n",
      "412507: done 221 games, mean reward -4.520, speed 45.92 f/s, eps 0.02\n",
      "415605: done 222 games, mean reward -4.320, speed 46.16 f/s, eps 0.02\n",
      "418574: done 223 games, mean reward -4.060, speed 45.90 f/s, eps 0.02\n",
      "421164: done 224 games, mean reward -3.730, speed 45.88 f/s, eps 0.02\n",
      "423190: done 225 games, mean reward -3.440, speed 46.01 f/s, eps 0.02\n",
      "425842: done 226 games, mean reward -3.160, speed 45.43 f/s, eps 0.02\n",
      "428031: done 227 games, mean reward -2.800, speed 45.94 f/s, eps 0.02\n",
      "429888: done 228 games, mean reward -2.440, speed 45.90 f/s, eps 0.02\n",
      "432652: done 229 games, mean reward -2.110, speed 45.95 f/s, eps 0.02\n",
      "434909: done 230 games, mean reward -1.770, speed 46.00 f/s, eps 0.02\n",
      "437111: done 231 games, mean reward -1.410, speed 46.13 f/s, eps 0.02\n",
      "439405: done 232 games, mean reward -1.190, speed 45.87 f/s, eps 0.02\n",
      "441621: done 233 games, mean reward -0.860, speed 45.92 f/s, eps 0.02\n",
      "443512: done 234 games, mean reward -0.490, speed 45.74 f/s, eps 0.02\n",
      "445355: done 235 games, mean reward -0.100, speed 45.96 f/s, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447341: done 236 games, mean reward 0.190, speed 45.87 f/s, eps 0.02\n",
      "449472: done 237 games, mean reward 0.410, speed 46.07 f/s, eps 0.02\n",
      "451853: done 238 games, mean reward 0.680, speed 45.86 f/s, eps 0.02\n",
      "453837: done 239 games, mean reward 1.030, speed 46.02 f/s, eps 0.02\n",
      "456013: done 240 games, mean reward 1.260, speed 45.70 f/s, eps 0.02\n",
      "457914: done 241 games, mean reward 1.540, speed 45.64 f/s, eps 0.02\n",
      "459833: done 242 games, mean reward 1.840, speed 46.01 f/s, eps 0.02\n",
      "461626: done 243 games, mean reward 2.190, speed 45.74 f/s, eps 0.02\n",
      "463699: done 244 games, mean reward 2.480, speed 45.90 f/s, eps 0.02\n",
      "465706: done 245 games, mean reward 2.750, speed 45.36 f/s, eps 0.02\n",
      "467430: done 246 games, mean reward 3.070, speed 45.76 f/s, eps 0.02\n",
      "469316: done 247 games, mean reward 3.380, speed 45.73 f/s, eps 0.02\n",
      "471260: done 248 games, mean reward 3.660, speed 46.06 f/s, eps 0.02\n",
      "473189: done 249 games, mean reward 3.900, speed 45.63 f/s, eps 0.02\n",
      "475072: done 250 games, mean reward 4.160, speed 45.90 f/s, eps 0.02\n",
      "476931: done 251 games, mean reward 4.440, speed 45.29 f/s, eps 0.02\n",
      "478621: done 252 games, mean reward 4.770, speed 46.14 f/s, eps 0.02\n",
      "480496: done 253 games, mean reward 5.000, speed 45.68 f/s, eps 0.02\n",
      "482554: done 254 games, mean reward 5.210, speed 45.67 f/s, eps 0.02\n",
      "484428: done 255 games, mean reward 5.500, speed 45.69 f/s, eps 0.02\n",
      "486197: done 256 games, mean reward 5.750, speed 45.96 f/s, eps 0.02\n",
      "488011: done 257 games, mean reward 6.010, speed 45.95 f/s, eps 0.02\n",
      "490246: done 258 games, mean reward 6.180, speed 45.93 f/s, eps 0.02\n",
      "492041: done 259 games, mean reward 6.450, speed 45.59 f/s, eps 0.02\n",
      "494023: done 260 games, mean reward 6.720, speed 45.92 f/s, eps 0.02\n",
      "496219: done 261 games, mean reward 6.830, speed 45.78 f/s, eps 0.02\n",
      "498000: done 262 games, mean reward 7.090, speed 45.73 f/s, eps 0.02\n",
      "499812: done 263 games, mean reward 7.320, speed 45.90 f/s, eps 0.02\n",
      "501692: done 264 games, mean reward 7.620, speed 45.80 f/s, eps 0.02\n",
      "504067: done 265 games, mean reward 7.850, speed 45.72 f/s, eps 0.02\n",
      "505824: done 266 games, mean reward 8.110, speed 45.68 f/s, eps 0.02\n",
      "507602: done 267 games, mean reward 8.420, speed 45.72 f/s, eps 0.02\n",
      "509523: done 268 games, mean reward 8.670, speed 46.04 f/s, eps 0.02\n",
      "511310: done 269 games, mean reward 8.910, speed 45.65 f/s, eps 0.02\n",
      "513009: done 270 games, mean reward 9.180, speed 45.74 f/s, eps 0.02\n",
      "515304: done 271 games, mean reward 9.290, speed 45.88 f/s, eps 0.02\n",
      "517252: done 272 games, mean reward 9.520, speed 46.05 f/s, eps 0.02\n",
      "519129: done 273 games, mean reward 9.740, speed 45.72 f/s, eps 0.02\n",
      "520796: done 274 games, mean reward 9.960, speed 45.73 f/s, eps 0.02\n",
      "522809: done 275 games, mean reward 10.080, speed 45.78 f/s, eps 0.02\n",
      "525177: done 276 games, mean reward 10.200, speed 45.80 f/s, eps 0.02\n",
      "527014: done 277 games, mean reward 10.470, speed 45.50 f/s, eps 0.02\n",
      "528836: done 278 games, mean reward 10.690, speed 46.08 f/s, eps 0.02\n",
      "530886: done 279 games, mean reward 10.990, speed 45.60 f/s, eps 0.02\n",
      "532698: done 280 games, mean reward 11.180, speed 45.67 f/s, eps 0.02\n",
      "534624: done 281 games, mean reward 11.470, speed 45.42 f/s, eps 0.02\n",
      "536291: done 282 games, mean reward 11.770, speed 45.67 f/s, eps 0.02\n",
      "538222: done 283 games, mean reward 11.960, speed 45.67 f/s, eps 0.02\n",
      "539921: done 284 games, mean reward 12.140, speed 45.69 f/s, eps 0.02\n",
      "541739: done 285 games, mean reward 12.400, speed 45.67 f/s, eps 0.02\n",
      "543481: done 286 games, mean reward 12.670, speed 45.93 f/s, eps 0.02\n",
      "545116: done 287 games, mean reward 12.930, speed 45.69 f/s, eps 0.02\n",
      "547001: done 288 games, mean reward 13.150, speed 45.45 f/s, eps 0.02\n",
      "548821: done 289 games, mean reward 13.390, speed 45.71 f/s, eps 0.02\n",
      "550754: done 290 games, mean reward 13.550, speed 46.07 f/s, eps 0.02\n",
      "552866: done 291 games, mean reward 13.680, speed 45.13 f/s, eps 0.02\n",
      "555045: done 292 games, mean reward 13.860, speed 45.62 f/s, eps 0.02\n",
      "557020: done 293 games, mean reward 14.090, speed 45.66 f/s, eps 0.02\n",
      "559078: done 294 games, mean reward 14.200, speed 45.97 f/s, eps 0.02\n",
      "560872: done 295 games, mean reward 14.320, speed 45.80 f/s, eps 0.02\n",
      "562563: done 296 games, mean reward 14.520, speed 45.41 f/s, eps 0.02\n",
      "564434: done 297 games, mean reward 14.610, speed 45.61 f/s, eps 0.02\n",
      "566127: done 298 games, mean reward 14.700, speed 45.96 f/s, eps 0.02\n",
      "568033: done 299 games, mean reward 14.840, speed 45.74 f/s, eps 0.02\n",
      "569909: done 300 games, mean reward 14.970, speed 45.50 f/s, eps 0.02\n",
      "571854: done 301 games, mean reward 15.180, speed 45.69 f/s, eps 0.02\n",
      "573485: done 302 games, mean reward 15.340, speed 45.98 f/s, eps 0.02\n",
      "575434: done 303 games, mean reward 15.430, speed 45.23 f/s, eps 0.02\n",
      "577318: done 304 games, mean reward 15.520, speed 45.99 f/s, eps 0.02\n",
      "579286: done 305 games, mean reward 15.540, speed 45.67 f/s, eps 0.02\n",
      "580933: done 306 games, mean reward 15.770, speed 45.67 f/s, eps 0.02\n",
      "582733: done 307 games, mean reward 15.930, speed 45.86 f/s, eps 0.02\n",
      "584641: done 308 games, mean reward 16.010, speed 45.86 f/s, eps 0.02\n",
      "586333: done 309 games, mean reward 16.160, speed 45.83 f/s, eps 0.02\n",
      "588465: done 310 games, mean reward 16.250, speed 46.05 f/s, eps 0.02\n",
      "590097: done 311 games, mean reward 16.320, speed 45.54 f/s, eps 0.02\n",
      "591778: done 312 games, mean reward 16.480, speed 45.69 f/s, eps 0.02\n",
      "593581: done 313 games, mean reward 16.710, speed 45.72 f/s, eps 0.02\n",
      "595383: done 314 games, mean reward 16.860, speed 45.68 f/s, eps 0.02\n",
      "597347: done 315 games, mean reward 16.870, speed 45.31 f/s, eps 0.02\n",
      "599598: done 316 games, mean reward 16.890, speed 45.60 f/s, eps 0.02\n",
      "601245: done 317 games, mean reward 16.970, speed 46.34 f/s, eps 0.02\n",
      "603279: done 318 games, mean reward 17.160, speed 45.62 f/s, eps 0.02\n",
      "605262: done 319 games, mean reward 17.140, speed 45.82 f/s, eps 0.02\n",
      "607204: done 320 games, mean reward 17.210, speed 45.97 f/s, eps 0.02\n",
      "608911: done 321 games, mean reward 17.290, speed 45.82 f/s, eps 0.02\n",
      "610559: done 322 games, mean reward 17.430, speed 45.83 f/s, eps 0.02\n",
      "612433: done 323 games, mean reward 17.510, speed 45.83 f/s, eps 0.02\n",
      "614486: done 324 games, mean reward 17.530, speed 45.70 f/s, eps 0.02\n",
      "616375: done 325 games, mean reward 17.570, speed 45.71 f/s, eps 0.02\n",
      "618168: done 326 games, mean reward 17.680, speed 45.19 f/s, eps 0.02\n",
      "619988: done 327 games, mean reward 17.700, speed 45.89 f/s, eps 0.02\n",
      "621637: done 328 games, mean reward 17.730, speed 45.48 f/s, eps 0.02\n",
      "623380: done 329 games, mean reward 17.800, speed 45.72 f/s, eps 0.02\n",
      "625164: done 330 games, mean reward 17.850, speed 45.68 f/s, eps 0.02\n",
      "627143: done 331 games, mean reward 17.870, speed 45.79 f/s, eps 0.02\n",
      "629014: done 332 games, mean reward 17.880, speed 45.88 f/s, eps 0.02\n",
      "631117: done 333 games, mean reward 17.900, speed 45.70 f/s, eps 0.02\n",
      "633014: done 334 games, mean reward 17.900, speed 45.49 f/s, eps 0.02\n",
      "634959: done 335 games, mean reward 17.890, speed 45.76 f/s, eps 0.02\n",
      "636995: done 336 games, mean reward 17.900, speed 45.66 f/s, eps 0.02\n",
      "638934: done 337 games, mean reward 17.930, speed 46.10 f/s, eps 0.02\n",
      "640639: done 338 games, mean reward 17.990, speed 45.63 f/s, eps 0.02\n",
      "642725: done 339 games, mean reward 17.980, speed 45.75 f/s, eps 0.02\n",
      "644571: done 340 games, mean reward 18.030, speed 45.60 f/s, eps 0.02\n",
      "Solved in 644571 frames!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #input hyperparameters, check CUDA available, create environment,then we use PTAN DQN wrapper to wrap up the environment\n",
    "    params = common_dqn_distrib.HYPERPARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=True, action=\"store_true\", help=\"Enable cuda\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    env = gym.make(params['env_name'])\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    #we make a writer for the environment and action dimension\n",
    "    writer = SummaryWriter(comment=\"-\" + params['run_name'] + \"-distrib\")\n",
    "    net = DistributionalDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    #the wrapper below can create a copy of DQN network, which is target network, and constantly synchronize with online\n",
    "    #network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    #we create agent to change observation to action value, we also need action selector to choose the action we use\n",
    "    #We use epsilon greedy method as action selector here\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "    epsilon_tracker = common_dqn_distrib.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(lambda x: net.qvals(x), selector, device=device)\n",
    "    \n",
    "    #experience source is from one step ExperienceSourceFirstLast and replay buffer, it will store fixed step transitions\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "    \n",
    "    #create optimizer and frame counter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    frame_idx = 0\n",
    "    \n",
    "    eval_states = None\n",
    "    prev_save = 0\n",
    "    save_prefix = None\n",
    "    \n",
    "    #reward tracker will report mean reward when episode end, and increase frame counter by 1, also getting a transition\n",
    "    #from frame buffer.\n",
    "    #buffer.populate(1) will activate following actions:\n",
    "    #ExperienceReplayBuffer will request for next transition from experience source.\n",
    "    #Experience source will send the observation to agent to get the action\n",
    "    #Action selector which use epsilon greedy method will choose an action based on greedy or random\n",
    "    #Action will be return to experience source and input to the environment to get reward and next observation, \n",
    "    # current observation, action, reward, next observation will be stored into replay buffer\n",
    "    #transfer information will be stored in replay buffer, and oldest observation will be dropped\n",
    "    with common_dqn_distrib.RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            epsilon_tracker.frame(frame_idx)\n",
    "            \n",
    "            #check undiscounted reward list after finishing an episode, and send to reward tracker to record the data\n",
    "            #Maybe it just play one step or didn't have finished episode, if it returns true, it means the mean reward\n",
    "            #reached the reward boundary and we can break and stop training\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                    break\n",
    "            \n",
    "            #we check buffer has cached enough data to start training or not. If not, we wait for more data.\n",
    "            if len(buffer) < params['replay_initial']:\n",
    "                continue\n",
    "                \n",
    "            if eval_states is None:\n",
    "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "                eval_states = np.array(eval_states, copy=False)\n",
    "            \n",
    "            #here we use Stochastic Gradient Descent(SGD) to calculate loss, zero the gradient,batch from the replay buffer\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(params['batch_size'])\n",
    "            \n",
    "            \n",
    "            save_prefix = None\n",
    "            if SAVE_TRANSITIONS_IMG:\n",
    "                interesting = any(map(lambda s: s.last_state is None or s.reward != 0.0, batch))\n",
    "                if interesting and frame_idx // 30000 > prev_save:\n",
    "                    save_prefix = \"images/img_%08d\" % frame_idx\n",
    "                    prev_save = frame_idx // 30000\n",
    "            \n",
    "            \n",
    "            loss_v = calc_loss(batch, net, tgt_net.target_model, gamma=params['gamma'], device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #synchronize the target network with the online network constantly\n",
    "            if frame_idx % params['target_net_sync'] == 0:\n",
    "                tgt_net.sync()\n",
    "                \n",
    "            if frame_idx % EVAL_EVERY_FRAME == 0:\n",
    "                mean_val = calc_values_of_states(eval_states, net, device=device)\n",
    "                writer.add_scalar(\"values_mean\", mean_val, frame_idx)\n",
    "\n",
    "            if SAVE_STATES_IMG and frame_idx % 10000 == 0:\n",
    "                save_state_images(frame_idx, eval_states, net, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
