{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06b99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a34ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the prior alpha value and the change of beta frames, in the first 100000 frames, beta will change from 0.4 to 1.0\n",
    "PRIO_REPLAY_ALPHA = 0.6\n",
    "BETA_START = 0.4\n",
    "BETA_FRAMES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946822d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioReplayBuffer:\n",
    "    #we use ring buffer to store the replay buffer data, ring buffer can store enough data without resetting, we record\n",
    "    #the priorities and put the iterator to the experience source object for efficient sampling.\n",
    "    def __init__(self, exp_source, buf_size, prob_alpha=0.6):\n",
    "        self.exp_source_iter = iter(exp_source)\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity = buf_size\n",
    "        self.pos = 0\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((buf_size, ), dtype=np.float32)\n",
    "    \n",
    "    #populate() will get fixed amount of item from ExperienceSource and save in buffer.\n",
    "    #if buffer is not full, we put the sample into buffer.\n",
    "    #if buffer is full, we overwrite the oldest data and change the record position and the buffer size\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def populate(self, count):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        for _ in range(count):\n",
    "            sample = next(self.exp_source_iter)\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(sample)\n",
    "            else:\n",
    "                self.buffer[self.pos] = sample\n",
    "            self.priorities[self.pos] = max_prio\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    #in sample(), we use hyperparameters ALPHA to change priority value to probabilities.\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        #we use the probabilities to sample in the replay buffer and get the batch\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        #we calculate the sample weight and return batch, indices and weights, we need the indices to update the priority\n",
    "        #level of the samples\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, np.array(weights, dtype=np.float32)\n",
    "    \n",
    "    #this function update the priority level for the processed batch, function caller should use the loss to call this batch\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98db443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSELoss in Pytorch do not support weighting, because in regression problem MSE is the loss, but sample weighting is\n",
    "#usually used in classification problem, therefore, we need to calculate MSE here and add weightings\n",
    "def calc_loss(batch, batch_weights, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "    \n",
    "    #all are same as before except we adding the batch weighting here\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "    \n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    \n",
    "    #we calculate loss below, we don't use library function and implement our own code because we can add weightings\n",
    "    #the weighting will transfer to priority replay buffer to renew the priority, to prevent zero priority existing, we\n",
    "    #will add a very small value to deal with the zero loss value problem\n",
    "    expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "    losses_v = batch_weights_v * (state_action_values - expected_state_action_values) ** 2\n",
    "    return losses_v.mean(), losses_v + 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ad9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080: done 1 games, mean reward -20.000, speed 211.83 f/s, eps 0.99\n",
      "2345: done 2 games, mean reward -19.000, speed 233.76 f/s, eps 0.98\n",
      "3127: done 3 games, mean reward -19.667, speed 210.77 f/s, eps 0.97\n",
      "4153: done 4 games, mean reward -20.000, speed 174.16 f/s, eps 0.96\n",
      "5051: done 5 games, mean reward -20.000, speed 162.26 f/s, eps 0.95\n",
      "5899: done 6 games, mean reward -20.167, speed 164.75 f/s, eps 0.94\n",
      "6825: done 7 games, mean reward -20.143, speed 163.06 f/s, eps 0.93\n",
      "7776: done 8 games, mean reward -20.000, speed 161.70 f/s, eps 0.92\n",
      "8594: done 9 games, mean reward -20.111, speed 162.22 f/s, eps 0.91\n",
      "9493: done 10 games, mean reward -20.200, speed 161.92 f/s, eps 0.91\n",
      "10311: done 11 games, mean reward -20.273, speed 93.85 f/s, eps 0.90\n",
      "11236: done 12 games, mean reward -20.250, speed 57.93 f/s, eps 0.89\n",
      "12059: done 13 games, mean reward -20.308, speed 55.39 f/s, eps 0.88\n",
      "13021: done 14 games, mean reward -20.286, speed 54.96 f/s, eps 0.87\n",
      "13826: done 15 games, mean reward -20.333, speed 54.98 f/s, eps 0.86\n",
      "14762: done 16 games, mean reward -20.375, speed 54.94 f/s, eps 0.85\n",
      "15744: done 17 games, mean reward -20.412, speed 54.31 f/s, eps 0.84\n",
      "16501: done 18 games, mean reward -20.444, speed 53.24 f/s, eps 0.83\n",
      "17726: done 19 games, mean reward -20.421, speed 54.87 f/s, eps 0.82\n",
      "18648: done 20 games, mean reward -20.450, speed 54.81 f/s, eps 0.81\n",
      "19631: done 21 games, mean reward -20.381, speed 53.30 f/s, eps 0.80\n",
      "20483: done 22 games, mean reward -20.409, speed 53.27 f/s, eps 0.80\n",
      "21286: done 23 games, mean reward -20.435, speed 53.43 f/s, eps 0.79\n",
      "22299: done 24 games, mean reward -20.375, speed 53.15 f/s, eps 0.78\n",
      "23157: done 25 games, mean reward -20.360, speed 53.08 f/s, eps 0.77\n",
      "24067: done 26 games, mean reward -20.346, speed 53.36 f/s, eps 0.76\n",
      "24847: done 27 games, mean reward -20.370, speed 53.25 f/s, eps 0.75\n",
      "25663: done 28 games, mean reward -20.393, speed 52.59 f/s, eps 0.74\n",
      "26547: done 29 games, mean reward -20.379, speed 53.16 f/s, eps 0.73\n",
      "27583: done 30 games, mean reward -20.300, speed 53.19 f/s, eps 0.72\n",
      "28625: done 31 games, mean reward -20.258, speed 53.57 f/s, eps 0.71\n",
      "29587: done 32 games, mean reward -20.219, speed 53.30 f/s, eps 0.70\n",
      "30725: done 33 games, mean reward -20.152, speed 53.38 f/s, eps 0.69\n",
      "31643: done 34 games, mean reward -20.147, speed 53.46 f/s, eps 0.68\n",
      "32551: done 35 games, mean reward -20.171, speed 52.98 f/s, eps 0.67\n",
      "33693: done 36 games, mean reward -20.139, speed 53.09 f/s, eps 0.66\n",
      "34646: done 37 games, mean reward -20.135, speed 53.03 f/s, eps 0.65\n",
      "35634: done 38 games, mean reward -20.132, speed 53.10 f/s, eps 0.64\n",
      "36761: done 39 games, mean reward -20.128, speed 51.94 f/s, eps 0.63\n",
      "37826: done 40 games, mean reward -20.125, speed 50.01 f/s, eps 0.62\n",
      "39032: done 41 games, mean reward -20.098, speed 50.49 f/s, eps 0.61\n",
      "40361: done 42 games, mean reward -20.024, speed 50.39 f/s, eps 0.60\n",
      "41858: done 43 games, mean reward -19.907, speed 51.85 f/s, eps 0.58\n",
      "43579: done 44 games, mean reward -19.841, speed 51.74 f/s, eps 0.56\n",
      "45035: done 45 games, mean reward -19.822, speed 51.74 f/s, eps 0.55\n",
      "46035: done 46 games, mean reward -19.804, speed 51.50 f/s, eps 0.54\n",
      "47580: done 47 games, mean reward -19.809, speed 51.68 f/s, eps 0.52\n",
      "48829: done 48 games, mean reward -19.771, speed 51.53 f/s, eps 0.51\n",
      "50541: done 49 games, mean reward -19.694, speed 50.92 f/s, eps 0.49\n",
      "52221: done 50 games, mean reward -19.660, speed 50.71 f/s, eps 0.48\n",
      "53553: done 51 games, mean reward -19.647, speed 51.22 f/s, eps 0.46\n",
      "54688: done 52 games, mean reward -19.673, speed 51.03 f/s, eps 0.45\n",
      "56246: done 53 games, mean reward -19.623, speed 50.05 f/s, eps 0.44\n",
      "58095: done 54 games, mean reward -19.519, speed 49.53 f/s, eps 0.42\n",
      "59817: done 55 games, mean reward -19.473, speed 49.35 f/s, eps 0.40\n",
      "61261: done 56 games, mean reward -19.464, speed 49.41 f/s, eps 0.39\n",
      "62755: done 57 games, mean reward -19.474, speed 49.29 f/s, eps 0.37\n",
      "64157: done 58 games, mean reward -19.448, speed 49.18 f/s, eps 0.36\n",
      "66023: done 59 games, mean reward -19.390, speed 48.97 f/s, eps 0.34\n",
      "67804: done 60 games, mean reward -19.300, speed 48.85 f/s, eps 0.32\n",
      "69239: done 61 games, mean reward -19.295, speed 48.77 f/s, eps 0.31\n",
      "70880: done 62 games, mean reward -19.323, speed 48.83 f/s, eps 0.29\n",
      "72780: done 63 games, mean reward -19.302, speed 48.64 f/s, eps 0.27\n",
      "74669: done 64 games, mean reward -19.234, speed 48.29 f/s, eps 0.25\n",
      "76857: done 65 games, mean reward -19.154, speed 49.94 f/s, eps 0.23\n",
      "78869: done 66 games, mean reward -19.076, speed 49.39 f/s, eps 0.21\n",
      "80895: done 67 games, mean reward -19.015, speed 48.13 f/s, eps 0.19\n",
      "83470: done 68 games, mean reward -18.853, speed 47.08 f/s, eps 0.17\n",
      "85862: done 69 games, mean reward -18.797, speed 46.89 f/s, eps 0.14\n",
      "87796: done 70 games, mean reward -18.757, speed 46.96 f/s, eps 0.12\n",
      "89665: done 71 games, mean reward -18.718, speed 46.99 f/s, eps 0.10\n",
      "92084: done 72 games, mean reward -18.653, speed 45.68 f/s, eps 0.08\n",
      "95204: done 73 games, mean reward -18.452, speed 43.76 f/s, eps 0.05\n",
      "98061: done 74 games, mean reward -18.405, speed 43.74 f/s, eps 0.02\n",
      "100833: done 75 games, mean reward -18.280, speed 44.01 f/s, eps 0.02\n",
      "103537: done 76 games, mean reward -18.237, speed 44.58 f/s, eps 0.02\n",
      "106539: done 77 games, mean reward -18.130, speed 44.45 f/s, eps 0.02\n",
      "108364: done 78 games, mean reward -18.128, speed 44.20 f/s, eps 0.02\n",
      "111406: done 79 games, mean reward -18.038, speed 44.18 f/s, eps 0.02\n",
      "113503: done 80 games, mean reward -17.950, speed 43.85 f/s, eps 0.02\n",
      "116734: done 81 games, mean reward -17.802, speed 44.14 f/s, eps 0.02\n",
      "119171: done 82 games, mean reward -17.732, speed 44.10 f/s, eps 0.02\n",
      "122036: done 83 games, mean reward -17.651, speed 44.80 f/s, eps 0.02\n",
      "124691: done 84 games, mean reward -17.571, speed 44.96 f/s, eps 0.02\n",
      "127398: done 85 games, mean reward -17.494, speed 45.04 f/s, eps 0.02\n",
      "129696: done 86 games, mean reward -17.442, speed 45.19 f/s, eps 0.02\n",
      "132823: done 87 games, mean reward -17.310, speed 45.35 f/s, eps 0.02\n",
      "135128: done 88 games, mean reward -17.284, speed 44.90 f/s, eps 0.02\n",
      "138144: done 89 games, mean reward -17.191, speed 44.11 f/s, eps 0.02\n",
      "141574: done 90 games, mean reward -17.067, speed 44.41 f/s, eps 0.02\n",
      "144438: done 91 games, mean reward -16.989, speed 44.33 f/s, eps 0.02\n",
      "147004: done 92 games, mean reward -16.935, speed 44.27 f/s, eps 0.02\n",
      "150181: done 93 games, mean reward -16.828, speed 44.37 f/s, eps 0.02\n",
      "152679: done 94 games, mean reward -16.787, speed 44.34 f/s, eps 0.02\n",
      "155630: done 95 games, mean reward -16.705, speed 44.16 f/s, eps 0.02\n",
      "159555: done 96 games, mean reward -16.583, speed 44.33 f/s, eps 0.02\n",
      "162509: done 97 games, mean reward -16.505, speed 43.98 f/s, eps 0.02\n",
      "165376: done 98 games, mean reward -16.408, speed 44.25 f/s, eps 0.02\n",
      "167859: done 99 games, mean reward -16.333, speed 44.43 f/s, eps 0.02\n",
      "170284: done 100 games, mean reward -16.290, speed 43.06 f/s, eps 0.02\n",
      "173212: done 101 games, mean reward -16.150, speed 42.72 f/s, eps 0.02\n",
      "176193: done 102 games, mean reward -16.050, speed 40.78 f/s, eps 0.02\n",
      "180073: done 103 games, mean reward -15.850, speed 43.15 f/s, eps 0.02\n",
      "182810: done 104 games, mean reward -15.740, speed 37.78 f/s, eps 0.02\n",
      "185824: done 105 games, mean reward -15.610, speed 35.44 f/s, eps 0.02\n",
      "188671: done 106 games, mean reward -15.470, speed 33.00 f/s, eps 0.02\n",
      "191581: done 107 games, mean reward -15.340, speed 32.55 f/s, eps 0.02\n",
      "193884: done 108 games, mean reward -15.240, speed 33.56 f/s, eps 0.02\n",
      "197146: done 109 games, mean reward -15.070, speed 32.37 f/s, eps 0.02\n",
      "199629: done 110 games, mean reward -14.930, speed 33.07 f/s, eps 0.02\n",
      "202917: done 111 games, mean reward -14.770, speed 31.95 f/s, eps 0.02\n",
      "206116: done 112 games, mean reward -14.610, speed 32.09 f/s, eps 0.02\n",
      "209331: done 113 games, mean reward -14.440, speed 32.72 f/s, eps 0.02\n",
      "212327: done 114 games, mean reward -14.290, speed 32.99 f/s, eps 0.02\n",
      "215628: done 115 games, mean reward -14.130, speed 33.10 f/s, eps 0.02\n",
      "219089: done 116 games, mean reward -13.980, speed 33.12 f/s, eps 0.02\n",
      "221751: done 117 games, mean reward -13.870, speed 33.35 f/s, eps 0.02\n",
      "225186: done 118 games, mean reward -13.670, speed 31.73 f/s, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227360: done 119 games, mean reward -13.620, speed 33.10 f/s, eps 0.02\n",
      "230640: done 120 games, mean reward -13.440, speed 32.34 f/s, eps 0.02\n",
      "233254: done 121 games, mean reward -13.320, speed 33.15 f/s, eps 0.02\n",
      "237167: done 122 games, mean reward -13.130, speed 33.07 f/s, eps 0.02\n",
      "240952: done 123 games, mean reward -12.910, speed 32.85 f/s, eps 0.02\n",
      "244338: done 124 games, mean reward -12.750, speed 33.59 f/s, eps 0.02\n",
      "247305: done 125 games, mean reward -12.570, speed 32.29 f/s, eps 0.02\n",
      "250614: done 126 games, mean reward -12.280, speed 32.66 f/s, eps 0.02\n",
      "254471: done 127 games, mean reward -12.030, speed 32.46 f/s, eps 0.02\n",
      "257741: done 128 games, mean reward -11.890, speed 32.42 f/s, eps 0.02\n",
      "261147: done 129 games, mean reward -11.700, speed 32.35 f/s, eps 0.02\n",
      "265596: done 130 games, mean reward -11.530, speed 32.57 f/s, eps 0.02\n",
      "269309: done 131 games, mean reward -11.310, speed 32.75 f/s, eps 0.02\n",
      "272520: done 132 games, mean reward -11.160, speed 38.96 f/s, eps 0.02\n",
      "276138: done 133 games, mean reward -11.030, speed 39.77 f/s, eps 0.02\n",
      "279284: done 134 games, mean reward -10.880, speed 42.90 f/s, eps 0.02\n",
      "282758: done 135 games, mean reward -10.690, speed 41.39 f/s, eps 0.02\n",
      "285925: done 136 games, mean reward -10.460, speed 38.11 f/s, eps 0.02\n",
      "288977: done 137 games, mean reward -10.180, speed 36.83 f/s, eps 0.02\n",
      "292161: done 138 games, mean reward -10.050, speed 37.86 f/s, eps 0.02\n",
      "295285: done 139 games, mean reward -9.780, speed 40.02 f/s, eps 0.02\n",
      "298792: done 140 games, mean reward -9.530, speed 37.12 f/s, eps 0.02\n",
      "302245: done 141 games, mean reward -9.310, speed 32.58 f/s, eps 0.02\n",
      "305976: done 142 games, mean reward -9.150, speed 32.46 f/s, eps 0.02\n",
      "309020: done 143 games, mean reward -9.030, speed 32.58 f/s, eps 0.02\n",
      "312391: done 144 games, mean reward -8.790, speed 32.37 f/s, eps 0.02\n",
      "315901: done 145 games, mean reward -8.570, speed 32.55 f/s, eps 0.02\n",
      "319464: done 146 games, mean reward -8.340, speed 32.23 f/s, eps 0.02\n",
      "322540: done 147 games, mean reward -8.030, speed 32.04 f/s, eps 0.02\n",
      "325235: done 148 games, mean reward -7.720, speed 32.05 f/s, eps 0.02\n",
      "328206: done 149 games, mean reward -7.460, speed 32.88 f/s, eps 0.02\n",
      "330877: done 150 games, mean reward -7.140, speed 33.79 f/s, eps 0.02\n",
      "333562: done 151 games, mean reward -6.840, speed 36.53 f/s, eps 0.02\n",
      "335534: done 152 games, mean reward -6.460, speed 40.02 f/s, eps 0.02\n",
      "337562: done 153 games, mean reward -6.120, speed 37.02 f/s, eps 0.02\n",
      "339738: done 154 games, mean reward -5.840, speed 38.69 f/s, eps 0.02\n",
      "341826: done 155 games, mean reward -5.500, speed 40.26 f/s, eps 0.02\n",
      "344068: done 156 games, mean reward -5.160, speed 40.75 f/s, eps 0.02\n",
      "346059: done 157 games, mean reward -4.790, speed 36.12 f/s, eps 0.02\n",
      "348239: done 158 games, mean reward -4.460, speed 40.13 f/s, eps 0.02\n",
      "350039: done 159 games, mean reward -4.100, speed 38.97 f/s, eps 0.02\n",
      "352888: done 160 games, mean reward -3.840, speed 39.91 f/s, eps 0.02\n",
      "355176: done 161 games, mean reward -3.500, speed 40.47 f/s, eps 0.02\n",
      "356973: done 162 games, mean reward -3.100, speed 39.46 f/s, eps 0.02\n",
      "358889: done 163 games, mean reward -2.740, speed 39.41 f/s, eps 0.02\n",
      "361040: done 164 games, mean reward -2.420, speed 42.43 f/s, eps 0.02\n",
      "363409: done 165 games, mean reward -2.140, speed 39.85 f/s, eps 0.02\n",
      "365737: done 166 games, mean reward -1.850, speed 41.50 f/s, eps 0.02\n",
      "367690: done 167 games, mean reward -1.520, speed 42.04 f/s, eps 0.02\n",
      "369487: done 168 games, mean reward -1.250, speed 42.11 f/s, eps 0.02\n",
      "371267: done 169 games, mean reward -0.900, speed 42.38 f/s, eps 0.02\n",
      "373200: done 170 games, mean reward -0.570, speed 41.29 f/s, eps 0.02\n",
      "375195: done 171 games, mean reward -0.250, speed 41.86 f/s, eps 0.02\n",
      "377005: done 172 games, mean reward 0.080, speed 41.92 f/s, eps 0.02\n",
      "379302: done 173 games, mean reward 0.290, speed 41.98 f/s, eps 0.02\n",
      "381128: done 174 games, mean reward 0.630, speed 42.09 f/s, eps 0.02\n",
      "382926: done 175 games, mean reward 0.910, speed 43.20 f/s, eps 0.02\n",
      "385045: done 176 games, mean reward 1.250, speed 42.69 f/s, eps 0.02\n",
      "386734: done 177 games, mean reward 1.560, speed 42.49 f/s, eps 0.02\n",
      "388815: done 178 games, mean reward 1.930, speed 42.44 f/s, eps 0.02\n",
      "390601: done 179 games, mean reward 2.240, speed 41.62 f/s, eps 0.02\n",
      "392311: done 180 games, mean reward 2.550, speed 41.46 f/s, eps 0.02\n",
      "394671: done 181 games, mean reward 2.760, speed 42.04 f/s, eps 0.02\n",
      "396852: done 182 games, mean reward 3.050, speed 41.89 f/s, eps 0.02\n",
      "398721: done 183 games, mean reward 3.350, speed 42.10 f/s, eps 0.02\n",
      "400357: done 184 games, mean reward 3.670, speed 42.20 f/s, eps 0.02\n",
      "401989: done 185 games, mean reward 3.990, speed 41.94 f/s, eps 0.02\n",
      "403916: done 186 games, mean reward 4.310, speed 41.64 f/s, eps 0.02\n",
      "405667: done 187 games, mean reward 4.560, speed 41.89 f/s, eps 0.02\n",
      "407330: done 188 games, mean reward 4.910, speed 42.57 f/s, eps 0.02\n",
      "409116: done 189 games, mean reward 5.190, speed 42.70 f/s, eps 0.02\n",
      "411429: done 190 games, mean reward 5.410, speed 42.38 f/s, eps 0.02\n",
      "413173: done 191 games, mean reward 5.700, speed 41.23 f/s, eps 0.02\n",
      "415089: done 192 games, mean reward 6.000, speed 38.72 f/s, eps 0.02\n",
      "416874: done 193 games, mean reward 6.260, speed 43.15 f/s, eps 0.02\n",
      "418769: done 194 games, mean reward 6.570, speed 43.00 f/s, eps 0.02\n",
      "420909: done 195 games, mean reward 6.830, speed 43.03 f/s, eps 0.02\n",
      "422602: done 196 games, mean reward 7.090, speed 42.86 f/s, eps 0.02\n",
      "424439: done 197 games, mean reward 7.360, speed 42.13 f/s, eps 0.02\n",
      "426295: done 198 games, mean reward 7.620, speed 39.91 f/s, eps 0.02\n",
      "428174: done 199 games, mean reward 7.890, speed 37.57 f/s, eps 0.02\n",
      "429826: done 200 games, mean reward 8.220, speed 40.31 f/s, eps 0.02\n",
      "432345: done 201 games, mean reward 8.390, speed 42.06 f/s, eps 0.02\n",
      "434056: done 202 games, mean reward 8.670, speed 42.01 f/s, eps 0.02\n",
      "436243: done 203 games, mean reward 8.840, speed 42.08 f/s, eps 0.02\n",
      "438336: done 204 games, mean reward 9.100, speed 41.77 f/s, eps 0.02\n",
      "440099: done 205 games, mean reward 9.370, speed 42.04 f/s, eps 0.02\n",
      "442083: done 206 games, mean reward 9.610, speed 41.86 f/s, eps 0.02\n",
      "443919: done 207 games, mean reward 9.880, speed 42.89 f/s, eps 0.02\n",
      "446047: done 208 games, mean reward 10.160, speed 42.75 f/s, eps 0.02\n",
      "447894: done 209 games, mean reward 10.390, speed 42.39 f/s, eps 0.02\n",
      "450076: done 210 games, mean reward 10.620, speed 42.78 f/s, eps 0.02\n",
      "452650: done 211 games, mean reward 10.820, speed 42.33 f/s, eps 0.02\n",
      "454335: done 212 games, mean reward 11.060, speed 42.32 f/s, eps 0.02\n",
      "456499: done 213 games, mean reward 11.280, speed 41.98 f/s, eps 0.02\n",
      "458453: done 214 games, mean reward 11.510, speed 42.05 f/s, eps 0.02\n",
      "460266: done 215 games, mean reward 11.750, speed 42.13 f/s, eps 0.02\n",
      "462573: done 216 games, mean reward 11.960, speed 42.05 f/s, eps 0.02\n",
      "465090: done 217 games, mean reward 12.210, speed 42.00 f/s, eps 0.02\n",
      "466939: done 218 games, mean reward 12.410, speed 42.09 f/s, eps 0.02\n",
      "468727: done 219 games, mean reward 12.760, speed 41.23 f/s, eps 0.02\n",
      "470510: done 220 games, mean reward 12.980, speed 42.09 f/s, eps 0.02\n",
      "472761: done 221 games, mean reward 13.200, speed 43.57 f/s, eps 0.02\n",
      "474895: done 222 games, mean reward 13.390, speed 42.92 f/s, eps 0.02\n",
      "477573: done 223 games, mean reward 13.500, speed 43.24 f/s, eps 0.02\n",
      "479990: done 224 games, mean reward 13.680, speed 41.32 f/s, eps 0.02\n",
      "481620: done 225 games, mean reward 13.910, speed 41.77 f/s, eps 0.02\n",
      "483949: done 226 games, mean reward 13.980, speed 42.09 f/s, eps 0.02\n",
      "486171: done 227 games, mean reward 14.120, speed 42.26 f/s, eps 0.02\n",
      "488615: done 228 games, mean reward 14.280, speed 40.19 f/s, eps 0.02\n",
      "490822: done 229 games, mean reward 14.480, speed 39.46 f/s, eps 0.02\n",
      "493431: done 230 games, mean reward 14.660, speed 39.19 f/s, eps 0.02\n",
      "495624: done 231 games, mean reward 14.790, speed 39.37 f/s, eps 0.02\n",
      "497855: done 232 games, mean reward 15.010, speed 39.45 f/s, eps 0.02\n",
      "500198: done 233 games, mean reward 15.220, speed 38.48 f/s, eps 0.02\n",
      "502080: done 234 games, mean reward 15.450, speed 37.67 f/s, eps 0.02\n",
      "503713: done 235 games, mean reward 15.680, speed 38.29 f/s, eps 0.02\n",
      "505742: done 236 games, mean reward 15.830, speed 38.44 f/s, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507755: done 237 games, mean reward 15.930, speed 38.40 f/s, eps 0.02\n",
      "509759: done 238 games, mean reward 16.190, speed 39.80 f/s, eps 0.02\n",
      "511499: done 239 games, mean reward 16.310, speed 39.09 f/s, eps 0.02\n",
      "513289: done 240 games, mean reward 16.470, speed 38.42 f/s, eps 0.02\n",
      "515436: done 241 games, mean reward 16.610, speed 38.41 f/s, eps 0.02\n",
      "517190: done 242 games, mean reward 16.810, speed 38.35 f/s, eps 0.02\n",
      "519133: done 243 games, mean reward 17.050, speed 39.15 f/s, eps 0.02\n",
      "520932: done 244 games, mean reward 17.180, speed 39.19 f/s, eps 0.02\n",
      "522665: done 245 games, mean reward 17.350, speed 39.12 f/s, eps 0.02\n",
      "524815: done 246 games, mean reward 17.480, speed 38.88 f/s, eps 0.02\n",
      "526847: done 247 games, mean reward 17.570, speed 38.80 f/s, eps 0.02\n",
      "529073: done 248 games, mean reward 17.610, speed 39.23 f/s, eps 0.02\n",
      "530890: done 249 games, mean reward 17.700, speed 37.98 f/s, eps 0.02\n",
      "532701: done 250 games, mean reward 17.760, speed 40.14 f/s, eps 0.02\n",
      "534487: done 251 games, mean reward 17.840, speed 42.34 f/s, eps 0.02\n",
      "536193: done 252 games, mean reward 17.880, speed 42.23 f/s, eps 0.02\n",
      "538032: done 253 games, mean reward 17.890, speed 38.19 f/s, eps 0.02\n",
      "539970: done 254 games, mean reward 17.930, speed 38.50 f/s, eps 0.02\n",
      "541599: done 255 games, mean reward 17.970, speed 37.86 f/s, eps 0.02\n",
      "543340: done 256 games, mean reward 18.020, speed 38.80 f/s, eps 0.02\n",
      "Solved in 543340 frames!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #input hyperparameters, check CUDA available, create environment,then we use PTAN DQN wrapper to wrap up the environment\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=True, action=\"store_true\", help=\"Enable cuda\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    env = gym.make(params['env_name'])\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    #we make a writer for the environment and action dimension\n",
    "    writer = SummaryWriter(comment=\"-\" + params['run_name'] + \"-prio-replay\")\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    #the wrapper below can create a copy of DQN network, which is target network, and constantly synchronize with online\n",
    "    #network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    #we create agent to change observation to action value, we also need action selector to choose the action we use\n",
    "    #We use epsilon greedy method as action selector here\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    \n",
    "    #experience source is from one step ExperienceSourceFirstLast and replay buffer, it will store fixed step transitions\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)\n",
    "    #we use priority buffer above instead of the original buffer\n",
    "    buffer = PrioReplayBuffer(exp_source, params['replay_size'], PRIO_REPLAY_ALPHA)\n",
    "    \n",
    "    #create optimizer and frame counter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    frame_idx = 0\n",
    "    \n",
    "    beta = BETA_START\n",
    "    \n",
    "    #reward tracker will report mean reward when episode end, and increase frame counter by 1, also getting a transition\n",
    "    #from frame buffer.\n",
    "    #buffer.populate(1) will activate following actions:\n",
    "    #ExperienceReplayBuffer will request for next transition from experience source.\n",
    "    #Experience source will send the observation to agent to get the action\n",
    "    #Action selector which use epsilon greedy method will choose an action based on greedy or random\n",
    "    #Action will be return to experience source and input to the environment to get reward and next observation, \n",
    "    # current observation, action, reward, next observation will be stored into replay buffer\n",
    "    #transfer information will be stored in replay buffer, and oldest observation will be dropped\n",
    "    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            epsilon_tracker.frame(frame_idx)\n",
    "            \n",
    "            #we linearly increase Beta value to update epsilon with epsilon similar linearly decreasing function\n",
    "            beta = min(1.0, BETA_START + frame_idx * (1.0 - BETA_START) / BETA_FRAMES)\n",
    "            \n",
    "            #check undiscounted reward list after finishing an episode, and send to reward tracker to record the data\n",
    "            #Maybe it just play one step or didn't have finished episode, if it returns true, it means the mean reward\n",
    "            #reached the reward boundary and we can break and stop training\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                writer.add_scalar(\"beta\", beta, frame_idx)\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                    break\n",
    "            \n",
    "            #we check buffer has cached enough data to start training or not. If not, we wait for more data.\n",
    "            if len(buffer) < params['replay_initial']:\n",
    "                continue\n",
    "            \n",
    "            #get sample from buffer, return 3 values instead of 1, which is batch, indices and weighting\n",
    "            #we send batch and weighting to loss function, and get 2 objects:\n",
    "            #loss_v which is the cumulative loss for back propagation\n",
    "            #sample_prios_v is a tensor which contains the loss value for each sample in the batch\n",
    "            #then we back propagate the loss and renew the sample priority level with the function buffer.update_priorities\n",
    "            optimizer.zero_grad()\n",
    "            batch, batch_indices, batch_weights = buffer.sample(params['batch_size'], beta)\n",
    "            loss_v, sample_prios_v = calc_loss(batch, batch_weights, net, tgt_net.target_model,\n",
    "                                               params['gamma'], device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            buffer.update_priorities(batch_indices, sample_prios_v.data.cpu().numpy())\n",
    "            \n",
    "            #synchronize the target network with the online network constantly\n",
    "            if frame_idx % params['target_net_sync'] == 0:\n",
    "                tgt_net.sync()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
