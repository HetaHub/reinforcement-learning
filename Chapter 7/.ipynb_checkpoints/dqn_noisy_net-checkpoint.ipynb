{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06b99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from lib import dqn_model_noise, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2eec096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDQN(nn.Module):\n",
    "    #same as DQN network\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(NoisyDQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #Use input, output shape to construct Noisy Layer, put into sequential model for access    \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.noisy_layers = [\n",
    "            dqn_model_noise.NoisyLinear(conv_out_size, 512),\n",
    "            dqn_model_noise.NoisyLinear(512, n_actions)\n",
    "        ]\n",
    "        self.fc = nn.Sequential(\n",
    "            self.noisy_layers[0],\n",
    "            nn.ReLU(),\n",
    "            self.noisy_layers[1]\n",
    "        )\n",
    "    \n",
    "    #get convolution shape, forward function is same as before\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    \n",
    "    #following function to calculate noisy layer SNR\n",
    "    def noisy_layers_sigma_snr(self):\n",
    "        return [\n",
    "            ((layer.weight ** 2).mean().sqrt() / (layer.sigma_weight ** 2).mean().sqrt()).item()\n",
    "            for layer in self.noisy_layers\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ad9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764: done 1 games, mean reward -21.000, speed 168.88 f/s\n",
      "1525: done 2 games, mean reward -21.000, speed 197.78 f/s\n",
      "2285: done 3 games, mean reward -21.000, speed 182.22 f/s\n",
      "3047: done 4 games, mean reward -21.000, speed 187.54 f/s\n",
      "3804: done 5 games, mean reward -21.000, speed 186.91 f/s\n",
      "4564: done 6 games, mean reward -21.000, speed 187.65 f/s\n",
      "5323: done 7 games, mean reward -21.000, speed 187.96 f/s\n",
      "6084: done 8 games, mean reward -21.000, speed 185.61 f/s\n",
      "6841: done 9 games, mean reward -21.000, speed 188.39 f/s\n",
      "7602: done 10 games, mean reward -21.000, speed 187.89 f/s\n",
      "8364: done 11 games, mean reward -21.000, speed 188.00 f/s\n",
      "9123: done 12 games, mean reward -21.000, speed 186.39 f/s\n",
      "9885: done 13 games, mean reward -21.000, speed 185.99 f/s\n",
      "10778: done 14 games, mean reward -20.929, speed 50.10 f/s\n",
      "11632: done 15 games, mean reward -20.867, speed 43.51 f/s\n",
      "12483: done 16 games, mean reward -20.875, speed 44.56 f/s\n",
      "13245: done 17 games, mean reward -20.882, speed 44.94 f/s\n",
      "14146: done 18 games, mean reward -20.833, speed 44.69 f/s\n",
      "14960: done 19 games, mean reward -20.842, speed 44.24 f/s\n",
      "15722: done 20 games, mean reward -20.850, speed 44.70 f/s\n",
      "16479: done 21 games, mean reward -20.857, speed 44.36 f/s\n",
      "17331: done 22 games, mean reward -20.864, speed 44.71 f/s\n",
      "18184: done 23 games, mean reward -20.870, speed 44.97 f/s\n",
      "19172: done 24 games, mean reward -20.875, speed 44.94 f/s\n",
      "20167: done 25 games, mean reward -20.880, speed 44.54 f/s\n",
      "21196: done 26 games, mean reward -20.885, speed 45.01 f/s\n",
      "22137: done 27 games, mean reward -20.889, speed 44.22 f/s\n",
      "22919: done 28 games, mean reward -20.893, speed 44.00 f/s\n",
      "23680: done 29 games, mean reward -20.897, speed 43.82 f/s\n",
      "24729: done 30 games, mean reward -20.867, speed 43.58 f/s\n",
      "25661: done 31 games, mean reward -20.806, speed 44.33 f/s\n",
      "26508: done 32 games, mean reward -20.812, speed 44.38 f/s\n",
      "27555: done 33 games, mean reward -20.818, speed 44.69 f/s\n",
      "28497: done 34 games, mean reward -20.824, speed 44.74 f/s\n",
      "29328: done 35 games, mean reward -20.829, speed 44.89 f/s\n",
      "30310: done 36 games, mean reward -20.806, speed 44.72 f/s\n",
      "31531: done 37 games, mean reward -20.730, speed 44.61 f/s\n",
      "32421: done 38 games, mean reward -20.711, speed 44.60 f/s\n",
      "33314: done 39 games, mean reward -20.718, speed 44.73 f/s\n",
      "34266: done 40 games, mean reward -20.725, speed 44.62 f/s\n",
      "35457: done 41 games, mean reward -20.707, speed 44.71 f/s\n",
      "36742: done 42 games, mean reward -20.667, speed 44.79 f/s\n",
      "38200: done 43 games, mean reward -20.581, speed 44.60 f/s\n",
      "39489: done 44 games, mean reward -20.545, speed 44.49 f/s\n",
      "40980: done 45 games, mean reward -20.489, speed 44.68 f/s\n",
      "42123: done 46 games, mean reward -20.500, speed 44.57 f/s\n",
      "43238: done 47 games, mean reward -20.489, speed 44.38 f/s\n",
      "44140: done 48 games, mean reward -20.479, speed 44.26 f/s\n",
      "45274: done 49 games, mean reward -20.490, speed 44.37 f/s\n",
      "46605: done 50 games, mean reward -20.480, speed 44.44 f/s\n",
      "47807: done 51 games, mean reward -20.490, speed 44.81 f/s\n",
      "49230: done 52 games, mean reward -20.462, speed 44.49 f/s\n",
      "50610: done 53 games, mean reward -20.434, speed 44.85 f/s\n",
      "51838: done 54 games, mean reward -20.444, speed 44.38 f/s\n",
      "53412: done 55 games, mean reward -20.400, speed 44.75 f/s\n",
      "54855: done 56 games, mean reward -20.393, speed 44.66 f/s\n",
      "56245: done 57 games, mean reward -20.351, speed 44.71 f/s\n",
      "57598: done 58 games, mean reward -20.345, speed 44.50 f/s\n",
      "58837: done 59 games, mean reward -20.322, speed 44.60 f/s\n",
      "60568: done 60 games, mean reward -20.283, speed 44.46 f/s\n",
      "62063: done 61 games, mean reward -20.246, speed 44.34 f/s\n",
      "63817: done 62 games, mean reward -20.210, speed 44.43 f/s\n",
      "65642: done 63 games, mean reward -20.175, speed 44.03 f/s\n",
      "67260: done 64 games, mean reward -20.141, speed 44.61 f/s\n",
      "68996: done 65 games, mean reward -20.092, speed 44.52 f/s\n",
      "70635: done 66 games, mean reward -20.076, speed 44.28 f/s\n",
      "72608: done 67 games, mean reward -20.015, speed 44.58 f/s\n",
      "74518: done 68 games, mean reward -19.971, speed 44.33 f/s\n",
      "76506: done 69 games, mean reward -19.913, speed 44.34 f/s\n",
      "78295: done 70 games, mean reward -19.914, speed 44.19 f/s\n",
      "80130: done 71 games, mean reward -19.873, speed 44.57 f/s\n",
      "82373: done 72 games, mean reward -19.833, speed 44.37 f/s\n",
      "84844: done 73 games, mean reward -19.781, speed 44.20 f/s\n",
      "86968: done 74 games, mean reward -19.757, speed 44.23 f/s\n",
      "88976: done 75 games, mean reward -19.760, speed 44.17 f/s\n",
      "91351: done 76 games, mean reward -19.684, speed 43.59 f/s\n",
      "93825: done 77 games, mean reward -19.649, speed 43.77 f/s\n",
      "96569: done 78 games, mean reward -19.590, speed 44.33 f/s\n",
      "98885: done 79 games, mean reward -19.557, speed 43.69 f/s\n",
      "100795: done 80 games, mean reward -19.525, speed 44.36 f/s\n",
      "103042: done 81 games, mean reward -19.506, speed 42.43 f/s\n",
      "105885: done 82 games, mean reward -19.463, speed 42.57 f/s\n",
      "107922: done 83 games, mean reward -19.446, speed 44.43 f/s\n",
      "110620: done 84 games, mean reward -19.345, speed 44.53 f/s\n",
      "113244: done 85 games, mean reward -19.282, speed 44.37 f/s\n",
      "115746: done 86 games, mean reward -19.221, speed 44.49 f/s\n",
      "118439: done 87 games, mean reward -19.126, speed 44.47 f/s\n",
      "121168: done 88 games, mean reward -19.057, speed 44.58 f/s\n",
      "123285: done 89 games, mean reward -19.045, speed 44.44 f/s\n",
      "126133: done 90 games, mean reward -18.956, speed 44.51 f/s\n",
      "128929: done 91 games, mean reward -18.901, speed 44.27 f/s\n",
      "131984: done 92 games, mean reward -18.848, speed 44.24 f/s\n",
      "134789: done 93 games, mean reward -18.785, speed 44.39 f/s\n",
      "137222: done 94 games, mean reward -18.755, speed 44.28 f/s\n",
      "139886: done 95 games, mean reward -18.421, speed 44.34 f/s\n",
      "143839: done 96 games, mean reward -18.260, speed 44.44 f/s\n",
      "147018: done 97 games, mean reward -18.165, speed 44.35 f/s\n",
      "150544: done 98 games, mean reward -18.041, speed 44.50 f/s\n",
      "153377: done 99 games, mean reward -17.990, speed 44.51 f/s\n",
      "156235: done 100 games, mean reward -17.970, speed 44.36 f/s\n",
      "158603: done 101 games, mean reward -17.910, speed 44.28 f/s\n",
      "162094: done 102 games, mean reward -17.810, speed 44.28 f/s\n",
      "165982: done 103 games, mean reward -17.710, speed 44.36 f/s\n",
      "169171: done 104 games, mean reward -17.610, speed 44.43 f/s\n",
      "171839: done 105 games, mean reward -17.560, speed 44.52 f/s\n",
      "174873: done 106 games, mean reward -17.470, speed 44.48 f/s\n",
      "178092: done 107 games, mean reward -17.380, speed 44.63 f/s\n",
      "181732: done 108 games, mean reward -17.230, speed 44.55 f/s\n",
      "184308: done 109 games, mean reward -16.890, speed 44.53 f/s\n",
      "186773: done 110 games, mean reward -16.530, speed 44.15 f/s\n",
      "189743: done 111 games, mean reward -16.230, speed 44.52 f/s\n",
      "192030: done 112 games, mean reward -15.840, speed 44.48 f/s\n",
      "194252: done 113 games, mean reward -15.460, speed 44.45 f/s\n",
      "197130: done 114 games, mean reward -15.150, speed 44.54 f/s\n",
      "198785: done 115 games, mean reward -14.740, speed 44.30 f/s\n",
      "201207: done 116 games, mean reward -14.410, speed 44.47 f/s\n",
      "204359: done 117 games, mean reward -14.120, speed 44.48 f/s\n",
      "206312: done 118 games, mean reward -13.720, speed 44.18 f/s\n",
      "208192: done 119 games, mean reward -13.300, speed 44.59 f/s\n",
      "209949: done 120 games, mean reward -12.880, speed 44.58 f/s\n",
      "211761: done 121 games, mean reward -12.460, speed 44.32 f/s\n",
      "214845: done 122 games, mean reward -12.170, speed 44.54 f/s\n",
      "216542: done 123 games, mean reward -11.750, speed 44.37 f/s\n",
      "218240: done 124 games, mean reward -11.330, speed 44.54 f/s\n",
      "219936: done 125 games, mean reward -10.910, speed 44.40 f/s\n",
      "223065: done 126 games, mean reward -10.670, speed 44.47 f/s\n",
      "224760: done 127 games, mean reward -10.250, speed 44.29 f/s\n",
      "226562: done 128 games, mean reward -9.850, speed 44.32 f/s\n",
      "228879: done 129 games, mean reward -9.500, speed 44.40 f/s\n",
      "230575: done 130 games, mean reward -9.090, speed 44.47 f/s\n",
      "232270: done 131 games, mean reward -8.690, speed 44.44 f/s\n",
      "233962: done 132 games, mean reward -8.270, speed 44.34 f/s\n",
      "236022: done 133 games, mean reward -7.900, speed 44.58 f/s\n",
      "238454: done 134 games, mean reward -7.560, speed 44.20 f/s\n",
      "240151: done 135 games, mean reward -7.140, speed 44.20 f/s\n",
      "241852: done 136 games, mean reward -6.740, speed 44.16 f/s\n",
      "243648: done 137 games, mean reward -6.370, speed 44.15 f/s\n",
      "245511: done 138 games, mean reward -5.990, speed 43.55 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247212: done 139 games, mean reward -5.570, speed 43.44 f/s\n",
      "248910: done 140 games, mean reward -5.150, speed 44.13 f/s\n",
      "250750: done 141 games, mean reward -4.770, speed 44.05 f/s\n",
      "252729: done 142 games, mean reward -4.400, speed 44.44 f/s\n",
      "254429: done 143 games, mean reward -4.020, speed 44.42 f/s\n",
      "256123: done 144 games, mean reward -3.620, speed 44.15 f/s\n",
      "257872: done 145 games, mean reward -3.240, speed 44.25 f/s\n",
      "259563: done 146 games, mean reward -2.820, speed 44.37 f/s\n",
      "261321: done 147 games, mean reward -2.420, speed 44.33 f/s\n",
      "263081: done 148 games, mean reward -2.010, speed 44.64 f/s\n",
      "265110: done 149 games, mean reward -1.620, speed 44.13 f/s\n",
      "266808: done 150 games, mean reward -1.210, speed 44.14 f/s\n",
      "268782: done 151 games, mean reward -0.840, speed 44.21 f/s\n",
      "270570: done 152 games, mean reward -0.470, speed 44.57 f/s\n",
      "272294: done 153 games, mean reward -0.080, speed 44.40 f/s\n",
      "273990: done 154 games, mean reward 0.340, speed 43.40 f/s\n",
      "276065: done 155 games, mean reward 0.690, speed 43.31 f/s\n",
      "277766: done 156 games, mean reward 1.100, speed 43.94 f/s\n",
      "279520: done 157 games, mean reward 1.480, speed 44.09 f/s\n",
      "281415: done 158 games, mean reward 1.850, speed 44.24 f/s\n",
      "283230: done 159 games, mean reward 2.230, speed 44.09 f/s\n",
      "284900: done 160 games, mean reward 2.610, speed 44.38 f/s\n",
      "286973: done 161 games, mean reward 2.950, speed 44.38 f/s\n",
      "288625: done 162 games, mean reward 3.340, speed 44.09 f/s\n",
      "290410: done 163 games, mean reward 3.720, speed 44.50 f/s\n",
      "292110: done 164 games, mean reward 4.110, speed 43.53 f/s\n",
      "294256: done 165 games, mean reward 4.420, speed 44.04 f/s\n",
      "295955: done 166 games, mean reward 4.820, speed 44.55 f/s\n",
      "297650: done 167 games, mean reward 5.180, speed 44.36 f/s\n",
      "299343: done 168 games, mean reward 5.560, speed 44.64 f/s\n",
      "301034: done 169 games, mean reward 5.930, speed 44.50 f/s\n",
      "302730: done 170 games, mean reward 6.340, speed 44.29 f/s\n",
      "304514: done 171 games, mean reward 6.700, speed 44.38 f/s\n",
      "306498: done 172 games, mean reward 7.060, speed 44.34 f/s\n",
      "308524: done 173 games, mean reward 7.390, speed 44.19 f/s\n",
      "310698: done 174 games, mean reward 7.720, speed 44.50 f/s\n",
      "312399: done 175 games, mean reward 8.130, speed 44.19 f/s\n",
      "314375: done 176 games, mean reward 8.440, speed 44.35 f/s\n",
      "316316: done 177 games, mean reward 8.770, speed 43.65 f/s\n",
      "318126: done 178 games, mean reward 9.120, speed 42.17 f/s\n",
      "319888: done 179 games, mean reward 9.500, speed 42.87 f/s\n",
      "321583: done 180 games, mean reward 9.880, speed 42.41 f/s\n",
      "323279: done 181 games, mean reward 10.270, speed 43.67 f/s\n",
      "324979: done 182 games, mean reward 10.640, speed 44.41 f/s\n",
      "326730: done 183 games, mean reward 11.010, speed 44.34 f/s\n",
      "328434: done 184 games, mean reward 11.320, speed 43.17 f/s\n",
      "330132: done 185 games, mean reward 11.670, speed 43.88 f/s\n",
      "331827: done 186 games, mean reward 12.020, speed 43.40 f/s\n",
      "333856: done 187 games, mean reward 12.310, speed 42.34 f/s\n",
      "335551: done 188 games, mean reward 12.650, speed 43.57 f/s\n",
      "337250: done 189 games, mean reward 13.030, speed 44.37 f/s\n",
      "339167: done 190 games, mean reward 13.330, speed 44.55 f/s\n",
      "340864: done 191 games, mean reward 13.680, speed 44.38 f/s\n",
      "342715: done 192 games, mean reward 14.000, speed 44.30 f/s\n",
      "345057: done 193 games, mean reward 14.210, speed 44.37 f/s\n",
      "346753: done 194 games, mean reward 14.580, speed 44.34 f/s\n",
      "348424: done 195 games, mean reward 14.650, speed 44.33 f/s\n",
      "351387: done 196 games, mean reward 14.750, speed 44.52 f/s\n",
      "353100: done 197 games, mean reward 15.050, speed 44.14 f/s\n",
      "354752: done 198 games, mean reward 15.320, speed 43.62 f/s\n",
      "356448: done 199 games, mean reward 15.660, speed 39.65 f/s\n",
      "358099: done 200 games, mean reward 16.030, speed 29.84 f/s\n",
      "360153: done 201 games, mean reward 16.340, speed 27.82 f/s\n",
      "361822: done 202 games, mean reward 16.650, speed 26.34 f/s\n",
      "363514: done 203 games, mean reward 16.970, speed 26.63 f/s\n",
      "365243: done 204 games, mean reward 17.280, speed 27.29 f/s\n",
      "366977: done 205 games, mean reward 17.640, speed 40.46 f/s\n",
      "368678: done 206 games, mean reward 17.970, speed 41.64 f/s\n",
      "370414: done 207 games, mean reward 18.290, speed 44.01 f/s\n",
      "Solved in 370414 frames!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #input hyperparameters, check CUDA available, create environment,then we use PTAN DQN wrapper to wrap up the environment\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=True, action=\"store_true\", help=\"Enable cuda\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    env = gym.make(params['env_name'])\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    #we make a writer for the environment and action dimension\n",
    "    writer = SummaryWriter(comment=\"-\" + params['run_name'] + \"-noisy-net\")\n",
    "    net = NoisyDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    #the wrapper below can create a copy of DQN network, which is target network, and constantly synchronize with online\n",
    "    #network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    agent = ptan.agent.DQNAgent(net, ptan.actions.ArgmaxActionSelector(), device=device)\n",
    "    \n",
    "    #experience source is from one step ExperienceSourceFirstLast and replay buffer, it will store fixed step transitions\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "    \n",
    "    #create optimizer and frame counter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    frame_idx = 0\n",
    "    \n",
    "    #reward tracker will report mean reward when episode end, and increase frame counter by 1, also getting a transition\n",
    "    #from frame buffer.\n",
    "    #buffer.populate(1) will activate following actions:\n",
    "    #ExperienceReplayBuffer will request for next transition from experience source.\n",
    "    #Experience source will send the observation to agent to get the action\n",
    "    #Action selector which use epsilon greedy method will choose an action based on greedy or random\n",
    "    #Action will be return to experience source and input to the environment to get reward and next observation, \n",
    "    # current observation, action, reward, next observation will be stored into replay buffer\n",
    "    #transfer information will be stored in replay buffer, and oldest observation will be dropped\n",
    "    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            \n",
    "            #check undiscounted reward list after finishing an episode, and send to reward tracker to record the data\n",
    "            #Maybe it just play one step or didn't have finished episode, if it returns true, it means the mean reward\n",
    "            #reached the reward boundary and we can break and stop training\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx):\n",
    "                    break\n",
    "            \n",
    "            #we check buffer has cached enough data to start training or not. If not, we wait for more data.\n",
    "            if len(buffer) < params['replay_initial']:\n",
    "                continue\n",
    "            \n",
    "            #here we use Stochastic Gradient Descent(SGD) to calculate loss, zero the gradient,batch from the replay buffer\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(params['batch_size'])\n",
    "            loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params['gamma'], device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #synchronize the target network with the online network constantly\n",
    "            if frame_idx % params['target_net_sync'] == 0:\n",
    "                tgt_net.sync()\n",
    "                \n",
    "            if frame_idx % 500 == 0:\n",
    "                snr_vals = net.noisy_layers_sigma_snr()\n",
    "                for layer_idx, sigma_l2 in enumerate(snr_vals):\n",
    "                    writer.add_scalar(\"sigma_snr_layer_%d\" %(layer_idx+1), sigma_l2, frame_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
