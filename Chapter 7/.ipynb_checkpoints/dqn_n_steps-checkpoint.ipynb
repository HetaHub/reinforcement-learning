{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06b99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adc1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_STEPS_DEFAULT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ad9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041: done 1 games, mean reward -18.000, speed 77.95 f/s, eps 0.99\n",
      "2000: done 2 games, mean reward -19.000, speed 165.24 f/s, eps 0.98\n",
      "2912: done 3 games, mean reward -19.333, speed 165.06 f/s, eps 0.97\n",
      "3909: done 4 games, mean reward -19.750, speed 164.94 f/s, eps 0.96\n",
      "4946: done 5 games, mean reward -19.800, speed 166.76 f/s, eps 0.95\n",
      "5763: done 6 games, mean reward -20.000, speed 166.89 f/s, eps 0.94\n",
      "6829: done 7 games, mean reward -19.857, speed 168.08 f/s, eps 0.93\n",
      "7890: done 8 games, mean reward -19.875, speed 166.92 f/s, eps 0.92\n",
      "8846: done 9 games, mean reward -19.889, speed 166.10 f/s, eps 0.91\n",
      "9693: done 10 games, mean reward -20.000, speed 166.81 f/s, eps 0.90\n",
      "10598: done 11 games, mean reward -20.091, speed 79.29 f/s, eps 0.89\n",
      "11422: done 12 games, mean reward -20.167, speed 61.22 f/s, eps 0.89\n",
      "12357: done 13 games, mean reward -20.154, speed 59.35 f/s, eps 0.88\n",
      "13117: done 14 games, mean reward -20.214, speed 59.69 f/s, eps 0.87\n",
      "13876: done 15 games, mean reward -20.267, speed 59.71 f/s, eps 0.86\n",
      "14970: done 16 games, mean reward -20.125, speed 58.20 f/s, eps 0.85\n",
      "15854: done 17 games, mean reward -20.176, speed 59.63 f/s, eps 0.84\n",
      "16670: done 18 games, mean reward -20.222, speed 59.17 f/s, eps 0.83\n",
      "17706: done 19 games, mean reward -20.211, speed 58.96 f/s, eps 0.82\n",
      "18635: done 20 games, mean reward -20.250, speed 58.45 f/s, eps 0.81\n",
      "19558: done 21 games, mean reward -20.238, speed 58.48 f/s, eps 0.80\n",
      "20553: done 22 games, mean reward -20.227, speed 58.76 f/s, eps 0.79\n",
      "21573: done 23 games, mean reward -20.261, speed 57.61 f/s, eps 0.78\n",
      "22484: done 24 games, mean reward -20.292, speed 58.98 f/s, eps 0.78\n",
      "23361: done 25 games, mean reward -20.320, speed 59.15 f/s, eps 0.77\n",
      "24229: done 26 games, mean reward -20.308, speed 59.09 f/s, eps 0.76\n",
      "25234: done 27 games, mean reward -20.296, speed 58.38 f/s, eps 0.75\n",
      "26067: done 28 games, mean reward -20.286, speed 57.88 f/s, eps 0.74\n",
      "27004: done 29 games, mean reward -20.241, speed 57.79 f/s, eps 0.73\n",
      "28119: done 30 games, mean reward -20.200, speed 58.21 f/s, eps 0.72\n",
      "29061: done 31 games, mean reward -20.194, speed 58.61 f/s, eps 0.71\n",
      "30021: done 32 games, mean reward -20.188, speed 57.82 f/s, eps 0.70\n",
      "31180: done 33 games, mean reward -20.152, speed 58.63 f/s, eps 0.69\n",
      "32135: done 34 games, mean reward -20.176, speed 58.97 f/s, eps 0.68\n",
      "33060: done 35 games, mean reward -20.200, speed 58.47 f/s, eps 0.67\n",
      "34183: done 36 games, mean reward -20.194, speed 58.73 f/s, eps 0.66\n",
      "35219: done 37 games, mean reward -20.162, speed 58.55 f/s, eps 0.65\n",
      "36271: done 38 games, mean reward -20.132, speed 59.22 f/s, eps 0.64\n",
      "37760: done 39 games, mean reward -20.051, speed 59.41 f/s, eps 0.62\n",
      "38941: done 40 games, mean reward -20.050, speed 59.21 f/s, eps 0.61\n",
      "40211: done 41 games, mean reward -20.049, speed 59.47 f/s, eps 0.60\n",
      "41430: done 42 games, mean reward -20.000, speed 59.27 f/s, eps 0.59\n",
      "42808: done 43 games, mean reward -19.977, speed 58.63 f/s, eps 0.57\n",
      "44089: done 44 games, mean reward -19.932, speed 58.20 f/s, eps 0.56\n",
      "45476: done 45 games, mean reward -19.867, speed 58.90 f/s, eps 0.55\n",
      "46699: done 46 games, mean reward -19.870, speed 58.82 f/s, eps 0.53\n",
      "48117: done 47 games, mean reward -19.787, speed 58.86 f/s, eps 0.52\n",
      "49479: done 48 games, mean reward -19.771, speed 57.60 f/s, eps 0.51\n",
      "50939: done 49 games, mean reward -19.714, speed 58.25 f/s, eps 0.49\n",
      "52480: done 50 games, mean reward -19.640, speed 59.32 f/s, eps 0.48\n",
      "54045: done 51 games, mean reward -19.627, speed 58.81 f/s, eps 0.46\n",
      "56041: done 52 games, mean reward -19.558, speed 56.27 f/s, eps 0.44\n",
      "57406: done 53 games, mean reward -19.566, speed 51.28 f/s, eps 0.43\n",
      "59415: done 54 games, mean reward -19.481, speed 56.70 f/s, eps 0.41\n",
      "61162: done 55 games, mean reward -19.473, speed 57.69 f/s, eps 0.39\n",
      "62966: done 56 games, mean reward -19.393, speed 58.07 f/s, eps 0.37\n",
      "64945: done 57 games, mean reward -19.316, speed 58.20 f/s, eps 0.35\n",
      "67158: done 58 games, mean reward -19.241, speed 57.43 f/s, eps 0.33\n",
      "69061: done 59 games, mean reward -19.186, speed 58.27 f/s, eps 0.31\n",
      "70842: done 60 games, mean reward -19.167, speed 55.98 f/s, eps 0.29\n",
      "73241: done 61 games, mean reward -19.049, speed 56.92 f/s, eps 0.27\n",
      "75478: done 62 games, mean reward -18.984, speed 57.69 f/s, eps 0.25\n",
      "77717: done 63 games, mean reward -18.968, speed 58.11 f/s, eps 0.22\n",
      "79820: done 64 games, mean reward -18.938, speed 55.28 f/s, eps 0.20\n",
      "82471: done 65 games, mean reward -18.862, speed 53.19 f/s, eps 0.18\n",
      "85833: done 66 games, mean reward -18.742, speed 57.92 f/s, eps 0.14\n",
      "88878: done 67 games, mean reward -18.657, speed 57.45 f/s, eps 0.11\n",
      "91556: done 68 games, mean reward -18.632, speed 55.19 f/s, eps 0.08\n",
      "94701: done 69 games, mean reward -18.507, speed 53.12 f/s, eps 0.05\n",
      "98011: done 70 games, mean reward -18.314, speed 50.72 f/s, eps 0.02\n",
      "100249: done 71 games, mean reward -18.324, speed 44.37 f/s, eps 0.02\n",
      "103588: done 72 games, mean reward -18.236, speed 51.34 f/s, eps 0.02\n",
      "106232: done 73 games, mean reward -18.192, speed 49.37 f/s, eps 0.02\n",
      "107943: done 74 games, mean reward -18.216, speed 55.15 f/s, eps 0.02\n",
      "110049: done 75 games, mean reward -18.187, speed 55.82 f/s, eps 0.02\n",
      "112407: done 76 games, mean reward -18.158, speed 55.36 f/s, eps 0.02\n",
      "114999: done 77 games, mean reward -18.091, speed 55.97 f/s, eps 0.02\n",
      "117973: done 78 games, mean reward -17.974, speed 55.63 f/s, eps 0.02\n",
      "120403: done 79 games, mean reward -17.937, speed 55.78 f/s, eps 0.02\n",
      "123861: done 80 games, mean reward -17.850, speed 55.20 f/s, eps 0.02\n",
      "126786: done 81 games, mean reward -17.778, speed 55.61 f/s, eps 0.02\n",
      "129779: done 82 games, mean reward -17.732, speed 50.91 f/s, eps 0.02\n",
      "134202: done 83 games, mean reward -17.554, speed 49.30 f/s, eps 0.02\n",
      "138919: done 84 games, mean reward -17.333, speed 49.55 f/s, eps 0.02\n",
      "142879: done 85 games, mean reward -17.176, speed 49.83 f/s, eps 0.02\n",
      "146983: done 86 games, mean reward -17.070, speed 48.99 f/s, eps 0.02\n",
      "150363: done 87 games, mean reward -16.989, speed 43.31 f/s, eps 0.02\n",
      "154738: done 88 games, mean reward -16.818, speed 39.12 f/s, eps 0.02\n",
      "158664: done 89 games, mean reward -16.697, speed 49.22 f/s, eps 0.02\n",
      "163567: done 90 games, mean reward -16.489, speed 48.03 f/s, eps 0.02\n",
      "167044: done 91 games, mean reward -16.253, speed 48.72 f/s, eps 0.02\n",
      "169645: done 92 games, mean reward -15.946, speed 46.78 f/s, eps 0.02\n",
      "173001: done 93 games, mean reward -15.710, speed 48.98 f/s, eps 0.02\n",
      "175986: done 94 games, mean reward -15.468, speed 50.02 f/s, eps 0.02\n",
      "179469: done 95 games, mean reward -15.284, speed 49.80 f/s, eps 0.02\n",
      "181909: done 96 games, mean reward -14.990, speed 50.21 f/s, eps 0.02\n",
      "184500: done 97 games, mean reward -14.722, speed 40.73 f/s, eps 0.02\n",
      "186391: done 98 games, mean reward -14.398, speed 39.39 f/s, eps 0.02\n",
      "188940: done 99 games, mean reward -14.111, speed 42.95 f/s, eps 0.02\n",
      "191304: done 100 games, mean reward -13.810, speed 47.59 f/s, eps 0.02\n",
      "193601: done 101 games, mean reward -13.470, speed 49.05 f/s, eps 0.02\n",
      "195608: done 102 games, mean reward -13.080, speed 49.65 f/s, eps 0.02\n",
      "197309: done 103 games, mean reward -12.670, speed 42.82 f/s, eps 0.02\n",
      "199158: done 104 games, mean reward -12.260, speed 40.98 f/s, eps 0.02\n",
      "200891: done 105 games, mean reward -11.860, speed 45.76 f/s, eps 0.02\n",
      "203166: done 106 games, mean reward -11.500, speed 51.99 f/s, eps 0.02\n",
      "204992: done 107 games, mean reward -11.110, speed 45.94 f/s, eps 0.02\n",
      "206973: done 108 games, mean reward -10.730, speed 50.58 f/s, eps 0.02\n",
      "208844: done 109 games, mean reward -10.330, speed 49.51 f/s, eps 0.02\n",
      "211677: done 110 games, mean reward -10.000, speed 46.92 f/s, eps 0.02\n",
      "213436: done 111 games, mean reward -9.580, speed 46.06 f/s, eps 0.02\n",
      "215347: done 112 games, mean reward -9.190, speed 46.36 f/s, eps 0.02\n",
      "218551: done 113 games, mean reward -8.890, speed 44.95 f/s, eps 0.02\n",
      "220501: done 114 games, mean reward -8.500, speed 45.37 f/s, eps 0.02\n",
      "222674: done 115 games, mean reward -8.100, speed 49.08 f/s, eps 0.02\n",
      "224430: done 116 games, mean reward -7.720, speed 49.02 f/s, eps 0.02\n",
      "226914: done 117 games, mean reward -7.370, speed 49.48 f/s, eps 0.02\n",
      "228620: done 118 games, mean reward -6.960, speed 49.41 f/s, eps 0.02\n",
      "231105: done 119 games, mean reward -6.610, speed 50.89 f/s, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233172: done 120 games, mean reward -6.220, speed 51.16 f/s, eps 0.02\n",
      "235248: done 121 games, mean reward -5.840, speed 50.77 f/s, eps 0.02\n",
      "237047: done 122 games, mean reward -5.450, speed 50.85 f/s, eps 0.02\n",
      "239204: done 123 games, mean reward -5.070, speed 50.81 f/s, eps 0.02\n",
      "240961: done 124 games, mean reward -4.650, speed 51.26 f/s, eps 0.02\n",
      "242863: done 125 games, mean reward -4.260, speed 50.43 f/s, eps 0.02\n",
      "244691: done 126 games, mean reward -3.860, speed 50.96 f/s, eps 0.02\n",
      "246876: done 127 games, mean reward -3.490, speed 49.95 f/s, eps 0.02\n",
      "249530: done 128 games, mean reward -3.150, speed 49.40 f/s, eps 0.02\n",
      "251497: done 129 games, mean reward -2.760, speed 49.41 f/s, eps 0.02\n",
      "253599: done 130 games, mean reward -2.380, speed 50.28 f/s, eps 0.02\n",
      "255389: done 131 games, mean reward -1.990, speed 51.12 f/s, eps 0.02\n",
      "257077: done 132 games, mean reward -1.590, speed 49.21 f/s, eps 0.02\n",
      "259300: done 133 games, mean reward -1.220, speed 48.89 f/s, eps 0.02\n",
      "261312: done 134 games, mean reward -0.820, speed 46.26 f/s, eps 0.02\n",
      "264033: done 135 games, mean reward -0.450, speed 43.61 f/s, eps 0.02\n",
      "265772: done 136 games, mean reward -0.040, speed 45.97 f/s, eps 0.02\n",
      "267716: done 137 games, mean reward 0.330, speed 50.16 f/s, eps 0.02\n",
      "269493: done 138 games, mean reward 0.720, speed 50.58 f/s, eps 0.02\n",
      "271192: done 139 games, mean reward 1.100, speed 49.82 f/s, eps 0.02\n",
      "273732: done 140 games, mean reward 1.460, speed 50.73 f/s, eps 0.02\n",
      "276102: done 141 games, mean reward 1.760, speed 50.02 f/s, eps 0.02\n",
      "278040: done 142 games, mean reward 2.120, speed 49.47 f/s, eps 0.02\n",
      "279974: done 143 games, mean reward 2.500, speed 50.44 f/s, eps 0.02\n",
      "282124: done 144 games, mean reward 2.810, speed 50.60 f/s, eps 0.02\n",
      "284457: done 145 games, mean reward 3.150, speed 50.47 f/s, eps 0.02\n",
      "286575: done 146 games, mean reward 3.540, speed 49.51 f/s, eps 0.02\n",
      "288629: done 147 games, mean reward 3.890, speed 47.60 f/s, eps 0.02\n",
      "290347: done 148 games, mean reward 4.270, speed 47.29 f/s, eps 0.02\n",
      "292049: done 149 games, mean reward 4.650, speed 48.20 f/s, eps 0.02\n",
      "293759: done 150 games, mean reward 5.010, speed 48.32 f/s, eps 0.02\n",
      "295665: done 151 games, mean reward 5.390, speed 50.78 f/s, eps 0.02\n",
      "297846: done 152 games, mean reward 5.730, speed 50.76 f/s, eps 0.02\n",
      "299589: done 153 games, mean reward 6.130, speed 49.75 f/s, eps 0.02\n",
      "301323: done 154 games, mean reward 6.480, speed 51.31 f/s, eps 0.02\n",
      "303101: done 155 games, mean reward 6.880, speed 50.24 f/s, eps 0.02\n",
      "304902: done 156 games, mean reward 7.220, speed 44.20 f/s, eps 0.02\n",
      "306805: done 157 games, mean reward 7.560, speed 45.42 f/s, eps 0.02\n",
      "308616: done 158 games, mean reward 7.910, speed 50.42 f/s, eps 0.02\n",
      "310523: done 159 games, mean reward 8.250, speed 43.07 f/s, eps 0.02\n",
      "312419: done 160 games, mean reward 8.600, speed 40.31 f/s, eps 0.02\n",
      "314320: done 161 games, mean reward 8.910, speed 39.64 f/s, eps 0.02\n",
      "316291: done 162 games, mean reward 9.240, speed 42.69 f/s, eps 0.02\n",
      "318035: done 163 games, mean reward 9.610, speed 39.74 f/s, eps 0.02\n",
      "320226: done 164 games, mean reward 9.950, speed 39.65 f/s, eps 0.02\n",
      "322282: done 165 games, mean reward 10.280, speed 43.05 f/s, eps 0.02\n",
      "324474: done 166 games, mean reward 10.570, speed 41.15 f/s, eps 0.02\n",
      "326353: done 167 games, mean reward 10.890, speed 43.57 f/s, eps 0.02\n",
      "328047: done 168 games, mean reward 11.270, speed 45.99 f/s, eps 0.02\n",
      "330370: done 169 games, mean reward 11.550, speed 46.90 f/s, eps 0.02\n",
      "332129: done 170 games, mean reward 11.800, speed 44.60 f/s, eps 0.02\n",
      "333853: done 171 games, mean reward 12.190, speed 48.23 f/s, eps 0.02\n",
      "335830: done 172 games, mean reward 12.480, speed 49.91 f/s, eps 0.02\n",
      "337644: done 173 games, mean reward 12.810, speed 48.17 f/s, eps 0.02\n",
      "339569: done 174 games, mean reward 13.190, speed 41.32 f/s, eps 0.02\n",
      "341586: done 175 games, mean reward 13.530, speed 41.65 f/s, eps 0.02\n",
      "343367: done 176 games, mean reward 13.890, speed 36.05 f/s, eps 0.02\n",
      "345063: done 177 games, mean reward 14.230, speed 45.90 f/s, eps 0.02\n",
      "346901: done 178 games, mean reward 14.520, speed 46.97 f/s, eps 0.02\n",
      "348604: done 179 games, mean reward 14.880, speed 46.57 f/s, eps 0.02\n",
      "350472: done 180 games, mean reward 15.190, speed 43.39 f/s, eps 0.02\n",
      "352299: done 181 games, mean reward 15.510, speed 41.32 f/s, eps 0.02\n",
      "354089: done 182 games, mean reward 15.850, speed 44.25 f/s, eps 0.02\n",
      "356347: done 183 games, mean reward 16.060, speed 43.66 f/s, eps 0.02\n",
      "358334: done 184 games, mean reward 16.230, speed 48.74 f/s, eps 0.02\n",
      "360373: done 185 games, mean reward 16.440, speed 48.29 f/s, eps 0.02\n",
      "362159: done 186 games, mean reward 16.710, speed 48.79 f/s, eps 0.02\n",
      "364511: done 187 games, mean reward 16.990, speed 44.83 f/s, eps 0.02\n",
      "366625: done 188 games, mean reward 17.210, speed 40.75 f/s, eps 0.02\n",
      "368645: done 189 games, mean reward 17.450, speed 47.56 f/s, eps 0.02\n",
      "370631: done 190 games, mean reward 17.610, speed 46.05 f/s, eps 0.02\n",
      "372651: done 191 games, mean reward 17.740, speed 41.87 f/s, eps 0.02\n",
      "374409: done 192 games, mean reward 17.820, speed 43.19 f/s, eps 0.02\n",
      "376218: done 193 games, mean reward 17.960, speed 49.36 f/s, eps 0.02\n",
      "378758: done 194 games, mean reward 18.030, speed 48.41 f/s, eps 0.02\n",
      "Solved in 378758 frames!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #input hyperparameters, check CUDA available, create environment,then we use PTAN DQN wrapper to wrap up the environment\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=True, action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"-n\", default=REWARD_STEPS_DEFAULT, type=int, help=\"Count of steps to unroll Bellman\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    env = gym.make(params['env_name'])\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    #we make a writer for the environment and action dimension\n",
    "    writer = SummaryWriter(comment=\"-\" + params['run_name'] + \"-basic\" + \"-%d-step\" % args.n)\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    #the wrapper below can create a copy of DQN network, which is target network, and constantly synchronize with online\n",
    "    #network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    #we create agent to change observation to action value, we also need action selector to choose the action we use\n",
    "    #We use epsilon greedy method as action selector here\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    \n",
    "    #experience source is from one step ExperienceSourceFirstLast and replay buffer, it will store fixed step transitions\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=args.n)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "    \n",
    "    #create optimizer and frame counter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    frame_idx = 0\n",
    "    \n",
    "    #reward tracker will report mean reward when episode end, and increase frame counter by 1, also getting a transition\n",
    "    #from frame buffer.\n",
    "    #buffer.populate(1) will activate following actions:\n",
    "    #ExperienceReplayBuffer will request for next transition from experience source.\n",
    "    #Experience source will send the observation to agent to get the action\n",
    "    #Action selector which use epsilon greedy method will choose an action based on greedy or random\n",
    "    #Action will be return to experience source and input to the environment to get reward and next observation, \n",
    "    # current observation, action, reward, next observation will be stored into replay buffer\n",
    "    #transfer information will be stored in replay buffer, and oldest observation will be dropped\n",
    "    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            epsilon_tracker.frame(frame_idx)\n",
    "            \n",
    "            #check undiscounted reward list after finishing an episode, and send to reward tracker to record the data\n",
    "            #Maybe it just play one step or didn't have finished episode, if it returns true, it means the mean reward\n",
    "            #reached the reward boundary and we can break and stop training\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                    break\n",
    "            \n",
    "            #we check buffer has cached enough data to start training or not. If not, we wait for more data.\n",
    "            if len(buffer) < params['replay_initial']:\n",
    "                continue\n",
    "            \n",
    "            #here we use Stochastic Gradient Descent(SGD) to calculate loss, zero the gradient,batch from the replay buffer\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(params['batch_size'])\n",
    "            #Discount factor for n steps is GAMMA^n\n",
    "            loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                                          gamma=params['gamma']**args.n, device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #synchronize the target network with the online network constantly\n",
    "            if frame_idx % params['target_net_sync'] == 0:\n",
    "                tgt_net.sync()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
