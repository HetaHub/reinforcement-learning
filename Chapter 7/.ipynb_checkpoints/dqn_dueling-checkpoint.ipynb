{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06b99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9207c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    #convolution layer same as before\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #we make 2 conversion instead of 1 in fully connected layer, one for action advantages, one for state value \n",
    "        #prediction\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    #we calculate the sample value and action advantage and add them together, then minus the advantage mean and get the \n",
    "    #final q-value\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        val = self.fc_val(conv_out)\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        return val + adv - adv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ad9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859: done 1 games, mean reward -21.000, speed 194.19 f/s, eps 0.99\n",
      "1798: done 2 games, mean reward -21.000, speed 223.57 f/s, eps 0.98\n",
      "2559: done 3 games, mean reward -21.000, speed 207.74 f/s, eps 0.97\n",
      "3379: done 4 games, mean reward -21.000, speed 158.02 f/s, eps 0.97\n",
      "4477: done 5 games, mean reward -20.800, speed 153.94 f/s, eps 0.96\n",
      "5512: done 6 games, mean reward -20.500, speed 154.17 f/s, eps 0.94\n",
      "6425: done 7 games, mean reward -20.429, speed 156.81 f/s, eps 0.94\n",
      "7419: done 8 games, mean reward -20.375, speed 157.00 f/s, eps 0.93\n",
      "8624: done 9 games, mean reward -20.111, speed 156.81 f/s, eps 0.91\n",
      "9446: done 10 games, mean reward -20.200, speed 156.51 f/s, eps 0.91\n",
      "10265: done 11 games, mean reward -20.273, speed 90.83 f/s, eps 0.90\n",
      "11087: done 12 games, mean reward -20.333, speed 50.00 f/s, eps 0.89\n",
      "11850: done 13 games, mean reward -20.385, speed 47.62 f/s, eps 0.88\n",
      "12700: done 14 games, mean reward -20.429, speed 47.90 f/s, eps 0.87\n",
      "13530: done 15 games, mean reward -20.467, speed 48.14 f/s, eps 0.86\n",
      "14669: done 16 games, mean reward -20.375, speed 48.01 f/s, eps 0.85\n",
      "15673: done 17 games, mean reward -20.353, speed 47.68 f/s, eps 0.84\n",
      "16597: done 18 games, mean reward -20.333, speed 47.51 f/s, eps 0.83\n",
      "17586: done 19 games, mean reward -20.316, speed 47.79 f/s, eps 0.82\n",
      "18649: done 20 games, mean reward -20.250, speed 47.68 f/s, eps 0.81\n",
      "19738: done 21 games, mean reward -20.143, speed 47.96 f/s, eps 0.80\n",
      "20616: done 22 games, mean reward -20.182, speed 47.65 f/s, eps 0.79\n",
      "21679: done 23 games, mean reward -20.174, speed 47.54 f/s, eps 0.78\n",
      "22438: done 24 games, mean reward -20.208, speed 47.23 f/s, eps 0.78\n",
      "23252: done 25 games, mean reward -20.240, speed 47.67 f/s, eps 0.77\n",
      "24181: done 26 games, mean reward -20.269, speed 47.34 f/s, eps 0.76\n",
      "25078: done 27 games, mean reward -20.296, speed 47.19 f/s, eps 0.75\n",
      "25978: done 28 games, mean reward -20.286, speed 47.55 f/s, eps 0.74\n",
      "27165: done 29 games, mean reward -20.207, speed 47.75 f/s, eps 0.73\n",
      "28285: done 30 games, mean reward -20.133, speed 47.90 f/s, eps 0.72\n",
      "29204: done 31 games, mean reward -20.129, speed 47.73 f/s, eps 0.71\n",
      "30117: done 32 games, mean reward -20.156, speed 47.68 f/s, eps 0.70\n",
      "31076: done 33 games, mean reward -20.121, speed 47.44 f/s, eps 0.69\n",
      "32152: done 34 games, mean reward -20.088, speed 47.22 f/s, eps 0.68\n",
      "33212: done 35 games, mean reward -20.086, speed 47.29 f/s, eps 0.67\n",
      "34232: done 36 games, mean reward -20.056, speed 47.18 f/s, eps 0.66\n",
      "35100: done 37 games, mean reward -20.081, speed 47.99 f/s, eps 0.65\n",
      "36357: done 38 games, mean reward -20.000, speed 47.75 f/s, eps 0.64\n",
      "37380: done 39 games, mean reward -19.974, speed 47.20 f/s, eps 0.63\n",
      "38491: done 40 games, mean reward -19.950, speed 47.69 f/s, eps 0.62\n",
      "39774: done 41 games, mean reward -19.951, speed 47.30 f/s, eps 0.60\n",
      "41044: done 42 games, mean reward -19.905, speed 47.20 f/s, eps 0.59\n",
      "42052: done 43 games, mean reward -19.907, speed 47.63 f/s, eps 0.58\n",
      "43766: done 44 games, mean reward -19.818, speed 47.47 f/s, eps 0.56\n",
      "45139: done 45 games, mean reward -19.800, speed 47.15 f/s, eps 0.55\n",
      "46252: done 46 games, mean reward -19.783, speed 47.64 f/s, eps 0.54\n",
      "47871: done 47 games, mean reward -19.745, speed 47.49 f/s, eps 0.52\n",
      "49250: done 48 games, mean reward -19.667, speed 46.98 f/s, eps 0.51\n",
      "50563: done 49 games, mean reward -19.612, speed 47.30 f/s, eps 0.49\n",
      "52153: done 50 games, mean reward -19.560, speed 47.25 f/s, eps 0.48\n",
      "53674: done 51 games, mean reward -19.451, speed 47.23 f/s, eps 0.46\n",
      "55676: done 52 games, mean reward -19.346, speed 47.02 f/s, eps 0.44\n",
      "57165: done 53 games, mean reward -19.358, speed 46.70 f/s, eps 0.43\n",
      "58822: done 54 games, mean reward -19.315, speed 47.51 f/s, eps 0.41\n",
      "60399: done 55 games, mean reward -19.291, speed 47.17 f/s, eps 0.40\n",
      "61744: done 56 games, mean reward -19.250, speed 47.18 f/s, eps 0.38\n",
      "63550: done 57 games, mean reward -19.158, speed 47.21 f/s, eps 0.36\n",
      "65509: done 58 games, mean reward -19.069, speed 47.16 f/s, eps 0.34\n",
      "67232: done 59 games, mean reward -19.034, speed 46.97 f/s, eps 0.33\n",
      "69133: done 60 games, mean reward -18.983, speed 47.17 f/s, eps 0.31\n",
      "71288: done 61 games, mean reward -18.902, speed 46.99 f/s, eps 0.29\n",
      "73453: done 62 games, mean reward -18.790, speed 47.17 f/s, eps 0.27\n",
      "76712: done 63 games, mean reward -18.556, speed 47.29 f/s, eps 0.23\n",
      "78413: done 64 games, mean reward -18.547, speed 47.07 f/s, eps 0.22\n",
      "80055: done 65 games, mean reward -18.585, speed 47.11 f/s, eps 0.20\n",
      "81841: done 66 games, mean reward -18.530, speed 47.27 f/s, eps 0.18\n",
      "83676: done 67 games, mean reward -18.493, speed 46.59 f/s, eps 0.16\n",
      "86502: done 68 games, mean reward -18.368, speed 47.26 f/s, eps 0.13\n",
      "88857: done 69 games, mean reward -18.304, speed 46.81 f/s, eps 0.11\n",
      "90840: done 70 games, mean reward -18.271, speed 47.16 f/s, eps 0.09\n",
      "92865: done 71 games, mean reward -18.225, speed 47.20 f/s, eps 0.07\n",
      "94652: done 72 games, mean reward -18.208, speed 46.88 f/s, eps 0.05\n",
      "96950: done 73 games, mean reward -18.123, speed 47.02 f/s, eps 0.03\n",
      "98862: done 74 games, mean reward -18.081, speed 47.04 f/s, eps 0.02\n",
      "100454: done 75 games, mean reward -18.080, speed 46.34 f/s, eps 0.02\n",
      "104221: done 76 games, mean reward -17.934, speed 46.71 f/s, eps 0.02\n",
      "106905: done 77 games, mean reward -17.844, speed 47.18 f/s, eps 0.02\n",
      "108821: done 78 games, mean reward -17.833, speed 45.68 f/s, eps 0.02\n",
      "110459: done 79 games, mean reward -17.823, speed 46.52 f/s, eps 0.02\n",
      "113922: done 80 games, mean reward -17.750, speed 43.25 f/s, eps 0.02\n",
      "116418: done 81 games, mean reward -17.704, speed 43.04 f/s, eps 0.02\n",
      "117910: done 82 games, mean reward -17.732, speed 42.49 f/s, eps 0.02\n",
      "120986: done 83 games, mean reward -17.675, speed 42.96 f/s, eps 0.02\n",
      "123521: done 84 games, mean reward -17.607, speed 42.78 f/s, eps 0.02\n",
      "125278: done 85 games, mean reward -17.600, speed 42.43 f/s, eps 0.02\n",
      "128023: done 86 games, mean reward -17.465, speed 42.65 f/s, eps 0.02\n",
      "129894: done 87 games, mean reward -17.460, speed 43.34 f/s, eps 0.02\n",
      "133140: done 88 games, mean reward -17.330, speed 44.83 f/s, eps 0.02\n",
      "136186: done 89 games, mean reward -17.180, speed 43.40 f/s, eps 0.02\n",
      "138015: done 90 games, mean reward -17.167, speed 42.57 f/s, eps 0.02\n",
      "140159: done 91 games, mean reward -17.165, speed 40.71 f/s, eps 0.02\n",
      "143207: done 92 games, mean reward -17.098, speed 43.23 f/s, eps 0.02\n",
      "145656: done 93 games, mean reward -17.022, speed 42.41 f/s, eps 0.02\n",
      "148456: done 94 games, mean reward -16.904, speed 43.57 f/s, eps 0.02\n",
      "150649: done 95 games, mean reward -16.916, speed 44.56 f/s, eps 0.02\n",
      "154600: done 96 games, mean reward -16.719, speed 45.05 f/s, eps 0.02\n",
      "158190: done 97 games, mean reward -16.536, speed 45.28 f/s, eps 0.02\n",
      "160679: done 98 games, mean reward -16.531, speed 44.35 f/s, eps 0.02\n",
      "163841: done 99 games, mean reward -16.505, speed 42.65 f/s, eps 0.02\n",
      "167416: done 100 games, mean reward -16.420, speed 42.90 f/s, eps 0.02\n",
      "170481: done 101 games, mean reward -16.130, speed 42.81 f/s, eps 0.02\n",
      "173546: done 102 games, mean reward -15.810, speed 46.47 f/s, eps 0.02\n",
      "176707: done 103 games, mean reward -15.550, speed 45.19 f/s, eps 0.02\n",
      "179987: done 104 games, mean reward -15.430, speed 44.06 f/s, eps 0.02\n",
      "182739: done 105 games, mean reward -15.110, speed 41.91 f/s, eps 0.02\n",
      "185292: done 106 games, mean reward -15.080, speed 43.88 f/s, eps 0.02\n",
      "188070: done 107 games, mean reward -14.990, speed 45.66 f/s, eps 0.02\n",
      "190750: done 108 games, mean reward -14.920, speed 45.26 f/s, eps 0.02\n",
      "193035: done 109 games, mean reward -14.610, speed 45.65 f/s, eps 0.02\n",
      "194840: done 110 games, mean reward -14.200, speed 46.61 f/s, eps 0.02\n",
      "197689: done 111 games, mean reward -14.120, speed 43.05 f/s, eps 0.02\n",
      "201153: done 112 games, mean reward -13.800, speed 45.66 f/s, eps 0.02\n",
      "203490: done 113 games, mean reward -13.710, speed 45.30 f/s, eps 0.02\n",
      "206619: done 114 games, mean reward -13.600, speed 45.42 f/s, eps 0.02\n",
      "210709: done 115 games, mean reward -13.360, speed 43.15 f/s, eps 0.02\n",
      "214982: done 116 games, mean reward -13.160, speed 44.07 f/s, eps 0.02\n",
      "219472: done 117 games, mean reward -12.930, speed 43.21 f/s, eps 0.02\n",
      "222667: done 118 games, mean reward -12.670, speed 43.44 f/s, eps 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225096: done 119 games, mean reward -12.350, speed 43.57 f/s, eps 0.02\n",
      "227310: done 120 games, mean reward -11.990, speed 44.01 f/s, eps 0.02\n",
      "229410: done 121 games, mean reward -11.660, speed 43.25 f/s, eps 0.02\n",
      "232495: done 122 games, mean reward -11.350, speed 44.64 f/s, eps 0.02\n",
      "234399: done 123 games, mean reward -10.970, speed 44.60 f/s, eps 0.02\n",
      "236573: done 124 games, mean reward -10.600, speed 42.77 f/s, eps 0.02\n",
      "239437: done 125 games, mean reward -10.250, speed 42.88 f/s, eps 0.02\n",
      "241655: done 126 games, mean reward -9.880, speed 42.25 f/s, eps 0.02\n",
      "243925: done 127 games, mean reward -9.510, speed 42.10 f/s, eps 0.02\n",
      "246711: done 128 games, mean reward -9.180, speed 44.20 f/s, eps 0.02\n",
      "248864: done 129 games, mean reward -8.870, speed 42.94 f/s, eps 0.02\n",
      "251703: done 130 games, mean reward -8.550, speed 43.82 f/s, eps 0.02\n",
      "253611: done 131 games, mean reward -8.180, speed 43.81 f/s, eps 0.02\n",
      "256019: done 132 games, mean reward -7.830, speed 43.19 f/s, eps 0.02\n",
      "257682: done 133 games, mean reward -7.440, speed 43.47 f/s, eps 0.02\n",
      "259644: done 134 games, mean reward -7.060, speed 43.93 f/s, eps 0.02\n",
      "261404: done 135 games, mean reward -6.660, speed 44.04 f/s, eps 0.02\n",
      "263570: done 136 games, mean reward -6.290, speed 43.31 f/s, eps 0.02\n",
      "265425: done 137 games, mean reward -5.890, speed 43.65 f/s, eps 0.02\n",
      "267411: done 138 games, mean reward -5.550, speed 44.00 f/s, eps 0.02\n",
      "269590: done 139 games, mean reward -5.190, speed 45.38 f/s, eps 0.02\n",
      "271357: done 140 games, mean reward -4.800, speed 46.74 f/s, eps 0.02\n",
      "273464: done 141 games, mean reward -4.430, speed 46.73 f/s, eps 0.02\n",
      "275157: done 142 games, mean reward -4.050, speed 43.01 f/s, eps 0.02\n",
      "277365: done 143 games, mean reward -3.700, speed 44.75 f/s, eps 0.02\n",
      "279079: done 144 games, mean reward -3.350, speed 43.55 f/s, eps 0.02\n",
      "281525: done 145 games, mean reward -3.010, speed 43.86 f/s, eps 0.02\n",
      "283509: done 146 games, mean reward -2.620, speed 42.89 f/s, eps 0.02\n",
      "285248: done 147 games, mean reward -2.250, speed 43.91 f/s, eps 0.02\n",
      "287120: done 148 games, mean reward -1.910, speed 42.78 f/s, eps 0.02\n",
      "288961: done 149 games, mean reward -1.550, speed 43.48 f/s, eps 0.02\n",
      "290620: done 150 games, mean reward -1.180, speed 43.22 f/s, eps 0.02\n",
      "292538: done 151 games, mean reward -0.880, speed 43.88 f/s, eps 0.02\n",
      "294500: done 152 games, mean reward -0.550, speed 43.74 f/s, eps 0.02\n",
      "296263: done 153 games, mean reward -0.150, speed 43.71 f/s, eps 0.02\n",
      "298098: done 154 games, mean reward 0.190, speed 43.47 f/s, eps 0.02\n",
      "300095: done 155 games, mean reward 0.540, speed 43.81 f/s, eps 0.02\n",
      "302066: done 156 games, mean reward 0.880, speed 43.79 f/s, eps 0.02\n",
      "303972: done 157 games, mean reward 1.200, speed 43.52 f/s, eps 0.02\n",
      "305666: done 158 games, mean reward 1.550, speed 42.36 f/s, eps 0.02\n",
      "307747: done 159 games, mean reward 1.840, speed 43.90 f/s, eps 0.02\n",
      "309569: done 160 games, mean reward 2.190, speed 43.98 f/s, eps 0.02\n",
      "311306: done 161 games, mean reward 2.530, speed 43.44 f/s, eps 0.02\n",
      "313195: done 162 games, mean reward 2.830, speed 43.43 f/s, eps 0.02\n",
      "314846: done 163 games, mean reward 3.080, speed 43.63 f/s, eps 0.02\n",
      "316638: done 164 games, mean reward 3.450, speed 43.29 f/s, eps 0.02\n",
      "318498: done 165 games, mean reward 3.860, speed 44.01 f/s, eps 0.02\n",
      "320457: done 166 games, mean reward 4.180, speed 43.34 f/s, eps 0.02\n",
      "322323: done 167 games, mean reward 4.540, speed 43.68 f/s, eps 0.02\n",
      "324479: done 168 games, mean reward 4.840, speed 43.79 f/s, eps 0.02\n",
      "326329: done 169 games, mean reward 5.150, speed 43.77 f/s, eps 0.02\n",
      "327997: done 170 games, mean reward 5.510, speed 43.13 f/s, eps 0.02\n",
      "329872: done 171 games, mean reward 5.860, speed 43.49 f/s, eps 0.02\n",
      "331533: done 172 games, mean reward 6.230, speed 42.45 f/s, eps 0.02\n",
      "333216: done 173 games, mean reward 6.560, speed 43.85 f/s, eps 0.02\n",
      "334940: done 174 games, mean reward 6.910, speed 43.28 f/s, eps 0.02\n",
      "337028: done 175 games, mean reward 7.290, speed 44.45 f/s, eps 0.02\n",
      "339153: done 176 games, mean reward 7.530, speed 45.75 f/s, eps 0.02\n",
      "340908: done 177 games, mean reward 7.840, speed 46.03 f/s, eps 0.02\n",
      "343101: done 178 games, mean reward 8.120, speed 46.14 f/s, eps 0.02\n",
      "345279: done 179 games, mean reward 8.400, speed 46.03 f/s, eps 0.02\n",
      "347142: done 180 games, mean reward 8.720, speed 46.38 f/s, eps 0.02\n",
      "348809: done 181 games, mean reward 9.060, speed 45.66 f/s, eps 0.02\n",
      "350473: done 182 games, mean reward 9.460, speed 46.06 f/s, eps 0.02\n",
      "352218: done 183 games, mean reward 9.790, speed 46.04 f/s, eps 0.02\n",
      "354259: done 184 games, mean reward 10.100, speed 45.53 f/s, eps 0.02\n",
      "356459: done 185 games, mean reward 10.440, speed 46.07 f/s, eps 0.02\n",
      "358122: done 186 games, mean reward 10.700, speed 45.84 f/s, eps 0.02\n",
      "359819: done 187 games, mean reward 11.070, speed 46.24 f/s, eps 0.02\n",
      "361789: done 188 games, mean reward 11.300, speed 46.06 f/s, eps 0.02\n",
      "363600: done 189 games, mean reward 11.550, speed 45.98 f/s, eps 0.02\n",
      "365389: done 190 games, mean reward 11.900, speed 46.00 f/s, eps 0.02\n",
      "367131: done 191 games, mean reward 12.270, speed 45.94 f/s, eps 0.02\n",
      "368820: done 192 games, mean reward 12.590, speed 45.93 f/s, eps 0.02\n",
      "370686: done 193 games, mean reward 12.890, speed 45.73 f/s, eps 0.02\n",
      "372469: done 194 games, mean reward 13.150, speed 45.62 f/s, eps 0.02\n",
      "374449: done 195 games, mean reward 13.520, speed 42.70 f/s, eps 0.02\n",
      "376264: done 196 games, mean reward 13.690, speed 42.74 f/s, eps 0.02\n",
      "378447: done 197 games, mean reward 13.840, speed 42.75 f/s, eps 0.02\n",
      "380317: done 198 games, mean reward 14.180, speed 42.53 f/s, eps 0.02\n",
      "382111: done 199 games, mean reward 14.510, speed 42.99 f/s, eps 0.02\n",
      "383900: done 200 games, mean reward 14.780, speed 42.22 f/s, eps 0.02\n",
      "385920: done 201 games, mean reward 14.850, speed 42.35 f/s, eps 0.02\n",
      "387632: done 202 games, mean reward 14.930, speed 43.09 f/s, eps 0.02\n",
      "389353: done 203 games, mean reward 15.080, speed 42.53 f/s, eps 0.02\n",
      "391656: done 204 games, mean reward 15.330, speed 42.88 f/s, eps 0.02\n",
      "393479: done 205 games, mean reward 15.380, speed 41.73 f/s, eps 0.02\n",
      "395143: done 206 games, mean reward 15.740, speed 42.06 f/s, eps 0.02\n",
      "396934: done 207 games, mean reward 16.050, speed 43.27 f/s, eps 0.02\n",
      "398719: done 208 games, mean reward 16.380, speed 42.90 f/s, eps 0.02\n",
      "400380: done 209 games, mean reward 16.450, speed 43.87 f/s, eps 0.02\n",
      "402189: done 210 games, mean reward 16.430, speed 42.73 f/s, eps 0.02\n",
      "404234: done 211 games, mean reward 16.740, speed 43.27 f/s, eps 0.02\n",
      "405978: done 212 games, mean reward 16.830, speed 43.38 f/s, eps 0.02\n",
      "407624: done 213 games, mean reward 17.160, speed 43.70 f/s, eps 0.02\n",
      "409493: done 214 games, mean reward 17.450, speed 44.18 f/s, eps 0.02\n",
      "411388: done 215 games, mean reward 17.600, speed 42.26 f/s, eps 0.02\n",
      "413171: done 216 games, mean reward 17.780, speed 42.17 f/s, eps 0.02\n",
      "415082: done 217 games, mean reward 17.930, speed 41.77 f/s, eps 0.02\n",
      "416763: done 218 games, mean reward 18.070, speed 43.20 f/s, eps 0.02\n",
      "Solved in 416763 frames!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #input hyperparameters, check CUDA available, create environment,then we use PTAN DQN wrapper to wrap up the environment\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=True, action=\"store_true\", help=\"Enable cuda\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    env = gym.make(params['env_name'])\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    #we make a writer for the environment and action dimension\n",
    "    writer = SummaryWriter(comment=\"-\" + params['run_name'] + \"-dueling\")\n",
    "    net = DuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    #the wrapper below can create a copy of DQN network, which is target network, and constantly synchronize with online\n",
    "    #network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    #we create agent to change observation to action value, we also need action selector to choose the action we use\n",
    "    #We use epsilon greedy method as action selector here\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    \n",
    "    #experience source is from one step ExperienceSourceFirstLast and replay buffer, it will store fixed step transitions\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "    \n",
    "    #create optimizer and frame counter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    frame_idx = 0\n",
    "    \n",
    "    #reward tracker will report mean reward when episode end, and increase frame counter by 1, also getting a transition\n",
    "    #from frame buffer.\n",
    "    #buffer.populate(1) will activate following actions:\n",
    "    #ExperienceReplayBuffer will request for next transition from experience source.\n",
    "    #Experience source will send the observation to agent to get the action\n",
    "    #Action selector which use epsilon greedy method will choose an action based on greedy or random\n",
    "    #Action will be return to experience source and input to the environment to get reward and next observation, \n",
    "    # current observation, action, reward, next observation will be stored into replay buffer\n",
    "    #transfer information will be stored in replay buffer, and oldest observation will be dropped\n",
    "    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            epsilon_tracker.frame(frame_idx)\n",
    "            \n",
    "            #check undiscounted reward list after finishing an episode, and send to reward tracker to record the data\n",
    "            #Maybe it just play one step or didn't have finished episode, if it returns true, it means the mean reward\n",
    "            #reached the reward boundary and we can break and stop training\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                    break\n",
    "            \n",
    "            #we check buffer has cached enough data to start training or not. If not, we wait for more data.\n",
    "            if len(buffer) < params['replay_initial']:\n",
    "                continue\n",
    "            \n",
    "            #here we use Stochastic Gradient Descent(SGD) to calculate loss, zero the gradient,batch from the replay buffer\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(params['batch_size'])\n",
    "            loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params['gamma'], device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #synchronize the target network with the online network constantly\n",
    "            if frame_idx % params['target_net_sync'] == 0:\n",
    "                tgt_net.sync()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
