{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1349851e",
   "metadata": {},
   "source": [
    "- Reinforce method need short episode like in CartPole, in Pong, it will converge very slow. And we sample a lots will just have a few successful samples.\n",
    "- We need the Q(s,a) and V(s) for each step q-value estimation, but in policy gradient, we just have the distribution but not the above value, so we need to estimate the above value with Actor-Critic method. We can also extend n steps for estimation, further steps has few contributions because discounted value is big.\n",
    "- In cartpole, if we have a successful game with 100 steps holding the pole, the reward will be 100. When compared to reward 5 with 5 steps, the total reward have very large difference, which makes one lucky episode will occupied the most at the final gradient, and this affect our training, because variance is large, so we need q to minus baseline value, which can be the mean discounted value or moving average or discounted value or the state value V(s).\n",
    "- We have entropy bonus introduced here, which means the uncertainty of an action, the math equation stated as:                H(π) = -Σπ(a|s)logπ(a|s), the entropy will have a maximum value, when all policy has same probabilities, the entropy will be larger than 0 and will be maximum, which means our agent is unsure which action to choose, otherwise, if 1 action has value 1 and others has 0, it means the agent is clearly what it is doing, therefore the entropy will change to minimum. To avoid local minimum problem, we use loss - entropy to punish agent over sure what it should be done.\n",
    "- In DQN, we use replay buffer with different sample to make the i.i.d requirement holds, but here we can't use replay buffer, because we are using on-policy method, we can't save the old policy data to buffer and sample it, we need the newest data to current action. Therefore, we use parallel environment, which means we will interact with multiple environment instead of 1 in the same action and sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6edf906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ed8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "#entropy_beta is the resize ratio value of entropy bonus\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "#Reward steps is the number of steps that Bellman equation extend steps, it is used to calculate discounted total reward\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa93809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    #network structure same as before, 128 neurons layer\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3239a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "16: reward:  15.00, mean_100:  15.00, episodes: 1\n",
      "28: reward:  11.00, mean_100:  13.00, episodes: 2\n",
      "48: reward:  19.00, mean_100:  15.00, episodes: 3\n",
      "57: reward:   9.00, mean_100:  13.50, episodes: 4\n",
      "72: reward:  14.00, mean_100:  13.60, episodes: 5\n",
      "95: reward:  22.00, mean_100:  15.00, episodes: 6\n",
      "147: reward:  51.00, mean_100:  20.14, episodes: 7\n",
      "159: reward:  11.00, mean_100:  19.00, episodes: 8\n",
      "235: reward:  75.00, mean_100:  25.22, episodes: 9\n",
      "282: reward:  46.00, mean_100:  27.30, episodes: 10\n",
      "311: reward:  28.00, mean_100:  27.36, episodes: 11\n",
      "342: reward:  30.00, mean_100:  27.58, episodes: 12\n",
      "361: reward:  18.00, mean_100:  26.85, episodes: 13\n",
      "394: reward:  32.00, mean_100:  27.21, episodes: 14\n",
      "419: reward:  24.00, mean_100:  27.00, episodes: 15\n",
      "433: reward:  13.00, mean_100:  26.12, episodes: 16\n",
      "468: reward:  34.00, mean_100:  26.59, episodes: 17\n",
      "493: reward:  24.00, mean_100:  26.44, episodes: 18\n",
      "546: reward:  52.00, mean_100:  27.79, episodes: 19\n",
      "574: reward:  27.00, mean_100:  27.75, episodes: 20\n",
      "596: reward:  21.00, mean_100:  27.43, episodes: 21\n",
      "612: reward:  15.00, mean_100:  26.86, episodes: 22\n",
      "742: reward: 129.00, mean_100:  31.30, episodes: 23\n",
      "763: reward:  20.00, mean_100:  30.83, episodes: 24\n",
      "785: reward:  21.00, mean_100:  30.44, episodes: 25\n",
      "842: reward:  56.00, mean_100:  31.42, episodes: 26\n",
      "872: reward:  29.00, mean_100:  31.33, episodes: 27\n",
      "882: reward:  10.00, mean_100:  30.57, episodes: 28\n",
      "898: reward:  15.00, mean_100:  30.03, episodes: 29\n",
      "942: reward:  43.00, mean_100:  30.47, episodes: 30\n",
      "966: reward:  23.00, mean_100:  30.23, episodes: 31\n",
      "990: reward:  23.00, mean_100:  30.00, episodes: 32\n",
      "1026: reward:  35.00, mean_100:  30.15, episodes: 33\n",
      "1078: reward:  51.00, mean_100:  30.76, episodes: 34\n",
      "1086: reward:   8.00, mean_100:  30.11, episodes: 35\n",
      "1107: reward:  20.00, mean_100:  29.83, episodes: 36\n",
      "1194: reward:  86.00, mean_100:  31.35, episodes: 37\n",
      "1223: reward:  28.00, mean_100:  31.26, episodes: 38\n",
      "1276: reward:  52.00, mean_100:  31.79, episodes: 39\n",
      "1301: reward:  24.00, mean_100:  31.60, episodes: 40\n",
      "1319: reward:  17.00, mean_100:  31.24, episodes: 41\n",
      "1346: reward:  26.00, mean_100:  31.12, episodes: 42\n",
      "1376: reward:  29.00, mean_100:  31.07, episodes: 43\n",
      "1420: reward:  43.00, mean_100:  31.34, episodes: 44\n",
      "1463: reward:  42.00, mean_100:  31.58, episodes: 45\n",
      "1484: reward:  20.00, mean_100:  31.33, episodes: 46\n",
      "1528: reward:  43.00, mean_100:  31.57, episodes: 47\n",
      "1578: reward:  49.00, mean_100:  31.94, episodes: 48\n",
      "1595: reward:  16.00, mean_100:  31.61, episodes: 49\n",
      "1642: reward:  46.00, mean_100:  31.90, episodes: 50\n",
      "1667: reward:  24.00, mean_100:  31.75, episodes: 51\n",
      "1689: reward:  21.00, mean_100:  31.54, episodes: 52\n",
      "1798: reward: 108.00, mean_100:  32.98, episodes: 53\n",
      "1835: reward:  36.00, mean_100:  33.04, episodes: 54\n",
      "1871: reward:  35.00, mean_100:  33.07, episodes: 55\n",
      "1912: reward:  40.00, mean_100:  33.20, episodes: 56\n",
      "1978: reward:  65.00, mean_100:  33.75, episodes: 57\n",
      "2015: reward:  36.00, mean_100:  33.79, episodes: 58\n",
      "2066: reward:  50.00, mean_100:  34.07, episodes: 59\n",
      "2078: reward:  11.00, mean_100:  33.68, episodes: 60\n",
      "2098: reward:  19.00, mean_100:  33.44, episodes: 61\n",
      "2137: reward:  38.00, mean_100:  33.52, episodes: 62\n",
      "2156: reward:  18.00, mean_100:  33.27, episodes: 63\n",
      "2191: reward:  34.00, mean_100:  33.28, episodes: 64\n",
      "2210: reward:  18.00, mean_100:  33.05, episodes: 65\n",
      "2267: reward:  56.00, mean_100:  33.39, episodes: 66\n",
      "2377: reward: 109.00, mean_100:  34.52, episodes: 67\n",
      "2453: reward:  75.00, mean_100:  35.12, episodes: 68\n",
      "2492: reward:  38.00, mean_100:  35.16, episodes: 69\n",
      "2574: reward:  81.00, mean_100:  35.81, episodes: 70\n",
      "2613: reward:  38.00, mean_100:  35.85, episodes: 71\n",
      "2680: reward:  66.00, mean_100:  36.26, episodes: 72\n",
      "2741: reward:  60.00, mean_100:  36.59, episodes: 73\n",
      "2775: reward:  33.00, mean_100:  36.54, episodes: 74\n",
      "2867: reward:  91.00, mean_100:  37.27, episodes: 75\n",
      "2899: reward:  31.00, mean_100:  37.18, episodes: 76\n",
      "2928: reward:  28.00, mean_100:  37.06, episodes: 77\n",
      "3003: reward:  74.00, mean_100:  37.54, episodes: 78\n",
      "3069: reward:  65.00, mean_100:  37.89, episodes: 79\n",
      "3110: reward:  40.00, mean_100:  37.91, episodes: 80\n",
      "3201: reward:  90.00, mean_100:  38.56, episodes: 81\n",
      "3240: reward:  38.00, mean_100:  38.55, episodes: 82\n",
      "3264: reward:  23.00, mean_100:  38.36, episodes: 83\n",
      "3341: reward:  76.00, mean_100:  38.81, episodes: 84\n",
      "3381: reward:  39.00, mean_100:  38.81, episodes: 85\n",
      "3403: reward:  21.00, mean_100:  38.60, episodes: 86\n",
      "3539: reward: 135.00, mean_100:  39.71, episodes: 87\n",
      "3591: reward:  51.00, mean_100:  39.84, episodes: 88\n",
      "3608: reward:  16.00, mean_100:  39.57, episodes: 89\n",
      "3683: reward:  74.00, mean_100:  39.96, episodes: 90\n",
      "3710: reward:  26.00, mean_100:  39.80, episodes: 91\n",
      "3813: reward: 102.00, mean_100:  40.48, episodes: 92\n",
      "3874: reward:  60.00, mean_100:  40.69, episodes: 93\n",
      "3941: reward:  66.00, mean_100:  40.96, episodes: 94\n",
      "4092: reward: 150.00, mean_100:  42.11, episodes: 95\n",
      "4188: reward:  95.00, mean_100:  42.66, episodes: 96\n",
      "4247: reward:  58.00, mean_100:  42.81, episodes: 97\n",
      "4294: reward:  46.00, mean_100:  42.85, episodes: 98\n",
      "4378: reward:  83.00, mean_100:  43.25, episodes: 99\n",
      "4498: reward: 119.00, mean_100:  44.01, episodes: 100\n",
      "4587: reward:  88.00, mean_100:  44.74, episodes: 101\n",
      "4726: reward: 138.00, mean_100:  46.01, episodes: 102\n",
      "4821: reward:  94.00, mean_100:  46.76, episodes: 103\n",
      "4872: reward:  50.00, mean_100:  47.17, episodes: 104\n",
      "5073: reward: 200.00, mean_100:  49.03, episodes: 105\n",
      "5123: reward:  49.00, mean_100:  49.30, episodes: 106\n",
      "5288: reward: 164.00, mean_100:  50.43, episodes: 107\n",
      "5336: reward:  47.00, mean_100:  50.79, episodes: 108\n",
      "5537: reward: 200.00, mean_100:  52.04, episodes: 109\n",
      "5738: reward: 200.00, mean_100:  53.58, episodes: 110\n",
      "5939: reward: 200.00, mean_100:  55.30, episodes: 111\n",
      "6004: reward:  64.00, mean_100:  55.64, episodes: 112\n",
      "6147: reward: 142.00, mean_100:  56.88, episodes: 113\n",
      "6348: reward: 200.00, mean_100:  58.56, episodes: 114\n",
      "6448: reward:  99.00, mean_100:  59.31, episodes: 115\n",
      "6607: reward: 158.00, mean_100:  60.76, episodes: 116\n",
      "6756: reward: 148.00, mean_100:  61.90, episodes: 117\n",
      "6923: reward: 166.00, mean_100:  63.32, episodes: 118\n",
      "7094: reward: 170.00, mean_100:  64.50, episodes: 119\n",
      "7260: reward: 165.00, mean_100:  65.88, episodes: 120\n",
      "7327: reward:  66.00, mean_100:  66.33, episodes: 121\n",
      "7364: reward:  36.00, mean_100:  66.54, episodes: 122\n",
      "7415: reward:  50.00, mean_100:  65.75, episodes: 123\n",
      "7554: reward: 138.00, mean_100:  66.93, episodes: 124\n",
      "7755: reward: 200.00, mean_100:  68.72, episodes: 125\n",
      "7935: reward: 179.00, mean_100:  69.95, episodes: 126\n",
      "8136: reward: 200.00, mean_100:  71.66, episodes: 127\n",
      "8309: reward: 172.00, mean_100:  73.28, episodes: 128\n",
      "8380: reward:  70.00, mean_100:  73.83, episodes: 129\n",
      "8581: reward: 200.00, mean_100:  75.40, episodes: 130\n",
      "8751: reward: 169.00, mean_100:  76.86, episodes: 131\n",
      "8952: reward: 200.00, mean_100:  78.63, episodes: 132\n",
      "9153: reward: 200.00, mean_100:  80.28, episodes: 133\n",
      "9354: reward: 200.00, mean_100:  81.77, episodes: 134\n",
      "9555: reward: 200.00, mean_100:  83.69, episodes: 135\n",
      "9669: reward: 113.00, mean_100:  84.62, episodes: 136\n",
      "9832: reward: 162.00, mean_100:  85.38, episodes: 137\n",
      "10029: reward: 196.00, mean_100:  87.06, episodes: 138\n",
      "10230: reward: 200.00, mean_100:  88.54, episodes: 139\n",
      "10361: reward: 130.00, mean_100:  89.60, episodes: 140\n",
      "10467: reward: 105.00, mean_100:  90.48, episodes: 141\n",
      "10668: reward: 200.00, mean_100:  92.22, episodes: 142\n",
      "10854: reward: 185.00, mean_100:  93.78, episodes: 143\n",
      "11055: reward: 200.00, mean_100:  95.35, episodes: 144\n",
      "11188: reward: 132.00, mean_100:  96.25, episodes: 145\n",
      "11389: reward: 200.00, mean_100:  98.05, episodes: 146\n",
      "11582: reward: 192.00, mean_100:  99.54, episodes: 147\n",
      "11783: reward: 200.00, mean_100: 101.05, episodes: 148\n",
      "11984: reward: 200.00, mean_100: 102.89, episodes: 149\n",
      "12121: reward: 136.00, mean_100: 103.79, episodes: 150\n",
      "12322: reward: 200.00, mean_100: 105.55, episodes: 151\n",
      "12523: reward: 200.00, mean_100: 107.34, episodes: 152\n",
      "12682: reward: 158.00, mean_100: 107.84, episodes: 153\n",
      "12883: reward: 200.00, mean_100: 109.48, episodes: 154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13084: reward: 200.00, mean_100: 111.13, episodes: 155\n",
      "13285: reward: 200.00, mean_100: 112.73, episodes: 156\n",
      "13486: reward: 200.00, mean_100: 114.08, episodes: 157\n",
      "13571: reward:  84.00, mean_100: 114.56, episodes: 158\n",
      "13760: reward: 188.00, mean_100: 115.94, episodes: 159\n",
      "13922: reward: 161.00, mean_100: 117.44, episodes: 160\n",
      "13966: reward:  43.00, mean_100: 117.68, episodes: 161\n",
      "14167: reward: 200.00, mean_100: 119.30, episodes: 162\n",
      "14368: reward: 200.00, mean_100: 121.12, episodes: 163\n",
      "14536: reward: 167.00, mean_100: 122.45, episodes: 164\n",
      "14737: reward: 200.00, mean_100: 124.27, episodes: 165\n",
      "14883: reward: 145.00, mean_100: 125.16, episodes: 166\n",
      "15059: reward: 175.00, mean_100: 125.82, episodes: 167\n",
      "15174: reward: 114.00, mean_100: 126.21, episodes: 168\n",
      "15327: reward: 152.00, mean_100: 127.35, episodes: 169\n",
      "15528: reward: 200.00, mean_100: 128.54, episodes: 170\n",
      "15652: reward: 123.00, mean_100: 129.39, episodes: 171\n",
      "15853: reward: 200.00, mean_100: 130.73, episodes: 172\n",
      "16054: reward: 200.00, mean_100: 132.13, episodes: 173\n",
      "16252: reward: 197.00, mean_100: 133.77, episodes: 174\n",
      "16453: reward: 200.00, mean_100: 134.86, episodes: 175\n",
      "16654: reward: 200.00, mean_100: 136.55, episodes: 176\n",
      "16855: reward: 200.00, mean_100: 138.27, episodes: 177\n",
      "17012: reward: 156.00, mean_100: 139.09, episodes: 178\n",
      "17133: reward: 120.00, mean_100: 139.64, episodes: 179\n",
      "17283: reward: 149.00, mean_100: 140.73, episodes: 180\n",
      "17458: reward: 174.00, mean_100: 141.57, episodes: 181\n",
      "17628: reward: 169.00, mean_100: 142.88, episodes: 182\n",
      "17829: reward: 200.00, mean_100: 144.65, episodes: 183\n",
      "18030: reward: 200.00, mean_100: 145.89, episodes: 184\n",
      "18224: reward: 193.00, mean_100: 147.43, episodes: 185\n",
      "18406: reward: 181.00, mean_100: 149.03, episodes: 186\n",
      "18607: reward: 200.00, mean_100: 149.68, episodes: 187\n",
      "18808: reward: 200.00, mean_100: 151.17, episodes: 188\n",
      "19009: reward: 200.00, mean_100: 153.01, episodes: 189\n",
      "19210: reward: 200.00, mean_100: 154.27, episodes: 190\n",
      "19394: reward: 183.00, mean_100: 155.84, episodes: 191\n",
      "19595: reward: 200.00, mean_100: 156.82, episodes: 192\n",
      "19772: reward: 176.00, mean_100: 157.98, episodes: 193\n",
      "19948: reward: 175.00, mean_100: 159.07, episodes: 194\n",
      "20110: reward: 161.00, mean_100: 159.18, episodes: 195\n",
      "20209: reward:  98.00, mean_100: 159.21, episodes: 196\n",
      "20316: reward: 106.00, mean_100: 159.69, episodes: 197\n",
      "20464: reward: 147.00, mean_100: 160.70, episodes: 198\n",
      "20612: reward: 147.00, mean_100: 161.34, episodes: 199\n",
      "20716: reward: 103.00, mean_100: 161.18, episodes: 200\n",
      "20801: reward:  84.00, mean_100: 161.14, episodes: 201\n",
      "21002: reward: 200.00, mean_100: 161.76, episodes: 202\n",
      "21203: reward: 200.00, mean_100: 162.82, episodes: 203\n",
      "21352: reward: 148.00, mean_100: 163.80, episodes: 204\n",
      "21553: reward: 200.00, mean_100: 163.80, episodes: 205\n",
      "21754: reward: 200.00, mean_100: 165.31, episodes: 206\n",
      "21935: reward: 180.00, mean_100: 165.47, episodes: 207\n",
      "22136: reward: 200.00, mean_100: 167.00, episodes: 208\n",
      "22260: reward: 123.00, mean_100: 166.23, episodes: 209\n",
      "22386: reward: 125.00, mean_100: 165.48, episodes: 210\n",
      "22587: reward: 200.00, mean_100: 165.48, episodes: 211\n",
      "22716: reward: 128.00, mean_100: 166.12, episodes: 212\n",
      "22917: reward: 200.00, mean_100: 166.70, episodes: 213\n",
      "23079: reward: 161.00, mean_100: 166.31, episodes: 214\n",
      "23280: reward: 200.00, mean_100: 167.32, episodes: 215\n",
      "23481: reward: 200.00, mean_100: 167.74, episodes: 216\n",
      "23682: reward: 200.00, mean_100: 168.26, episodes: 217\n",
      "23883: reward: 200.00, mean_100: 168.60, episodes: 218\n",
      "24012: reward: 128.00, mean_100: 168.18, episodes: 219\n",
      "24213: reward: 200.00, mean_100: 168.53, episodes: 220\n",
      "24414: reward: 200.00, mean_100: 169.87, episodes: 221\n",
      "24615: reward: 200.00, mean_100: 171.51, episodes: 222\n",
      "24816: reward: 200.00, mean_100: 173.01, episodes: 223\n",
      "25009: reward: 192.00, mean_100: 173.55, episodes: 224\n",
      "25153: reward: 143.00, mean_100: 172.98, episodes: 225\n",
      "25334: reward: 180.00, mean_100: 172.99, episodes: 226\n",
      "25535: reward: 200.00, mean_100: 172.99, episodes: 227\n",
      "25736: reward: 200.00, mean_100: 173.27, episodes: 228\n",
      "25937: reward: 200.00, mean_100: 174.57, episodes: 229\n",
      "26130: reward: 192.00, mean_100: 174.49, episodes: 230\n",
      "26331: reward: 200.00, mean_100: 174.80, episodes: 231\n",
      "26496: reward: 164.00, mean_100: 174.44, episodes: 232\n",
      "26697: reward: 200.00, mean_100: 174.44, episodes: 233\n",
      "26878: reward: 180.00, mean_100: 174.24, episodes: 234\n",
      "27055: reward: 176.00, mean_100: 174.00, episodes: 235\n",
      "27256: reward: 200.00, mean_100: 174.87, episodes: 236\n",
      "27423: reward: 166.00, mean_100: 174.91, episodes: 237\n",
      "27568: reward: 144.00, mean_100: 174.39, episodes: 238\n",
      "27710: reward: 141.00, mean_100: 173.80, episodes: 239\n",
      "27878: reward: 167.00, mean_100: 174.17, episodes: 240\n",
      "28037: reward: 158.00, mean_100: 174.70, episodes: 241\n",
      "28187: reward: 149.00, mean_100: 174.19, episodes: 242\n",
      "28373: reward: 185.00, mean_100: 174.19, episodes: 243\n",
      "28551: reward: 177.00, mean_100: 173.96, episodes: 244\n",
      "28752: reward: 200.00, mean_100: 174.64, episodes: 245\n",
      "28953: reward: 200.00, mean_100: 174.64, episodes: 246\n",
      "29154: reward: 200.00, mean_100: 174.72, episodes: 247\n",
      "29355: reward: 200.00, mean_100: 174.72, episodes: 248\n",
      "29556: reward: 200.00, mean_100: 174.72, episodes: 249\n",
      "29757: reward: 200.00, mean_100: 175.36, episodes: 250\n",
      "29958: reward: 200.00, mean_100: 175.36, episodes: 251\n",
      "30159: reward: 200.00, mean_100: 175.36, episodes: 252\n",
      "30360: reward: 200.00, mean_100: 175.78, episodes: 253\n",
      "30561: reward: 200.00, mean_100: 175.78, episodes: 254\n",
      "30762: reward: 200.00, mean_100: 175.78, episodes: 255\n",
      "30963: reward: 200.00, mean_100: 175.78, episodes: 256\n",
      "31164: reward: 200.00, mean_100: 175.78, episodes: 257\n",
      "31365: reward: 200.00, mean_100: 176.94, episodes: 258\n",
      "31566: reward: 200.00, mean_100: 177.06, episodes: 259\n",
      "31767: reward: 200.00, mean_100: 177.45, episodes: 260\n",
      "31963: reward: 195.00, mean_100: 178.97, episodes: 261\n",
      "32164: reward: 200.00, mean_100: 178.97, episodes: 262\n",
      "32365: reward: 200.00, mean_100: 178.97, episodes: 263\n",
      "32566: reward: 200.00, mean_100: 179.30, episodes: 264\n",
      "32767: reward: 200.00, mean_100: 179.30, episodes: 265\n",
      "32968: reward: 200.00, mean_100: 179.85, episodes: 266\n",
      "33169: reward: 200.00, mean_100: 180.10, episodes: 267\n",
      "33370: reward: 200.00, mean_100: 180.96, episodes: 268\n",
      "33571: reward: 200.00, mean_100: 181.44, episodes: 269\n",
      "33644: reward:  72.00, mean_100: 180.16, episodes: 270\n",
      "33845: reward: 200.00, mean_100: 180.93, episodes: 271\n",
      "34046: reward: 200.00, mean_100: 180.93, episodes: 272\n",
      "34247: reward: 200.00, mean_100: 180.93, episodes: 273\n",
      "34383: reward: 135.00, mean_100: 180.31, episodes: 274\n",
      "34584: reward: 200.00, mean_100: 180.31, episodes: 275\n",
      "34785: reward: 200.00, mean_100: 180.31, episodes: 276\n",
      "34986: reward: 200.00, mean_100: 180.31, episodes: 277\n",
      "35187: reward: 200.00, mean_100: 180.75, episodes: 278\n",
      "35388: reward: 200.00, mean_100: 181.55, episodes: 279\n",
      "35589: reward: 200.00, mean_100: 182.06, episodes: 280\n",
      "35790: reward: 200.00, mean_100: 182.32, episodes: 281\n",
      "35991: reward: 200.00, mean_100: 182.63, episodes: 282\n",
      "36124: reward: 132.00, mean_100: 181.95, episodes: 283\n",
      "36325: reward: 200.00, mean_100: 181.95, episodes: 284\n",
      "36526: reward: 200.00, mean_100: 182.02, episodes: 285\n",
      "36727: reward: 200.00, mean_100: 182.21, episodes: 286\n",
      "36928: reward: 200.00, mean_100: 182.21, episodes: 287\n",
      "37105: reward: 176.00, mean_100: 181.97, episodes: 288\n",
      "37306: reward: 200.00, mean_100: 181.97, episodes: 289\n",
      "37507: reward: 200.00, mean_100: 181.97, episodes: 290\n",
      "37708: reward: 200.00, mean_100: 182.14, episodes: 291\n",
      "37891: reward: 182.00, mean_100: 181.96, episodes: 292\n",
      "38092: reward: 200.00, mean_100: 182.20, episodes: 293\n",
      "38293: reward: 200.00, mean_100: 182.45, episodes: 294\n",
      "38494: reward: 200.00, mean_100: 182.84, episodes: 295\n",
      "38695: reward: 200.00, mean_100: 183.86, episodes: 296\n",
      "38896: reward: 200.00, mean_100: 184.80, episodes: 297\n",
      "39097: reward: 200.00, mean_100: 185.33, episodes: 298\n",
      "39198: reward: 100.00, mean_100: 184.86, episodes: 299\n",
      "39399: reward: 200.00, mean_100: 185.83, episodes: 300\n",
      "39600: reward: 200.00, mean_100: 186.99, episodes: 301\n",
      "39801: reward: 200.00, mean_100: 186.99, episodes: 302\n",
      "40002: reward: 200.00, mean_100: 186.99, episodes: 303\n",
      "40203: reward: 200.00, mean_100: 187.51, episodes: 304\n",
      "40404: reward: 200.00, mean_100: 187.51, episodes: 305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40539: reward: 134.00, mean_100: 186.85, episodes: 306\n",
      "40740: reward: 200.00, mean_100: 187.05, episodes: 307\n",
      "40941: reward: 200.00, mean_100: 187.05, episodes: 308\n",
      "41142: reward: 200.00, mean_100: 187.82, episodes: 309\n",
      "41313: reward: 170.00, mean_100: 188.27, episodes: 310\n",
      "41514: reward: 200.00, mean_100: 188.27, episodes: 311\n",
      "41571: reward:  56.00, mean_100: 187.55, episodes: 312\n",
      "41772: reward: 200.00, mean_100: 187.55, episodes: 313\n",
      "41973: reward: 200.00, mean_100: 187.94, episodes: 314\n",
      "42174: reward: 200.00, mean_100: 187.94, episodes: 315\n",
      "42339: reward: 164.00, mean_100: 187.58, episodes: 316\n",
      "42498: reward: 158.00, mean_100: 187.16, episodes: 317\n",
      "42699: reward: 200.00, mean_100: 187.16, episodes: 318\n",
      "42804: reward: 104.00, mean_100: 186.92, episodes: 319\n",
      "42907: reward: 102.00, mean_100: 185.94, episodes: 320\n",
      "43043: reward: 135.00, mean_100: 185.29, episodes: 321\n",
      "43244: reward: 200.00, mean_100: 185.29, episodes: 322\n",
      "43445: reward: 200.00, mean_100: 185.29, episodes: 323\n",
      "43646: reward: 200.00, mean_100: 185.37, episodes: 324\n",
      "43847: reward: 200.00, mean_100: 185.94, episodes: 325\n",
      "44048: reward: 200.00, mean_100: 186.14, episodes: 326\n",
      "44249: reward: 200.00, mean_100: 186.14, episodes: 327\n",
      "44438: reward: 188.00, mean_100: 186.02, episodes: 328\n",
      "44639: reward: 200.00, mean_100: 186.02, episodes: 329\n",
      "44818: reward: 178.00, mean_100: 185.88, episodes: 330\n",
      "45019: reward: 200.00, mean_100: 185.88, episodes: 331\n",
      "45220: reward: 200.00, mean_100: 186.24, episodes: 332\n",
      "45421: reward: 200.00, mean_100: 186.24, episodes: 333\n",
      "45622: reward: 200.00, mean_100: 186.44, episodes: 334\n",
      "45823: reward: 200.00, mean_100: 186.68, episodes: 335\n",
      "46024: reward: 200.00, mean_100: 186.68, episodes: 336\n",
      "46225: reward: 200.00, mean_100: 187.02, episodes: 337\n",
      "46426: reward: 200.00, mean_100: 187.58, episodes: 338\n",
      "46627: reward: 200.00, mean_100: 188.17, episodes: 339\n",
      "46798: reward: 170.00, mean_100: 188.20, episodes: 340\n",
      "46999: reward: 200.00, mean_100: 188.62, episodes: 341\n",
      "47194: reward: 194.00, mean_100: 189.07, episodes: 342\n",
      "47395: reward: 200.00, mean_100: 189.22, episodes: 343\n",
      "47596: reward: 200.00, mean_100: 189.45, episodes: 344\n",
      "47764: reward: 167.00, mean_100: 189.12, episodes: 345\n",
      "47965: reward: 200.00, mean_100: 189.12, episodes: 346\n",
      "48166: reward: 200.00, mean_100: 189.12, episodes: 347\n",
      "48367: reward: 200.00, mean_100: 189.12, episodes: 348\n",
      "48568: reward: 200.00, mean_100: 189.12, episodes: 349\n",
      "48759: reward: 190.00, mean_100: 189.02, episodes: 350\n",
      "48918: reward: 158.00, mean_100: 188.60, episodes: 351\n",
      "49119: reward: 200.00, mean_100: 188.60, episodes: 352\n",
      "49320: reward: 200.00, mean_100: 188.60, episodes: 353\n",
      "49521: reward: 200.00, mean_100: 188.60, episodes: 354\n",
      "49722: reward: 200.00, mean_100: 188.60, episodes: 355\n",
      "49923: reward: 200.00, mean_100: 188.60, episodes: 356\n",
      "50124: reward: 200.00, mean_100: 188.60, episodes: 357\n",
      "50325: reward: 200.00, mean_100: 188.60, episodes: 358\n",
      "50526: reward: 200.00, mean_100: 188.60, episodes: 359\n",
      "50727: reward: 200.00, mean_100: 188.60, episodes: 360\n",
      "50928: reward: 200.00, mean_100: 188.65, episodes: 361\n",
      "51129: reward: 200.00, mean_100: 188.65, episodes: 362\n",
      "51330: reward: 200.00, mean_100: 188.65, episodes: 363\n",
      "51531: reward: 200.00, mean_100: 188.65, episodes: 364\n",
      "51579: reward:  47.00, mean_100: 187.12, episodes: 365\n",
      "51780: reward: 200.00, mean_100: 187.12, episodes: 366\n",
      "51981: reward: 200.00, mean_100: 187.12, episodes: 367\n",
      "52182: reward: 200.00, mean_100: 187.12, episodes: 368\n",
      "52383: reward: 200.00, mean_100: 187.12, episodes: 369\n",
      "52584: reward: 200.00, mean_100: 188.40, episodes: 370\n",
      "52785: reward: 200.00, mean_100: 188.40, episodes: 371\n",
      "52986: reward: 200.00, mean_100: 188.40, episodes: 372\n",
      "53187: reward: 200.00, mean_100: 188.40, episodes: 373\n",
      "53388: reward: 200.00, mean_100: 189.05, episodes: 374\n",
      "53589: reward: 200.00, mean_100: 189.05, episodes: 375\n",
      "53790: reward: 200.00, mean_100: 189.05, episodes: 376\n",
      "53991: reward: 200.00, mean_100: 189.05, episodes: 377\n",
      "54192: reward: 200.00, mean_100: 189.05, episodes: 378\n",
      "54393: reward: 200.00, mean_100: 189.05, episodes: 379\n",
      "54594: reward: 200.00, mean_100: 189.05, episodes: 380\n",
      "54795: reward: 200.00, mean_100: 189.05, episodes: 381\n",
      "54996: reward: 200.00, mean_100: 189.05, episodes: 382\n",
      "55197: reward: 200.00, mean_100: 189.73, episodes: 383\n",
      "55398: reward: 200.00, mean_100: 189.73, episodes: 384\n",
      "55599: reward: 200.00, mean_100: 189.73, episodes: 385\n",
      "55800: reward: 200.00, mean_100: 189.73, episodes: 386\n",
      "56001: reward: 200.00, mean_100: 189.73, episodes: 387\n",
      "56202: reward: 200.00, mean_100: 189.97, episodes: 388\n",
      "56403: reward: 200.00, mean_100: 189.97, episodes: 389\n",
      "56604: reward: 200.00, mean_100: 189.97, episodes: 390\n",
      "56805: reward: 200.00, mean_100: 189.97, episodes: 391\n",
      "57006: reward: 200.00, mean_100: 190.15, episodes: 392\n",
      "57207: reward: 200.00, mean_100: 190.15, episodes: 393\n",
      "57408: reward: 200.00, mean_100: 190.15, episodes: 394\n",
      "57609: reward: 200.00, mean_100: 190.15, episodes: 395\n",
      "57810: reward: 200.00, mean_100: 190.15, episodes: 396\n",
      "58011: reward: 200.00, mean_100: 190.15, episodes: 397\n",
      "58212: reward: 200.00, mean_100: 190.15, episodes: 398\n",
      "58413: reward: 200.00, mean_100: 191.15, episodes: 399\n",
      "58614: reward: 200.00, mean_100: 191.15, episodes: 400\n",
      "58815: reward: 200.00, mean_100: 191.15, episodes: 401\n",
      "59016: reward: 200.00, mean_100: 191.15, episodes: 402\n",
      "59052: reward:  35.00, mean_100: 189.50, episodes: 403\n",
      "59200: reward: 147.00, mean_100: 188.97, episodes: 404\n",
      "59401: reward: 200.00, mean_100: 188.97, episodes: 405\n",
      "59602: reward: 200.00, mean_100: 189.63, episodes: 406\n",
      "59803: reward: 200.00, mean_100: 189.63, episodes: 407\n",
      "60004: reward: 200.00, mean_100: 189.63, episodes: 408\n",
      "60205: reward: 200.00, mean_100: 189.63, episodes: 409\n",
      "60406: reward: 200.00, mean_100: 189.93, episodes: 410\n",
      "60607: reward: 200.00, mean_100: 189.93, episodes: 411\n",
      "60808: reward: 200.00, mean_100: 191.37, episodes: 412\n",
      "61009: reward: 200.00, mean_100: 191.37, episodes: 413\n",
      "61210: reward: 200.00, mean_100: 191.37, episodes: 414\n",
      "61411: reward: 200.00, mean_100: 191.37, episodes: 415\n",
      "61612: reward: 200.00, mean_100: 191.73, episodes: 416\n",
      "61813: reward: 200.00, mean_100: 192.15, episodes: 417\n",
      "62014: reward: 200.00, mean_100: 192.15, episodes: 418\n",
      "62215: reward: 200.00, mean_100: 193.11, episodes: 419\n",
      "62416: reward: 200.00, mean_100: 194.09, episodes: 420\n",
      "62617: reward: 200.00, mean_100: 194.74, episodes: 421\n",
      "62802: reward: 184.00, mean_100: 194.58, episodes: 422\n",
      "63003: reward: 200.00, mean_100: 194.58, episodes: 423\n",
      "63204: reward: 200.00, mean_100: 194.58, episodes: 424\n",
      "63405: reward: 200.00, mean_100: 194.58, episodes: 425\n",
      "63606: reward: 200.00, mean_100: 194.58, episodes: 426\n",
      "63807: reward: 200.00, mean_100: 194.58, episodes: 427\n",
      "64008: reward: 200.00, mean_100: 194.70, episodes: 428\n",
      "64209: reward: 200.00, mean_100: 194.70, episodes: 429\n",
      "64410: reward: 200.00, mean_100: 194.92, episodes: 430\n",
      "64611: reward: 200.00, mean_100: 194.92, episodes: 431\n",
      "64812: reward: 200.00, mean_100: 194.92, episodes: 432\n",
      "65013: reward: 200.00, mean_100: 194.92, episodes: 433\n",
      "65214: reward: 200.00, mean_100: 194.92, episodes: 434\n",
      "65415: reward: 200.00, mean_100: 194.92, episodes: 435\n",
      "65616: reward: 200.00, mean_100: 194.92, episodes: 436\n",
      "65817: reward: 200.00, mean_100: 194.92, episodes: 437\n",
      "66018: reward: 200.00, mean_100: 194.92, episodes: 438\n",
      "66219: reward: 200.00, mean_100: 194.92, episodes: 439\n",
      "66420: reward: 200.00, mean_100: 195.22, episodes: 440\n",
      "Solved in 66420 steps and 440 episodes!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    writer = SummaryWriter(comment=\"-cartpole-pg\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    #extend Bellman equation for 10 steps from experience source.\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    reward_sum = 0.0\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "\n",
    "    #record the total discounted reward in training, and use it to calculate the baseline of policy gradient\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        reward_sum += exp.reward\n",
    "        baseline = reward_sum / (step_idx + 1)\n",
    "        writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if len(batch_states) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_scale_v = torch.FloatTensor(batch_scales)\n",
    "\n",
    "        #same code as before to calculate the negative policy gradient, which is the policy loss\n",
    "        optimizer.zero_grad()\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        #we add entropy bonus to loss and calculate batch entropy\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "        loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Kullback Leibler to test the difference between new and old policy, it state how much one distribution is variance\n",
    "        #to another distribution. If the value is high, it means the policy is much difference from the previous, which is\n",
    "        #bad because weightings are changing seriously and hardly converge.\n",
    "        # calc KL-div\n",
    "        new_logits_v = net(states_v)\n",
    "        new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "        kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "        writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "\n",
    "        #we show the gradient statistics in training step with maximum value and L2-norm(L2 範數) \n",
    "        grad_max = 0.0\n",
    "        grad_means = 0.0\n",
    "        grad_count = 0\n",
    "        for p in net.parameters():\n",
    "            grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "            grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "            grad_count += 1\n",
    "\n",
    "        #save everything to TensorBoard\n",
    "        writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "        writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "        writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "        writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "        writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "        writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "        writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_scales.clear()\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
