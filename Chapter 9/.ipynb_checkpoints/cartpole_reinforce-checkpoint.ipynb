{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68384b02",
   "metadata": {},
   "source": [
    "# Reinforce method is based on cross entropy, but using q-distribution and scaling of gradient with how good the reward is:\n",
    "1. initialize weighting randomly\n",
    "2. Play N full episodes, and store the (s, a, r, s') transfer.\n",
    "3. In episode k for every step t, calculate discounted total reward:\n",
    "   Qk,t = Σi=0 (GAMMA^i )*ri\n",
    "4. Calculate all transfer loss function:\n",
    "   L = -Σk,t Qk,t log(π(sk,t, ak,t))\n",
    "5. Use SGD renew to the weight to reduce loss.\n",
    "6. Repeat step 2 until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d698c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aff70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "#episode to train will state how many episodes is used for training\n",
    "EPISODES_TO_TRAIN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3e1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    #we don't call softmax function, instead, we use Pytorch log_softmax function to calculate the value, the calculation\n",
    "    #will be more stable, also, network output is not probabilities, it is logits, which is a fraction.\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "#this function accept full episode reward and calculate discounted reward on each step.\n",
    "#we get the partial reward at last step reward, which is rt-1 + GAMMA*rt, where rt is last index, sum_r will get the\n",
    "#previous step total reward, so to get the total reward at last step, we need sum_r * GAMMA + partial reward\n",
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e83ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "59: reward:  58.00, mean_100:  58.00, episodes: 1\n",
      "89: reward:  29.00, mean_100:  43.50, episodes: 2\n",
      "100: reward:  10.00, mean_100:  32.33, episodes: 3\n",
      "134: reward:  33.00, mean_100:  32.50, episodes: 4\n",
      "151: reward:  16.00, mean_100:  29.20, episodes: 5\n",
      "185: reward:  33.00, mean_100:  29.83, episodes: 6\n",
      "221: reward:  35.00, mean_100:  30.57, episodes: 7\n",
      "250: reward:  28.00, mean_100:  30.25, episodes: 8\n",
      "269: reward:  18.00, mean_100:  28.89, episodes: 9\n",
      "341: reward:  71.00, mean_100:  33.10, episodes: 10\n",
      "382: reward:  40.00, mean_100:  33.73, episodes: 11\n",
      "410: reward:  27.00, mean_100:  33.17, episodes: 12\n",
      "442: reward:  31.00, mean_100:  33.00, episodes: 13\n",
      "470: reward:  27.00, mean_100:  32.57, episodes: 14\n",
      "493: reward:  22.00, mean_100:  31.87, episodes: 15\n",
      "514: reward:  20.00, mean_100:  31.12, episodes: 16\n",
      "550: reward:  35.00, mean_100:  31.35, episodes: 17\n",
      "601: reward:  50.00, mean_100:  32.39, episodes: 18\n",
      "692: reward:  90.00, mean_100:  35.42, episodes: 19\n",
      "728: reward:  35.00, mean_100:  35.40, episodes: 20\n",
      "746: reward:  17.00, mean_100:  34.52, episodes: 21\n",
      "817: reward:  70.00, mean_100:  36.14, episodes: 22\n",
      "842: reward:  24.00, mean_100:  35.61, episodes: 23\n",
      "871: reward:  28.00, mean_100:  35.29, episodes: 24\n",
      "932: reward:  60.00, mean_100:  36.28, episodes: 25\n",
      "956: reward:  23.00, mean_100:  35.77, episodes: 26\n",
      "979: reward:  22.00, mean_100:  35.26, episodes: 27\n",
      "1045: reward:  65.00, mean_100:  36.32, episodes: 28\n",
      "1075: reward:  29.00, mean_100:  36.07, episodes: 29\n",
      "1103: reward:  27.00, mean_100:  35.77, episodes: 30\n",
      "1139: reward:  35.00, mean_100:  35.74, episodes: 31\n",
      "1195: reward:  55.00, mean_100:  36.34, episodes: 32\n",
      "1241: reward:  45.00, mean_100:  36.61, episodes: 33\n",
      "1296: reward:  54.00, mean_100:  37.12, episodes: 34\n",
      "1338: reward:  41.00, mean_100:  37.23, episodes: 35\n",
      "1399: reward:  60.00, mean_100:  37.86, episodes: 36\n",
      "1462: reward:  62.00, mean_100:  38.51, episodes: 37\n",
      "1529: reward:  66.00, mean_100:  39.24, episodes: 38\n",
      "1576: reward:  46.00, mean_100:  39.41, episodes: 39\n",
      "1601: reward:  24.00, mean_100:  39.02, episodes: 40\n",
      "1624: reward:  22.00, mean_100:  38.61, episodes: 41\n",
      "1688: reward:  63.00, mean_100:  39.19, episodes: 42\n",
      "1748: reward:  59.00, mean_100:  39.65, episodes: 43\n",
      "1781: reward:  32.00, mean_100:  39.48, episodes: 44\n",
      "1876: reward:  94.00, mean_100:  40.69, episodes: 45\n",
      "1931: reward:  54.00, mean_100:  40.98, episodes: 46\n",
      "1977: reward:  45.00, mean_100:  41.06, episodes: 47\n",
      "2034: reward:  56.00, mean_100:  41.38, episodes: 48\n",
      "2086: reward:  51.00, mean_100:  41.57, episodes: 49\n",
      "2182: reward:  95.00, mean_100:  42.64, episodes: 50\n",
      "2230: reward:  47.00, mean_100:  42.73, episodes: 51\n",
      "2258: reward:  27.00, mean_100:  42.42, episodes: 52\n",
      "2343: reward:  84.00, mean_100:  43.21, episodes: 53\n",
      "2416: reward:  72.00, mean_100:  43.74, episodes: 54\n",
      "2554: reward: 137.00, mean_100:  45.44, episodes: 55\n",
      "2590: reward:  35.00, mean_100:  45.25, episodes: 56\n",
      "2629: reward:  38.00, mean_100:  45.12, episodes: 57\n",
      "2657: reward:  27.00, mean_100:  44.81, episodes: 58\n",
      "2733: reward:  75.00, mean_100:  45.32, episodes: 59\n",
      "2780: reward:  46.00, mean_100:  45.33, episodes: 60\n",
      "2890: reward: 109.00, mean_100:  46.38, episodes: 61\n",
      "2935: reward:  44.00, mean_100:  46.34, episodes: 62\n",
      "3014: reward:  78.00, mean_100:  46.84, episodes: 63\n",
      "3046: reward:  31.00, mean_100:  46.59, episodes: 64\n",
      "3092: reward:  45.00, mean_100:  46.57, episodes: 65\n",
      "3131: reward:  38.00, mean_100:  46.44, episodes: 66\n",
      "3212: reward:  80.00, mean_100:  46.94, episodes: 67\n",
      "3278: reward:  65.00, mean_100:  47.21, episodes: 68\n",
      "3370: reward:  91.00, mean_100:  47.84, episodes: 69\n",
      "3491: reward: 120.00, mean_100:  48.87, episodes: 70\n",
      "3568: reward:  76.00, mean_100:  49.25, episodes: 71\n",
      "3711: reward: 142.00, mean_100:  50.54, episodes: 72\n",
      "3747: reward:  35.00, mean_100:  50.33, episodes: 73\n",
      "3800: reward:  52.00, mean_100:  50.35, episodes: 74\n",
      "3859: reward:  58.00, mean_100:  50.45, episodes: 75\n",
      "3989: reward: 129.00, mean_100:  51.49, episodes: 76\n",
      "4093: reward: 103.00, mean_100:  52.16, episodes: 77\n",
      "4169: reward:  75.00, mean_100:  52.45, episodes: 78\n",
      "4203: reward:  33.00, mean_100:  52.20, episodes: 79\n",
      "4348: reward: 144.00, mean_100:  53.35, episodes: 80\n",
      "4410: reward:  61.00, mean_100:  53.44, episodes: 81\n",
      "4467: reward:  56.00, mean_100:  53.48, episodes: 82\n",
      "4508: reward:  40.00, mean_100:  53.31, episodes: 83\n",
      "4569: reward:  60.00, mean_100:  53.39, episodes: 84\n",
      "4596: reward:  26.00, mean_100:  53.07, episodes: 85\n",
      "4653: reward:  56.00, mean_100:  53.10, episodes: 86\n",
      "4713: reward:  59.00, mean_100:  53.17, episodes: 87\n",
      "4750: reward:  36.00, mean_100:  52.98, episodes: 88\n",
      "4791: reward:  40.00, mean_100:  52.83, episodes: 89\n",
      "4868: reward:  76.00, mean_100:  53.09, episodes: 90\n",
      "4890: reward:  21.00, mean_100:  52.74, episodes: 91\n",
      "4940: reward:  49.00, mean_100:  52.70, episodes: 92\n",
      "5021: reward:  80.00, mean_100:  52.99, episodes: 93\n",
      "5102: reward:  80.00, mean_100:  53.28, episodes: 94\n",
      "5136: reward:  33.00, mean_100:  53.06, episodes: 95\n",
      "5171: reward:  34.00, mean_100:  52.86, episodes: 96\n",
      "5216: reward:  44.00, mean_100:  52.77, episodes: 97\n",
      "5250: reward:  33.00, mean_100:  52.57, episodes: 98\n",
      "5292: reward:  41.00, mean_100:  52.45, episodes: 99\n",
      "5364: reward:  71.00, mean_100:  52.64, episodes: 100\n",
      "5403: reward:  38.00, mean_100:  52.44, episodes: 101\n",
      "5459: reward:  55.00, mean_100:  52.70, episodes: 102\n",
      "5532: reward:  72.00, mean_100:  53.32, episodes: 103\n",
      "5563: reward:  30.00, mean_100:  53.29, episodes: 104\n",
      "5656: reward:  92.00, mean_100:  54.05, episodes: 105\n",
      "5741: reward:  84.00, mean_100:  54.56, episodes: 106\n",
      "5824: reward:  82.00, mean_100:  55.03, episodes: 107\n",
      "5874: reward:  49.00, mean_100:  55.24, episodes: 108\n",
      "6057: reward: 182.00, mean_100:  56.88, episodes: 109\n",
      "6098: reward:  40.00, mean_100:  56.57, episodes: 110\n",
      "6138: reward:  39.00, mean_100:  56.56, episodes: 111\n",
      "6229: reward:  90.00, mean_100:  57.19, episodes: 112\n",
      "6336: reward: 106.00, mean_100:  57.94, episodes: 113\n",
      "6430: reward:  93.00, mean_100:  58.60, episodes: 114\n",
      "6572: reward: 141.00, mean_100:  59.79, episodes: 115\n",
      "6728: reward: 155.00, mean_100:  61.14, episodes: 116\n",
      "6800: reward:  71.00, mean_100:  61.50, episodes: 117\n",
      "7001: reward: 200.00, mean_100:  63.00, episodes: 118\n",
      "7202: reward: 200.00, mean_100:  64.10, episodes: 119\n",
      "7359: reward: 156.00, mean_100:  65.31, episodes: 120\n",
      "7560: reward: 200.00, mean_100:  67.14, episodes: 121\n",
      "7761: reward: 200.00, mean_100:  68.44, episodes: 122\n",
      "7962: reward: 200.00, mean_100:  70.20, episodes: 123\n",
      "8163: reward: 200.00, mean_100:  71.92, episodes: 124\n",
      "8343: reward: 179.00, mean_100:  73.11, episodes: 125\n",
      "8496: reward: 152.00, mean_100:  74.40, episodes: 126\n",
      "8617: reward: 120.00, mean_100:  75.38, episodes: 127\n",
      "8818: reward: 200.00, mean_100:  76.73, episodes: 128\n",
      "9014: reward: 195.00, mean_100:  78.39, episodes: 129\n",
      "9156: reward: 141.00, mean_100:  79.53, episodes: 130\n",
      "9259: reward: 102.00, mean_100:  80.20, episodes: 131\n",
      "9460: reward: 200.00, mean_100:  81.65, episodes: 132\n",
      "9591: reward: 130.00, mean_100:  82.50, episodes: 133\n",
      "9734: reward: 142.00, mean_100:  83.38, episodes: 134\n",
      "9786: reward:  51.00, mean_100:  83.48, episodes: 135\n",
      "9883: reward:  96.00, mean_100:  83.84, episodes: 136\n",
      "9927: reward:  43.00, mean_100:  83.65, episodes: 137\n",
      "10049: reward: 121.00, mean_100:  84.20, episodes: 138\n",
      "10073: reward:  23.00, mean_100:  83.97, episodes: 139\n",
      "10199: reward: 125.00, mean_100:  84.98, episodes: 140\n",
      "10254: reward:  54.00, mean_100:  85.30, episodes: 141\n",
      "10301: reward:  46.00, mean_100:  85.13, episodes: 142\n",
      "10328: reward:  26.00, mean_100:  84.80, episodes: 143\n",
      "10378: reward:  49.00, mean_100:  84.97, episodes: 144\n",
      "10478: reward:  99.00, mean_100:  85.02, episodes: 145\n",
      "10503: reward:  24.00, mean_100:  84.72, episodes: 146\n",
      "10530: reward:  26.00, mean_100:  84.53, episodes: 147\n",
      "10651: reward: 120.00, mean_100:  85.17, episodes: 148\n",
      "10780: reward: 128.00, mean_100:  85.94, episodes: 149\n",
      "10889: reward: 108.00, mean_100:  86.07, episodes: 150\n",
      "10963: reward:  73.00, mean_100:  86.33, episodes: 151\n",
      "11023: reward:  59.00, mean_100:  86.65, episodes: 152\n",
      "11078: reward:  54.00, mean_100:  86.35, episodes: 153\n",
      "11158: reward:  79.00, mean_100:  86.42, episodes: 154\n",
      "11186: reward:  27.00, mean_100:  85.32, episodes: 155\n",
      "11251: reward:  64.00, mean_100:  85.61, episodes: 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11373: reward: 121.00, mean_100:  86.44, episodes: 157\n",
      "11484: reward: 110.00, mean_100:  87.27, episodes: 158\n",
      "11520: reward:  35.00, mean_100:  86.87, episodes: 159\n",
      "11640: reward: 119.00, mean_100:  87.60, episodes: 160\n",
      "11747: reward: 106.00, mean_100:  87.57, episodes: 161\n",
      "11771: reward:  23.00, mean_100:  87.36, episodes: 162\n",
      "11897: reward: 125.00, mean_100:  87.83, episodes: 163\n",
      "12036: reward: 138.00, mean_100:  88.90, episodes: 164\n",
      "12163: reward: 126.00, mean_100:  89.71, episodes: 165\n",
      "12329: reward: 165.00, mean_100:  90.98, episodes: 166\n",
      "12440: reward: 110.00, mean_100:  91.28, episodes: 167\n",
      "12465: reward:  24.00, mean_100:  90.87, episodes: 168\n",
      "12631: reward: 165.00, mean_100:  91.61, episodes: 169\n",
      "12775: reward: 143.00, mean_100:  91.84, episodes: 170\n",
      "12824: reward:  48.00, mean_100:  91.56, episodes: 171\n",
      "12973: reward: 148.00, mean_100:  91.62, episodes: 172\n",
      "13104: reward: 130.00, mean_100:  92.57, episodes: 173\n",
      "13296: reward: 191.00, mean_100:  93.96, episodes: 174\n",
      "13424: reward: 127.00, mean_100:  94.65, episodes: 175\n",
      "13555: reward: 130.00, mean_100:  94.66, episodes: 176\n",
      "13656: reward: 100.00, mean_100:  94.63, episodes: 177\n",
      "13784: reward: 127.00, mean_100:  95.15, episodes: 178\n",
      "13904: reward: 119.00, mean_100:  96.01, episodes: 179\n",
      "14052: reward: 147.00, mean_100:  96.04, episodes: 180\n",
      "14204: reward: 151.00, mean_100:  96.94, episodes: 181\n",
      "14314: reward: 109.00, mean_100:  97.47, episodes: 182\n",
      "14482: reward: 167.00, mean_100:  98.74, episodes: 183\n",
      "14597: reward: 114.00, mean_100:  99.28, episodes: 184\n",
      "14730: reward: 132.00, mean_100: 100.34, episodes: 185\n",
      "14843: reward: 112.00, mean_100: 100.90, episodes: 186\n",
      "14947: reward: 103.00, mean_100: 101.34, episodes: 187\n",
      "15077: reward: 129.00, mean_100: 102.27, episodes: 188\n",
      "15178: reward: 100.00, mean_100: 102.87, episodes: 189\n",
      "15279: reward: 100.00, mean_100: 103.11, episodes: 190\n",
      "15390: reward: 110.00, mean_100: 104.00, episodes: 191\n",
      "15480: reward:  89.00, mean_100: 104.40, episodes: 192\n",
      "15604: reward: 123.00, mean_100: 104.83, episodes: 193\n",
      "15756: reward: 151.00, mean_100: 105.54, episodes: 194\n",
      "15883: reward: 126.00, mean_100: 106.47, episodes: 195\n",
      "15908: reward:  24.00, mean_100: 106.37, episodes: 196\n",
      "16027: reward: 118.00, mean_100: 107.11, episodes: 197\n",
      "16135: reward: 107.00, mean_100: 107.85, episodes: 198\n",
      "16250: reward: 114.00, mean_100: 108.58, episodes: 199\n",
      "16371: reward: 120.00, mean_100: 109.07, episodes: 200\n",
      "16480: reward: 108.00, mean_100: 109.77, episodes: 201\n",
      "16598: reward: 117.00, mean_100: 110.39, episodes: 202\n",
      "16772: reward: 173.00, mean_100: 111.40, episodes: 203\n",
      "16883: reward: 110.00, mean_100: 112.20, episodes: 204\n",
      "16985: reward: 101.00, mean_100: 112.29, episodes: 205\n",
      "17110: reward: 124.00, mean_100: 112.69, episodes: 206\n",
      "17265: reward: 154.00, mean_100: 113.41, episodes: 207\n",
      "17376: reward: 110.00, mean_100: 114.02, episodes: 208\n",
      "17505: reward: 128.00, mean_100: 113.48, episodes: 209\n",
      "17627: reward: 121.00, mean_100: 114.29, episodes: 210\n",
      "17736: reward: 108.00, mean_100: 114.98, episodes: 211\n",
      "17892: reward: 155.00, mean_100: 115.63, episodes: 212\n",
      "18076: reward: 183.00, mean_100: 116.40, episodes: 213\n",
      "18197: reward: 120.00, mean_100: 116.67, episodes: 214\n",
      "18351: reward: 153.00, mean_100: 116.79, episodes: 215\n",
      "18473: reward: 121.00, mean_100: 116.45, episodes: 216\n",
      "18610: reward: 136.00, mean_100: 117.10, episodes: 217\n",
      "18780: reward: 169.00, mean_100: 116.79, episodes: 218\n",
      "18981: reward: 200.00, mean_100: 116.79, episodes: 219\n",
      "19161: reward: 179.00, mean_100: 117.02, episodes: 220\n",
      "19320: reward: 158.00, mean_100: 116.60, episodes: 221\n",
      "19504: reward: 183.00, mean_100: 116.43, episodes: 222\n",
      "19705: reward: 200.00, mean_100: 116.43, episodes: 223\n",
      "19844: reward: 138.00, mean_100: 115.81, episodes: 224\n",
      "20045: reward: 200.00, mean_100: 116.02, episodes: 225\n",
      "20246: reward: 200.00, mean_100: 116.50, episodes: 226\n",
      "20420: reward: 173.00, mean_100: 117.03, episodes: 227\n",
      "20590: reward: 169.00, mean_100: 116.72, episodes: 228\n",
      "20791: reward: 200.00, mean_100: 116.77, episodes: 229\n",
      "20992: reward: 200.00, mean_100: 117.36, episodes: 230\n",
      "21193: reward: 200.00, mean_100: 118.34, episodes: 231\n",
      "21394: reward: 200.00, mean_100: 118.34, episodes: 232\n",
      "21595: reward: 200.00, mean_100: 119.04, episodes: 233\n",
      "21796: reward: 200.00, mean_100: 119.62, episodes: 234\n",
      "21997: reward: 200.00, mean_100: 121.11, episodes: 235\n",
      "22198: reward: 200.00, mean_100: 122.15, episodes: 236\n",
      "22399: reward: 200.00, mean_100: 123.72, episodes: 237\n",
      "22600: reward: 200.00, mean_100: 124.51, episodes: 238\n",
      "22801: reward: 200.00, mean_100: 126.28, episodes: 239\n",
      "23002: reward: 200.00, mean_100: 127.03, episodes: 240\n",
      "23203: reward: 200.00, mean_100: 128.49, episodes: 241\n",
      "23404: reward: 200.00, mean_100: 130.03, episodes: 242\n",
      "23605: reward: 200.00, mean_100: 131.77, episodes: 243\n",
      "23806: reward: 200.00, mean_100: 133.28, episodes: 244\n",
      "24007: reward: 200.00, mean_100: 134.29, episodes: 245\n",
      "24208: reward: 200.00, mean_100: 136.05, episodes: 246\n",
      "24409: reward: 200.00, mean_100: 137.79, episodes: 247\n",
      "24610: reward: 200.00, mean_100: 138.59, episodes: 248\n",
      "24811: reward: 200.00, mean_100: 139.31, episodes: 249\n",
      "25012: reward: 200.00, mean_100: 140.23, episodes: 250\n",
      "25213: reward: 200.00, mean_100: 141.50, episodes: 251\n",
      "25414: reward: 200.00, mean_100: 142.91, episodes: 252\n",
      "25615: reward: 200.00, mean_100: 144.37, episodes: 253\n",
      "25816: reward: 200.00, mean_100: 145.58, episodes: 254\n",
      "26017: reward: 200.00, mean_100: 147.31, episodes: 255\n",
      "26218: reward: 200.00, mean_100: 148.67, episodes: 256\n",
      "26419: reward: 200.00, mean_100: 149.46, episodes: 257\n",
      "26620: reward: 200.00, mean_100: 150.36, episodes: 258\n",
      "26821: reward: 200.00, mean_100: 152.01, episodes: 259\n",
      "27022: reward: 200.00, mean_100: 152.82, episodes: 260\n",
      "27223: reward: 200.00, mean_100: 153.76, episodes: 261\n",
      "27424: reward: 200.00, mean_100: 155.53, episodes: 262\n",
      "27625: reward: 200.00, mean_100: 156.28, episodes: 263\n",
      "27826: reward: 200.00, mean_100: 156.90, episodes: 264\n",
      "28027: reward: 200.00, mean_100: 157.64, episodes: 265\n",
      "28228: reward: 200.00, mean_100: 157.99, episodes: 266\n",
      "28429: reward: 200.00, mean_100: 158.89, episodes: 267\n",
      "28630: reward: 200.00, mean_100: 160.65, episodes: 268\n",
      "28831: reward: 200.00, mean_100: 161.00, episodes: 269\n",
      "29032: reward: 200.00, mean_100: 161.57, episodes: 270\n",
      "29233: reward: 200.00, mean_100: 163.09, episodes: 271\n",
      "29434: reward: 200.00, mean_100: 163.61, episodes: 272\n",
      "29635: reward: 200.00, mean_100: 164.31, episodes: 273\n",
      "29836: reward: 200.00, mean_100: 164.40, episodes: 274\n",
      "30037: reward: 200.00, mean_100: 165.13, episodes: 275\n",
      "30238: reward: 200.00, mean_100: 165.83, episodes: 276\n",
      "30439: reward: 200.00, mean_100: 166.83, episodes: 277\n",
      "30640: reward: 200.00, mean_100: 167.56, episodes: 278\n",
      "30841: reward: 200.00, mean_100: 168.37, episodes: 279\n",
      "31042: reward: 200.00, mean_100: 168.90, episodes: 280\n",
      "31243: reward: 200.00, mean_100: 169.39, episodes: 281\n",
      "31444: reward: 200.00, mean_100: 170.30, episodes: 282\n",
      "31645: reward: 200.00, mean_100: 170.63, episodes: 283\n",
      "31846: reward: 200.00, mean_100: 171.49, episodes: 284\n",
      "32047: reward: 200.00, mean_100: 172.17, episodes: 285\n",
      "32248: reward: 200.00, mean_100: 173.05, episodes: 286\n",
      "32449: reward: 200.00, mean_100: 174.02, episodes: 287\n",
      "32650: reward: 200.00, mean_100: 174.73, episodes: 288\n",
      "32851: reward: 200.00, mean_100: 175.73, episodes: 289\n",
      "33052: reward: 200.00, mean_100: 176.73, episodes: 290\n",
      "33253: reward: 200.00, mean_100: 177.63, episodes: 291\n",
      "33454: reward: 200.00, mean_100: 178.74, episodes: 292\n",
      "33655: reward: 200.00, mean_100: 179.51, episodes: 293\n",
      "33856: reward: 200.00, mean_100: 180.00, episodes: 294\n",
      "34057: reward: 200.00, mean_100: 180.74, episodes: 295\n",
      "34258: reward: 200.00, mean_100: 182.50, episodes: 296\n",
      "34459: reward: 200.00, mean_100: 183.32, episodes: 297\n",
      "34660: reward: 200.00, mean_100: 184.25, episodes: 298\n",
      "34861: reward: 200.00, mean_100: 185.11, episodes: 299\n",
      "35062: reward: 200.00, mean_100: 185.91, episodes: 300\n",
      "35263: reward: 200.00, mean_100: 186.83, episodes: 301\n",
      "35464: reward: 200.00, mean_100: 187.66, episodes: 302\n",
      "35665: reward: 200.00, mean_100: 187.93, episodes: 303\n",
      "35866: reward: 200.00, mean_100: 188.83, episodes: 304\n",
      "36067: reward: 200.00, mean_100: 189.82, episodes: 305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36268: reward: 200.00, mean_100: 190.58, episodes: 306\n",
      "36469: reward: 200.00, mean_100: 191.04, episodes: 307\n",
      "36670: reward: 200.00, mean_100: 191.94, episodes: 308\n",
      "36871: reward: 200.00, mean_100: 192.66, episodes: 309\n",
      "37072: reward: 200.00, mean_100: 193.45, episodes: 310\n",
      "37273: reward: 200.00, mean_100: 194.37, episodes: 311\n",
      "37474: reward: 200.00, mean_100: 194.82, episodes: 312\n",
      "37675: reward: 200.00, mean_100: 194.99, episodes: 313\n",
      "37876: reward: 200.00, mean_100: 195.79, episodes: 314\n",
      "Solved in 37876 steps and 314 episodes!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "    \n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "    \n",
    "    #if using q-value and 1st action is 0.4 and 2nd action is 0.5, we have 100% to choose 2nd action, but if using\n",
    "    #probability distribution, we have 40% to choose 1st action and 50% to choose 2nd action, we can also set it to 100%\n",
    "    #using 2nd network by set it to 1. We use random.choice in numpy to call it, and apply softmax to transfer output as\n",
    "    #probabilities, then we transfer the Cartpole float64 result to pytorch float32.\n",
    "    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    #collect data for writer, total rewards, finished episodes, current episode reward\n",
    "    #when the episode end, we use calc_qvals to calculate current reward to calculated discounted total reward and add to \n",
    "    #batch_qvals, batch_states and batch_actions included the states and action we see from last training.\n",
    "    total_rewards = []\n",
    "    done_episodes = 0\n",
    "    \n",
    "    batch_episodes = 0\n",
    "    cur_rewards = []\n",
    "    batch_states, batch_actions, batch_qvals = [], [], []\n",
    "    \n",
    "    #we get state, action, current reward and next state. If episode end, next state is None. If not episode end,\n",
    "    # we store state, action and current reward. When episode end, we change current reward to q value and add episode\n",
    "    # counter\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        cur_rewards.append(exp.reward)\n",
    "        \n",
    "        if exp.last_state is None:\n",
    "            batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "            cur_rewards.clear()\n",
    "            batch_episodes += 1\n",
    "        \n",
    "        #when episode end, it record progress and write to TensorBoard\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\"\\\n",
    "                  %(step_idx, reward, mean_rewards, done_episodes))\n",
    "            writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" %(step_idx, done_episodes))\n",
    "                break\n",
    "                \n",
    "        if batch_episodes < EPISODES_TO_TRAIN:\n",
    "            continue\n",
    "        \n",
    "        #optimize after getting enough samples, change state, action, q-values to pytorch format and type\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "        \n",
    "        #calculate loss. Change the state to logits, calculate logarithm + softmax, we choose the log probabilities and \n",
    "        #use q-values for resize, then take the mean for the resized value and set as negative to reduce loss.\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "        loss_v = -log_prob_actions_v.mean()\n",
    "        \n",
    "        #back propagate to get the gradient, and use optimizer for SGD. When the loop end, we reset batch states, actions\n",
    "        #qvals for collecting new data.\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_episodes = 0\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_qvals.clear()\n",
    "        \n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
