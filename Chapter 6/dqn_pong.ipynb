{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f037f60",
   "metadata": {},
   "source": [
    "1. Use random epsilon and replay buffer to initialize Q(s,a) on online network and Q'(s, a) on target network.\n",
    "2. Use epsilon to choose a random action a, otherwise a = argmax_a Q(s, a)\n",
    "3. Play one step on online network with action a and check the reward r and the next state s'.\n",
    "4. Store transition data (s, a, r, s') to replay buffer.\n",
    "5. Sample one random batch from replay buffer.\n",
    "6. For each transition calculation y, if episode end at this step, then y = r, otherwise y = r + GAMMA * max_a' Q'(s', a').\n",
    "7. Calculate loss: L = ( Q(s, a) - y )^2 , Q(s, a) is the online network and y is the target network.\n",
    "8. With the smallest loss L, use the Stochastic Gradient Descent Algorithm to update Q(s, a).\n",
    "9. For every N steps, update weighting from Q to Q'.\n",
    "10. Repeat step 2 until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739c88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import wrappers\n",
    "from lib import dqn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a56b3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bd1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "#discount factor\n",
    "GAMMA = 0.99\n",
    "#batch size to get from replay buffer\n",
    "BATCH_SIZE = 32\n",
    "#largest replay buffer size\n",
    "REPLAY_SIZE = 10000\n",
    "#Wait the frames number below before start\n",
    "REPLAY_START_SIZE = 10000\n",
    "#Adam learning rate\n",
    "LEARNING_RATE = 1e-4\n",
    "#How long we sync the online model to the target model\n",
    "SYNC_TARGET_FRAMES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon start at 1, and after 100000 frames, epsilon will decreased to 0.02, it means action has 2% randomness\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16237e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define experience buffer at below, we will append the result to buffer after each step, and only record\n",
    "#certain number of steps, here we use 10000 steps, also we will random sample a batch to make it independent from previous\n",
    "#steps\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    #we repack it in numpy array for calculation convenience\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "#Agent interact with environment and store result in replay buffer\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "    \n",
    "    #Agent will act one step in environment and store in buffer, it first choose the action by epsilon with either random\n",
    "    #or best action with maximum q value\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "            \n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        new_state = new_state\n",
    "        \n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "    \n",
    "#we use parallel processing to calculate batch loss with GPU, it will faster than normal version at least 2 times\n",
    "#for the non-ending step, loss: L = (Q(s,a)- (r + GAMMA * max_action Q(s',a')))^2\n",
    "#for ending step, L = (Q(s,a) - r)^2\n",
    "\n",
    "#we send the batch as array to the function, it is sample() from the experience buffer, the online and target network\n",
    "#also included, we use detach to prevent gradient go to target network\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    #we pack batch data as numpy array, if parameters need CUDA device, we add to GPU.\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions, dtype=np.long).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    #we put the observation to the model and use gather to get the q-value. First parameterm is the parameter position\n",
    "    #we want to operate, 1 correspond to action parameter, unsqueeze will insert a new dimension,here at final position,\n",
    "    #the result is the action taken\n",
    "    #gather result is differentiable, it record the last loss gradient\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    #we apply next state observation to target network and calculate the largest q-value for action dimension(1).\n",
    "    #max() will return the largest value and the index at the same time, which is max and argmax, we use the value here\n",
    "    #only, therefore we get array[0]\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "\n",
    "    #for the last step q-values, we set it as 0.0 for convergence because there are no next step to collect reward\n",
    "    #action value won't have next state discounted reward. If we don't set this, it won't converge.\n",
    "    next_state_values[done_mask] = 0.0\n",
    "\n",
    "    #we detach the value from calculation map to avoid back propagation will let current state and next state affect\n",
    "    #together\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    #we calculate Bellman approximation and Mean Square Loss here\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2d899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "762: done 1 games, mean reward -21.000, eps 0.99, speed 313.00 f/s\n",
      "1602: done 2 games, mean reward -20.500, eps 0.98, speed 303.73 f/s\n",
      "Best mean reward updated -21.000 -> -20.500, model saved\n",
      "2578: done 3 games, mean reward -20.000, eps 0.97, speed 299.36 f/s\n",
      "Best mean reward updated -20.500 -> -20.000, model saved\n",
      "3466: done 4 games, mean reward -20.250, eps 0.97, speed 263.24 f/s\n",
      "4486: done 5 games, mean reward -20.000, eps 0.96, speed 285.28 f/s\n",
      "5368: done 6 games, mean reward -20.167, eps 0.95, speed 295.68 f/s\n",
      "6158: done 7 games, mean reward -20.286, eps 0.94, speed 270.05 f/s\n",
      "7038: done 8 games, mean reward -20.375, eps 0.93, speed 264.89 f/s\n",
      "7961: done 9 games, mean reward -20.333, eps 0.92, speed 272.52 f/s\n",
      "8886: done 10 games, mean reward -20.400, eps 0.91, speed 251.21 f/s\n",
      "9696: done 11 games, mean reward -20.455, eps 0.90, speed 236.67 f/s\n",
      "10574: done 12 games, mean reward -20.417, eps 0.89, speed 14.78 f/s\n",
      "11443: done 13 games, mean reward -20.385, eps 0.89, speed 9.73 f/s\n",
      "12239: done 14 games, mean reward -20.429, eps 0.88, speed 9.43 f/s\n",
      "13001: done 15 games, mean reward -20.467, eps 0.87, speed 9.32 f/s\n",
      "13823: done 16 games, mean reward -20.500, eps 0.86, speed 9.29 f/s\n",
      "14733: done 17 games, mean reward -20.529, eps 0.85, speed 9.39 f/s\n",
      "15617: done 18 games, mean reward -20.556, eps 0.84, speed 9.36 f/s\n",
      "16379: done 19 games, mean reward -20.579, eps 0.84, speed 9.39 f/s\n",
      "17249: done 20 games, mean reward -20.600, eps 0.83, speed 9.15 f/s\n",
      "18276: done 21 games, mean reward -20.524, eps 0.82, speed 9.12 f/s\n",
      "19284: done 22 games, mean reward -20.455, eps 0.81, speed 9.03 f/s\n",
      "20046: done 23 games, mean reward -20.478, eps 0.80, speed 9.05 f/s\n",
      "20947: done 24 games, mean reward -20.458, eps 0.79, speed 9.05 f/s\n",
      "21797: done 25 games, mean reward -20.480, eps 0.78, speed 8.85 f/s\n",
      "22559: done 26 games, mean reward -20.500, eps 0.77, speed 9.17 f/s\n",
      "23349: done 27 games, mean reward -20.519, eps 0.77, speed 9.10 f/s\n",
      "24373: done 28 games, mean reward -20.464, eps 0.76, speed 9.12 f/s\n",
      "25213: done 29 games, mean reward -20.448, eps 0.75, speed 9.11 f/s\n",
      "26218: done 30 games, mean reward -20.467, eps 0.74, speed 9.01 f/s\n",
      "27101: done 31 games, mean reward -20.484, eps 0.73, speed 8.99 f/s\n",
      "27909: done 32 games, mean reward -20.500, eps 0.72, speed 9.00 f/s\n",
      "28731: done 33 games, mean reward -20.515, eps 0.71, speed 8.95 f/s\n",
      "29599: done 34 games, mean reward -20.500, eps 0.70, speed 8.84 f/s\n",
      "30640: done 35 games, mean reward -20.486, eps 0.69, speed 8.66 f/s\n",
      "31524: done 36 games, mean reward -20.500, eps 0.68, speed 8.61 f/s\n",
      "32376: done 37 games, mean reward -20.514, eps 0.68, speed 8.81 f/s\n",
      "33194: done 38 games, mean reward -20.526, eps 0.67, speed 8.87 f/s\n",
      "34328: done 39 games, mean reward -20.462, eps 0.66, speed 8.84 f/s\n",
      "35168: done 40 games, mean reward -20.450, eps 0.65, speed 8.83 f/s\n",
      "36167: done 41 games, mean reward -20.439, eps 0.64, speed 8.82 f/s\n",
      "36991: done 42 games, mean reward -20.452, eps 0.63, speed 8.79 f/s\n",
      "38000: done 43 games, mean reward -20.442, eps 0.62, speed 8.73 f/s\n",
      "38910: done 44 games, mean reward -20.455, eps 0.61, speed 8.76 f/s\n",
      "39672: done 45 games, mean reward -20.467, eps 0.60, speed 8.75 f/s\n",
      "40494: done 46 games, mean reward -20.478, eps 0.60, speed 8.54 f/s\n",
      "41337: done 47 games, mean reward -20.489, eps 0.59, speed 8.39 f/s\n",
      "42392: done 48 games, mean reward -20.500, eps 0.58, speed 8.32 f/s\n",
      "43466: done 49 games, mean reward -20.469, eps 0.57, speed 8.36 f/s\n",
      "44733: done 50 games, mean reward -20.440, eps 0.55, speed 8.33 f/s\n",
      "45740: done 51 games, mean reward -20.431, eps 0.54, speed 8.30 f/s\n",
      "46530: done 52 games, mean reward -20.442, eps 0.53, speed 8.22 f/s\n",
      "47350: done 53 games, mean reward -20.453, eps 0.53, speed 8.38 f/s\n",
      "48324: done 54 games, mean reward -20.444, eps 0.52, speed 7.98 f/s\n",
      "49277: done 55 games, mean reward -20.436, eps 0.51, speed 7.46 f/s\n",
      "50238: done 56 games, mean reward -20.446, eps 0.50, speed 8.21 f/s\n",
      "51239: done 57 games, mean reward -20.456, eps 0.49, speed 8.09 f/s\n",
      "52211: done 58 games, mean reward -20.466, eps 0.48, speed 8.09 f/s\n",
      "53235: done 59 games, mean reward -20.441, eps 0.47, speed 8.17 f/s\n",
      "54141: done 60 games, mean reward -20.450, eps 0.46, speed 8.15 f/s\n",
      "55318: done 61 games, mean reward -20.443, eps 0.45, speed 8.09 f/s\n",
      "56196: done 62 games, mean reward -20.452, eps 0.44, speed 8.08 f/s\n",
      "57155: done 63 games, mean reward -20.460, eps 0.43, speed 8.04 f/s\n",
      "58212: done 64 games, mean reward -20.453, eps 0.42, speed 7.97 f/s\n",
      "59366: done 65 games, mean reward -20.462, eps 0.41, speed 7.92 f/s\n",
      "60831: done 66 games, mean reward -20.439, eps 0.39, speed 8.01 f/s\n",
      "62205: done 67 games, mean reward -20.433, eps 0.38, speed 7.91 f/s\n",
      "63613: done 68 games, mean reward -20.397, eps 0.36, speed 7.96 f/s\n",
      "65303: done 69 games, mean reward -20.377, eps 0.35, speed 7.88 f/s\n",
      "66792: done 70 games, mean reward -20.371, eps 0.33, speed 7.91 f/s\n",
      "68595: done 71 games, mean reward -20.352, eps 0.31, speed 7.90 f/s\n",
      "70202: done 72 games, mean reward -20.361, eps 0.30, speed 7.77 f/s\n",
      "71802: done 73 games, mean reward -20.342, eps 0.28, speed 7.75 f/s\n",
      "73387: done 74 games, mean reward -20.324, eps 0.27, speed 7.69 f/s\n",
      "75303: done 75 games, mean reward -20.307, eps 0.25, speed 7.62 f/s\n",
      "77245: done 76 games, mean reward -20.276, eps 0.23, speed 7.59 f/s\n",
      "79260: done 77 games, mean reward -20.247, eps 0.21, speed 7.54 f/s\n",
      "80832: done 78 games, mean reward -20.244, eps 0.19, speed 7.54 f/s\n",
      "82384: done 79 games, mean reward -20.253, eps 0.18, speed 7.57 f/s\n",
      "83479: done 80 games, mean reward -20.262, eps 0.17, speed 7.45 f/s\n",
      "85779: done 81 games, mean reward -20.235, eps 0.14, speed 7.49 f/s\n",
      "88650: done 82 games, mean reward -20.171, eps 0.11, speed 7.48 f/s\n",
      "89998: done 83 games, mean reward -20.181, eps 0.10, speed 7.50 f/s\n",
      "92056: done 84 games, mean reward -20.155, eps 0.08, speed 7.54 f/s\n",
      "93706: done 85 games, mean reward -20.153, eps 0.06, speed 7.48 f/s\n",
      "95410: done 86 games, mean reward -20.140, eps 0.05, speed 7.52 f/s\n",
      "97408: done 87 games, mean reward -20.115, eps 0.03, speed 7.69 f/s\n",
      "100062: done 88 games, mean reward -20.034, eps 0.02, speed 7.77 f/s\n",
      "102538: done 89 games, mean reward -20.000, eps 0.02, speed 8.08 f/s\n",
      "104903: done 90 games, mean reward -19.956, eps 0.02, speed 8.10 f/s\n",
      "Best mean reward updated -20.000 -> -19.956, model saved\n",
      "107666: done 91 games, mean reward -19.879, eps 0.02, speed 8.15 f/s\n",
      "Best mean reward updated -19.956 -> -19.879, model saved\n",
      "110739: done 92 games, mean reward -19.815, eps 0.02, speed 7.97 f/s\n",
      "Best mean reward updated -19.879 -> -19.815, model saved\n",
      "113828: done 93 games, mean reward -19.720, eps 0.02, speed 7.93 f/s\n",
      "Best mean reward updated -19.815 -> -19.720, model saved\n",
      "117361: done 94 games, mean reward -19.585, eps 0.02, speed 7.78 f/s\n",
      "Best mean reward updated -19.720 -> -19.585, model saved\n",
      "120449: done 95 games, mean reward -19.516, eps 0.02, speed 7.78 f/s\n",
      "Best mean reward updated -19.585 -> -19.516, model saved\n",
      "123162: done 96 games, mean reward -19.448, eps 0.02, speed 7.72 f/s\n",
      "Best mean reward updated -19.516 -> -19.448, model saved\n",
      "126775: done 97 games, mean reward -19.289, eps 0.02, speed 7.80 f/s\n",
      "Best mean reward updated -19.448 -> -19.289, model saved\n",
      "129898: done 98 games, mean reward -19.173, eps 0.02, speed 7.90 f/s\n",
      "Best mean reward updated -19.289 -> -19.173, model saved\n",
      "133137: done 99 games, mean reward -18.939, eps 0.02, speed 7.66 f/s\n",
      "Best mean reward updated -19.173 -> -18.939, model saved\n",
      "135768: done 100 games, mean reward -18.620, eps 0.02, speed 7.76 f/s\n",
      "Best mean reward updated -18.939 -> -18.620, model saved\n",
      "138447: done 101 games, mean reward -18.310, eps 0.02, speed 7.66 f/s\n",
      "Best mean reward updated -18.620 -> -18.310, model saved\n",
      "142288: done 102 games, mean reward -18.100, eps 0.02, speed 7.77 f/s\n",
      "Best mean reward updated -18.310 -> -18.100, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144743: done 103 games, mean reward -17.800, eps 0.02, speed 7.87 f/s\n",
      "Best mean reward updated -18.100 -> -17.800, model saved\n",
      "146838: done 104 games, mean reward -17.460, eps 0.02, speed 8.04 f/s\n",
      "Best mean reward updated -17.800 -> -17.460, model saved\n",
      "148659: done 105 games, mean reward -17.090, eps 0.02, speed 8.10 f/s\n",
      "Best mean reward updated -17.460 -> -17.090, model saved\n",
      "150794: done 106 games, mean reward -16.720, eps 0.02, speed 8.24 f/s\n",
      "Best mean reward updated -17.090 -> -16.720, model saved\n",
      "153079: done 107 games, mean reward -16.380, eps 0.02, speed 8.21 f/s\n",
      "Best mean reward updated -16.720 -> -16.380, model saved\n",
      "154802: done 108 games, mean reward -15.980, eps 0.02, speed 8.19 f/s\n",
      "Best mean reward updated -16.380 -> -15.980, model saved\n",
      "156497: done 109 games, mean reward -15.580, eps 0.02, speed 8.23 f/s\n",
      "Best mean reward updated -15.980 -> -15.580, model saved\n",
      "159949: done 110 games, mean reward -15.340, eps 0.02, speed 8.23 f/s\n",
      "Best mean reward updated -15.580 -> -15.340, model saved\n",
      "162167: done 111 games, mean reward -14.970, eps 0.02, speed 8.22 f/s\n",
      "Best mean reward updated -15.340 -> -14.970, model saved\n",
      "163832: done 112 games, mean reward -14.570, eps 0.02, speed 8.23 f/s\n",
      "Best mean reward updated -14.970 -> -14.570, model saved\n",
      "165712: done 113 games, mean reward -14.180, eps 0.02, speed 8.01 f/s\n",
      "Best mean reward updated -14.570 -> -14.180, model saved\n",
      "168249: done 114 games, mean reward -13.870, eps 0.02, speed 0.08 f/s\n",
      "Best mean reward updated -14.180 -> -13.870, model saved\n",
      "170521: done 115 games, mean reward -13.540, eps 0.02, speed 8.25 f/s\n",
      "Best mean reward updated -13.870 -> -13.540, model saved\n",
      "173030: done 116 games, mean reward -13.230, eps 0.02, speed 8.10 f/s\n",
      "Best mean reward updated -13.540 -> -13.230, model saved\n",
      "176005: done 117 games, mean reward -12.950, eps 0.02, speed 8.28 f/s\n",
      "Best mean reward updated -13.230 -> -12.950, model saved\n",
      "178520: done 118 games, mean reward -12.630, eps 0.02, speed 8.25 f/s\n",
      "Best mean reward updated -12.950 -> -12.630, model saved\n",
      "181004: done 119 games, mean reward -12.300, eps 0.02, speed 8.09 f/s\n",
      "Best mean reward updated -12.630 -> -12.300, model saved\n",
      "183602: done 120 games, mean reward -12.010, eps 0.02, speed 8.13 f/s\n",
      "Best mean reward updated -12.300 -> -12.010, model saved\n",
      "185770: done 121 games, mean reward -11.690, eps 0.02, speed 8.05 f/s\n",
      "Best mean reward updated -12.010 -> -11.690, model saved\n",
      "187983: done 122 games, mean reward -11.350, eps 0.02, speed 7.84 f/s\n",
      "Best mean reward updated -11.690 -> -11.350, model saved\n",
      "189898: done 123 games, mean reward -10.960, eps 0.02, speed 7.83 f/s\n",
      "Best mean reward updated -11.350 -> -10.960, model saved\n",
      "191875: done 124 games, mean reward -10.580, eps 0.02, speed 7.83 f/s\n",
      "Best mean reward updated -10.960 -> -10.580, model saved\n",
      "193813: done 125 games, mean reward -10.190, eps 0.02, speed 7.85 f/s\n",
      "Best mean reward updated -10.580 -> -10.190, model saved\n",
      "196452: done 126 games, mean reward -9.880, eps 0.02, speed 7.83 f/s\n",
      "Best mean reward updated -10.190 -> -9.880, model saved\n",
      "198629: done 127 games, mean reward -9.510, eps 0.02, speed 7.85 f/s\n",
      "Best mean reward updated -9.880 -> -9.510, model saved\n",
      "200394: done 128 games, mean reward -9.120, eps 0.02, speed 7.88 f/s\n",
      "Best mean reward updated -9.510 -> -9.120, model saved\n",
      "203493: done 129 games, mean reward -8.890, eps 0.02, speed 7.82 f/s\n",
      "Best mean reward updated -9.120 -> -8.890, model saved\n",
      "205853: done 130 games, mean reward -8.570, eps 0.02, speed 7.83 f/s\n",
      "Best mean reward updated -8.890 -> -8.570, model saved\n",
      "208233: done 131 games, mean reward -8.260, eps 0.02, speed 7.88 f/s\n",
      "Best mean reward updated -8.570 -> -8.260, model saved\n",
      "210045: done 132 games, mean reward -7.870, eps 0.02, speed 8.08 f/s\n",
      "Best mean reward updated -8.260 -> -7.870, model saved\n",
      "211976: done 133 games, mean reward -7.500, eps 0.02, speed 8.02 f/s\n",
      "Best mean reward updated -7.870 -> -7.500, model saved\n",
      "214095: done 134 games, mean reward -7.150, eps 0.02, speed 7.85 f/s\n",
      "Best mean reward updated -7.500 -> -7.150, model saved\n",
      "215816: done 135 games, mean reward -6.750, eps 0.02, speed 7.84 f/s\n",
      "Best mean reward updated -7.150 -> -6.750, model saved\n",
      "217722: done 136 games, mean reward -6.350, eps 0.02, speed 8.03 f/s\n",
      "Best mean reward updated -6.750 -> -6.350, model saved\n",
      "219818: done 137 games, mean reward -6.020, eps 0.02, speed 8.24 f/s\n",
      "Best mean reward updated -6.350 -> -6.020, model saved\n",
      "221485: done 138 games, mean reward -5.610, eps 0.02, speed 7.86 f/s\n",
      "Best mean reward updated -6.020 -> -5.610, model saved\n",
      "223264: done 139 games, mean reward -5.230, eps 0.02, speed 7.88 f/s\n",
      "Best mean reward updated -5.610 -> -5.230, model saved\n",
      "225064: done 140 games, mean reward -4.830, eps 0.02, speed 7.84 f/s\n",
      "Best mean reward updated -5.230 -> -4.830, model saved\n",
      "227661: done 141 games, mean reward -4.550, eps 0.02, speed 7.84 f/s\n",
      "Best mean reward updated -4.830 -> -4.550, model saved\n",
      "230172: done 142 games, mean reward -4.280, eps 0.02, speed 7.87 f/s\n",
      "Best mean reward updated -4.550 -> -4.280, model saved\n",
      "232071: done 143 games, mean reward -3.920, eps 0.02, speed 7.87 f/s\n",
      "Best mean reward updated -4.280 -> -3.920, model saved\n",
      "234145: done 144 games, mean reward -3.550, eps 0.02, speed 7.85 f/s\n",
      "Best mean reward updated -3.920 -> -3.550, model saved\n",
      "236350: done 145 games, mean reward -3.200, eps 0.02, speed 7.91 f/s\n",
      "Best mean reward updated -3.550 -> -3.200, model saved\n",
      "238108: done 146 games, mean reward -2.800, eps 0.02, speed 7.91 f/s\n",
      "Best mean reward updated -3.200 -> -2.800, model saved\n",
      "239774: done 147 games, mean reward -2.390, eps 0.02, speed 8.03 f/s\n",
      "Best mean reward updated -2.800 -> -2.390, model saved\n",
      "241517: done 148 games, mean reward -1.980, eps 0.02, speed 7.90 f/s\n",
      "Best mean reward updated -2.390 -> -1.980, model saved\n",
      "243402: done 149 games, mean reward -1.610, eps 0.02, speed 7.93 f/s\n",
      "Best mean reward updated -1.980 -> -1.610, model saved\n",
      "245191: done 150 games, mean reward -1.230, eps 0.02, speed 7.80 f/s\n",
      "Best mean reward updated -1.610 -> -1.230, model saved\n",
      "246887: done 151 games, mean reward -0.830, eps 0.02, speed 7.91 f/s\n",
      "Best mean reward updated -1.230 -> -0.830, model saved\n",
      "248726: done 152 games, mean reward -0.430, eps 0.02, speed 8.19 f/s\n",
      "Best mean reward updated -0.830 -> -0.430, model saved\n",
      "250496: done 153 games, mean reward -0.030, eps 0.02, speed 8.12 f/s\n",
      "Best mean reward updated -0.430 -> -0.030, model saved\n",
      "252275: done 154 games, mean reward 0.370, eps 0.02, speed 7.92 f/s\n",
      "Best mean reward updated -0.030 -> 0.370, model saved\n",
      "254212: done 155 games, mean reward 0.720, eps 0.02, speed 7.90 f/s\n",
      "Best mean reward updated 0.370 -> 0.720, model saved\n",
      "255990: done 156 games, mean reward 1.110, eps 0.02, speed 7.88 f/s\n",
      "Best mean reward updated 0.720 -> 1.110, model saved\n",
      "257990: done 157 games, mean reward 1.500, eps 0.02, speed 7.92 f/s\n",
      "Best mean reward updated 1.110 -> 1.500, model saved\n",
      "260262: done 158 games, mean reward 1.880, eps 0.02, speed 7.95 f/s\n",
      "Best mean reward updated 1.500 -> 1.880, model saved\n",
      "262598: done 159 games, mean reward 2.220, eps 0.02, speed 7.95 f/s\n",
      "Best mean reward updated 1.880 -> 2.220, model saved\n",
      "265157: done 160 games, mean reward 2.550, eps 0.02, speed 7.96 f/s\n",
      "Best mean reward updated 2.220 -> 2.550, model saved\n",
      "267453: done 161 games, mean reward 2.830, eps 0.02, speed 7.97 f/s\n",
      "Best mean reward updated 2.550 -> 2.830, model saved\n",
      "269663: done 162 games, mean reward 3.200, eps 0.02, speed 8.05 f/s\n",
      "Best mean reward updated 2.830 -> 3.200, model saved\n",
      "271650: done 163 games, mean reward 3.600, eps 0.02, speed 8.37 f/s\n",
      "Best mean reward updated 3.200 -> 3.600, model saved\n",
      "273981: done 164 games, mean reward 3.950, eps 0.02, speed 8.01 f/s\n",
      "Best mean reward updated 3.600 -> 3.950, model saved\n",
      "275637: done 165 games, mean reward 4.370, eps 0.02, speed 7.97 f/s\n",
      "Best mean reward updated 3.950 -> 4.370, model saved\n",
      "277394: done 166 games, mean reward 4.750, eps 0.02, speed 7.97 f/s\n",
      "Best mean reward updated 4.370 -> 4.750, model saved\n",
      "279081: done 167 games, mean reward 5.150, eps 0.02, speed 7.99 f/s\n",
      "Best mean reward updated 4.750 -> 5.150, model saved\n",
      "281054: done 168 games, mean reward 5.490, eps 0.02, speed 7.94 f/s\n",
      "Best mean reward updated 5.150 -> 5.490, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282876: done 169 games, mean reward 5.860, eps 0.02, speed 7.97 f/s\n",
      "Best mean reward updated 5.490 -> 5.860, model saved\n",
      "284724: done 170 games, mean reward 6.250, eps 0.02, speed 7.96 f/s\n",
      "Best mean reward updated 5.860 -> 6.250, model saved\n",
      "286636: done 171 games, mean reward 6.600, eps 0.02, speed 7.95 f/s\n",
      "Best mean reward updated 6.250 -> 6.600, model saved\n",
      "288901: done 172 games, mean reward 6.930, eps 0.02, speed 8.35 f/s\n",
      "Best mean reward updated 6.600 -> 6.930, model saved\n",
      "290809: done 173 games, mean reward 7.280, eps 0.02, speed 8.05 f/s\n",
      "Best mean reward updated 6.930 -> 7.280, model saved\n",
      "292820: done 174 games, mean reward 7.650, eps 0.02, speed 7.95 f/s\n",
      "Best mean reward updated 7.280 -> 7.650, model saved\n",
      "295107: done 175 games, mean reward 7.970, eps 0.02, speed 7.97 f/s\n",
      "Best mean reward updated 7.650 -> 7.970, model saved\n",
      "296917: done 176 games, mean reward 8.330, eps 0.02, speed 7.98 f/s\n",
      "Best mean reward updated 7.970 -> 8.330, model saved\n",
      "298613: done 177 games, mean reward 8.710, eps 0.02, speed 7.96 f/s\n",
      "Best mean reward updated 8.330 -> 8.710, model saved\n",
      "300386: done 178 games, mean reward 9.120, eps 0.02, speed 7.95 f/s\n",
      "Best mean reward updated 8.710 -> 9.120, model saved\n",
      "302255: done 179 games, mean reward 9.510, eps 0.02, speed 7.98 f/s\n",
      "Best mean reward updated 9.120 -> 9.510, model saved\n",
      "303993: done 180 games, mean reward 9.910, eps 0.02, speed 7.99 f/s\n",
      "Best mean reward updated 9.510 -> 9.910, model saved\n",
      "305740: done 181 games, mean reward 10.290, eps 0.02, speed 7.98 f/s\n",
      "Best mean reward updated 9.910 -> 10.290, model saved\n",
      "307623: done 182 games, mean reward 10.630, eps 0.02, speed 7.93 f/s\n",
      "Best mean reward updated 10.290 -> 10.630, model saved\n",
      "309462: done 183 games, mean reward 11.030, eps 0.02, speed 7.91 f/s\n",
      "Best mean reward updated 10.630 -> 11.030, model saved\n",
      "311166: done 184 games, mean reward 11.410, eps 0.02, speed 7.81 f/s\n",
      "Best mean reward updated 11.030 -> 11.410, model saved\n",
      "313279: done 185 games, mean reward 11.780, eps 0.02, speed 7.90 f/s\n",
      "Best mean reward updated 11.410 -> 11.780, model saved\n",
      "315606: done 186 games, mean reward 12.100, eps 0.02, speed 7.95 f/s\n",
      "Best mean reward updated 11.780 -> 12.100, model saved\n",
      "317655: done 187 games, mean reward 12.430, eps 0.02, speed 8.28 f/s\n",
      "Best mean reward updated 12.100 -> 12.430, model saved\n",
      "319374: done 188 games, mean reward 12.760, eps 0.02, speed 8.01 f/s\n",
      "Best mean reward updated 12.430 -> 12.760, model saved\n",
      "321367: done 189 games, mean reward 13.120, eps 0.02, speed 8.01 f/s\n",
      "Best mean reward updated 12.760 -> 13.120, model saved\n",
      "323172: done 190 games, mean reward 13.460, eps 0.02, speed 8.00 f/s\n",
      "Best mean reward updated 13.120 -> 13.460, model saved\n",
      "325326: done 191 games, mean reward 13.700, eps 0.02, speed 8.02 f/s\n",
      "Best mean reward updated 13.460 -> 13.700, model saved\n",
      "327413: done 192 games, mean reward 13.990, eps 0.02, speed 8.23 f/s\n",
      "Best mean reward updated 13.700 -> 13.990, model saved\n",
      "329682: done 193 games, mean reward 14.200, eps 0.02, speed 8.27 f/s\n",
      "Best mean reward updated 13.990 -> 14.200, model saved\n",
      "331566: done 194 games, mean reward 14.450, eps 0.02, speed 8.32 f/s\n",
      "Best mean reward updated 14.200 -> 14.450, model saved\n",
      "333237: done 195 games, mean reward 14.780, eps 0.02, speed 8.09 f/s\n",
      "Best mean reward updated 14.450 -> 14.780, model saved\n",
      "334968: done 196 games, mean reward 15.110, eps 0.02, speed 8.00 f/s\n",
      "Best mean reward updated 14.780 -> 15.110, model saved\n",
      "336846: done 197 games, mean reward 15.350, eps 0.02, speed 8.01 f/s\n",
      "Best mean reward updated 15.110 -> 15.350, model saved\n",
      "338897: done 198 games, mean reward 15.600, eps 0.02, speed 8.06 f/s\n",
      "Best mean reward updated 15.350 -> 15.600, model saved\n",
      "340816: done 199 games, mean reward 15.730, eps 0.02, speed 8.02 f/s\n",
      "Best mean reward updated 15.600 -> 15.730, model saved\n",
      "343286: done 200 games, mean reward 15.740, eps 0.02, speed 7.97 f/s\n",
      "Best mean reward updated 15.730 -> 15.740, model saved\n",
      "345286: done 201 games, mean reward 15.820, eps 0.02, speed 8.21 f/s\n",
      "Best mean reward updated 15.740 -> 15.820, model saved\n",
      "346943: done 202 games, mean reward 16.020, eps 0.02, speed 8.31 f/s\n",
      "Best mean reward updated 15.820 -> 16.020, model saved\n",
      "348759: done 203 games, mean reward 16.100, eps 0.02, speed 8.32 f/s\n",
      "Best mean reward updated 16.020 -> 16.100, model saved\n",
      "350571: done 204 games, mean reward 16.160, eps 0.02, speed 8.32 f/s\n",
      "Best mean reward updated 16.100 -> 16.160, model saved\n",
      "352241: done 205 games, mean reward 16.180, eps 0.02, speed 8.34 f/s\n",
      "Best mean reward updated 16.160 -> 16.180, model saved\n",
      "354361: done 206 games, mean reward 16.190, eps 0.02, speed 8.40 f/s\n",
      "Best mean reward updated 16.180 -> 16.190, model saved\n",
      "356289: done 207 games, mean reward 16.260, eps 0.02, speed 4.00 f/s\n",
      "Best mean reward updated 16.190 -> 16.260, model saved\n",
      "358020: done 208 games, mean reward 16.270, eps 0.02, speed 8.28 f/s\n",
      "Best mean reward updated 16.260 -> 16.270, model saved\n",
      "360283: done 209 games, mean reward 16.170, eps 0.02, speed 8.13 f/s\n",
      "362205: done 210 games, mean reward 16.310, eps 0.02, speed 7.99 f/s\n",
      "Best mean reward updated 16.270 -> 16.310, model saved\n",
      "364037: done 211 games, mean reward 16.330, eps 0.02, speed 7.80 f/s\n",
      "Best mean reward updated 16.310 -> 16.330, model saved\n",
      "365694: done 212 games, mean reward 16.340, eps 0.02, speed 7.54 f/s\n",
      "Best mean reward updated 16.330 -> 16.340, model saved\n",
      "367365: done 213 games, mean reward 16.350, eps 0.02, speed 7.73 f/s\n",
      "Best mean reward updated 16.340 -> 16.350, model saved\n",
      "369161: done 214 games, mean reward 16.440, eps 0.02, speed 7.79 f/s\n",
      "Best mean reward updated 16.350 -> 16.440, model saved\n",
      "371184: done 215 games, mean reward 16.490, eps 0.02, speed 7.71 f/s\n",
      "Best mean reward updated 16.440 -> 16.490, model saved\n",
      "373320: done 216 games, mean reward 16.340, eps 0.02, speed 7.68 f/s\n",
      "375536: done 217 games, mean reward 16.430, eps 0.02, speed 7.79 f/s\n",
      "377449: done 218 games, mean reward 16.520, eps 0.02, speed 7.74 f/s\n",
      "Best mean reward updated 16.490 -> 16.520, model saved\n",
      "379655: done 219 games, mean reward 16.570, eps 0.02, speed 7.78 f/s\n",
      "Best mean reward updated 16.520 -> 16.570, model saved\n",
      "381482: done 220 games, mean reward 16.680, eps 0.02, speed 7.89 f/s\n",
      "Best mean reward updated 16.570 -> 16.680, model saved\n",
      "383530: done 221 games, mean reward 16.700, eps 0.02, speed 8.02 f/s\n",
      "Best mean reward updated 16.680 -> 16.700, model saved\n",
      "385447: done 222 games, mean reward 16.750, eps 0.02, speed 8.02 f/s\n",
      "Best mean reward updated 16.700 -> 16.750, model saved\n",
      "387803: done 223 games, mean reward 16.710, eps 0.02, speed 8.04 f/s\n",
      "389804: done 224 games, mean reward 16.690, eps 0.02, speed 8.02 f/s\n",
      "391875: done 225 games, mean reward 16.660, eps 0.02, speed 8.32 f/s\n",
      "393769: done 226 games, mean reward 16.740, eps 0.02, speed 8.44 f/s\n",
      "395545: done 227 games, mean reward 16.780, eps 0.02, speed 8.47 f/s\n",
      "Best mean reward updated 16.750 -> 16.780, model saved\n",
      "397542: done 228 games, mean reward 16.740, eps 0.02, speed 8.53 f/s\n",
      "399729: done 229 games, mean reward 16.870, eps 0.02, speed 8.14 f/s\n",
      "Best mean reward updated 16.780 -> 16.870, model saved\n",
      "401580: done 230 games, mean reward 16.950, eps 0.02, speed 8.05 f/s\n",
      "Best mean reward updated 16.870 -> 16.950, model saved\n",
      "403236: done 231 games, mean reward 17.060, eps 0.02, speed 8.10 f/s\n",
      "Best mean reward updated 16.950 -> 17.060, model saved\n",
      "405468: done 232 games, mean reward 17.000, eps 0.02, speed 8.09 f/s\n",
      "407398: done 233 games, mean reward 17.010, eps 0.02, speed 8.10 f/s\n",
      "409497: done 234 games, mean reward 17.040, eps 0.02, speed 8.12 f/s\n",
      "411247: done 235 games, mean reward 17.040, eps 0.02, speed 8.08 f/s\n",
      "412914: done 236 games, mean reward 17.050, eps 0.02, speed 8.07 f/s\n",
      "414875: done 237 games, mean reward 17.100, eps 0.02, speed 8.06 f/s\n",
      "Best mean reward updated 17.060 -> 17.100, model saved\n",
      "416636: done 238 games, mean reward 17.100, eps 0.02, speed 8.10 f/s\n",
      "418303: done 239 games, mean reward 17.100, eps 0.02, speed 8.09 f/s\n",
      "420400: done 240 games, mean reward 17.050, eps 0.02, speed 8.13 f/s\n",
      "422621: done 241 games, mean reward 17.120, eps 0.02, speed 8.12 f/s\n",
      "Best mean reward updated 17.100 -> 17.120, model saved\n",
      "424554: done 242 games, mean reward 17.240, eps 0.02, speed 8.13 f/s\n",
      "Best mean reward updated 17.120 -> 17.240, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426338: done 243 games, mean reward 17.270, eps 0.02, speed 8.11 f/s\n",
      "Best mean reward updated 17.240 -> 17.270, model saved\n",
      "428177: done 244 games, mean reward 17.290, eps 0.02, speed 8.35 f/s\n",
      "Best mean reward updated 17.270 -> 17.290, model saved\n",
      "430120: done 245 games, mean reward 17.310, eps 0.02, speed 8.51 f/s\n",
      "Best mean reward updated 17.290 -> 17.310, model saved\n",
      "432052: done 246 games, mean reward 17.320, eps 0.02, speed 8.16 f/s\n",
      "Best mean reward updated 17.310 -> 17.320, model saved\n",
      "433857: done 247 games, mean reward 17.310, eps 0.02, speed 8.09 f/s\n",
      "435513: done 248 games, mean reward 17.320, eps 0.02, speed 8.07 f/s\n",
      "437290: done 249 games, mean reward 17.350, eps 0.02, speed 8.19 f/s\n",
      "Best mean reward updated 17.320 -> 17.350, model saved\n",
      "438944: done 250 games, mean reward 17.370, eps 0.02, speed 8.12 f/s\n",
      "Best mean reward updated 17.350 -> 17.370, model saved\n",
      "441085: done 251 games, mean reward 17.350, eps 0.02, speed 8.07 f/s\n",
      "442939: done 252 games, mean reward 17.340, eps 0.02, speed 8.10 f/s\n",
      "444939: done 253 games, mean reward 17.320, eps 0.02, speed 7.75 f/s\n",
      "446810: done 254 games, mean reward 17.320, eps 0.02, speed 7.62 f/s\n",
      "448831: done 255 games, mean reward 17.370, eps 0.02, speed 7.83 f/s\n",
      "450683: done 256 games, mean reward 17.370, eps 0.02, speed 8.08 f/s\n",
      "452440: done 257 games, mean reward 17.400, eps 0.02, speed 8.49 f/s\n",
      "Best mean reward updated 17.370 -> 17.400, model saved\n",
      "454108: done 258 games, mean reward 17.430, eps 0.02, speed 8.42 f/s\n",
      "Best mean reward updated 17.400 -> 17.430, model saved\n",
      "456104: done 259 games, mean reward 17.440, eps 0.02, speed 7.75 f/s\n",
      "Best mean reward updated 17.430 -> 17.440, model saved\n",
      "458347: done 260 games, mean reward 17.490, eps 0.02, speed 7.99 f/s\n",
      "Best mean reward updated 17.440 -> 17.490, model saved\n",
      "460342: done 261 games, mean reward 17.580, eps 0.02, speed 8.13 f/s\n",
      "Best mean reward updated 17.490 -> 17.580, model saved\n",
      "462191: done 262 games, mean reward 17.620, eps 0.02, speed 8.11 f/s\n",
      "Best mean reward updated 17.580 -> 17.620, model saved\n",
      "464191: done 263 games, mean reward 17.600, eps 0.02, speed 7.90 f/s\n",
      "466189: done 264 games, mean reward 17.600, eps 0.02, speed 7.92 f/s\n",
      "468093: done 265 games, mean reward 17.570, eps 0.02, speed 7.55 f/s\n",
      "470143: done 266 games, mean reward 17.560, eps 0.02, speed 7.71 f/s\n",
      "471811: done 267 games, mean reward 17.560, eps 0.02, speed 7.75 f/s\n",
      "473809: done 268 games, mean reward 17.580, eps 0.02, speed 8.13 f/s\n",
      "475558: done 269 games, mean reward 17.600, eps 0.02, speed 8.15 f/s\n",
      "477272: done 270 games, mean reward 17.620, eps 0.02, speed 8.17 f/s\n",
      "479348: done 271 games, mean reward 17.630, eps 0.02, speed 8.24 f/s\n",
      "Best mean reward updated 17.620 -> 17.630, model saved\n",
      "481656: done 272 games, mean reward 17.640, eps 0.02, speed 8.47 f/s\n",
      "Best mean reward updated 17.630 -> 17.640, model saved\n",
      "483309: done 273 games, mean reward 17.690, eps 0.02, speed 8.63 f/s\n",
      "Best mean reward updated 17.640 -> 17.690, model saved\n",
      "485116: done 274 games, mean reward 17.700, eps 0.02, speed 8.59 f/s\n",
      "Best mean reward updated 17.690 -> 17.700, model saved\n",
      "487389: done 275 games, mean reward 17.700, eps 0.02, speed 8.66 f/s\n",
      "489042: done 276 games, mean reward 17.730, eps 0.02, speed 8.63 f/s\n",
      "Best mean reward updated 17.700 -> 17.730, model saved\n",
      "490888: done 277 games, mean reward 17.720, eps 0.02, speed 8.59 f/s\n",
      "492698: done 278 games, mean reward 17.710, eps 0.02, speed 8.57 f/s\n",
      "494546: done 279 games, mean reward 17.720, eps 0.02, speed 8.63 f/s\n",
      "496426: done 280 games, mean reward 17.730, eps 0.02, speed 8.69 f/s\n",
      "498189: done 281 games, mean reward 17.720, eps 0.02, speed 8.52 f/s\n",
      "499855: done 282 games, mean reward 17.730, eps 0.02, speed 8.61 f/s\n",
      "501926: done 283 games, mean reward 17.710, eps 0.02, speed 8.54 f/s\n",
      "504038: done 284 games, mean reward 17.680, eps 0.02, speed 8.31 f/s\n",
      "505847: done 285 games, mean reward 17.710, eps 0.02, speed 8.18 f/s\n",
      "507808: done 286 games, mean reward 17.770, eps 0.02, speed 8.16 f/s\n",
      "Best mean reward updated 17.730 -> 17.770, model saved\n",
      "509628: done 287 games, mean reward 17.800, eps 0.02, speed 8.22 f/s\n",
      "Best mean reward updated 17.770 -> 17.800, model saved\n",
      "511822: done 288 games, mean reward 17.750, eps 0.02, speed 8.19 f/s\n",
      "513927: done 289 games, mean reward 17.710, eps 0.02, speed 8.06 f/s\n",
      "515593: done 290 games, mean reward 17.730, eps 0.02, speed 8.17 f/s\n",
      "517601: done 291 games, mean reward 17.810, eps 0.02, speed 8.12 f/s\n",
      "Best mean reward updated 17.800 -> 17.810, model saved\n",
      "519297: done 292 games, mean reward 17.870, eps 0.02, speed 8.07 f/s\n",
      "Best mean reward updated 17.810 -> 17.870, model saved\n",
      "521137: done 293 games, mean reward 17.960, eps 0.02, speed 8.04 f/s\n",
      "Best mean reward updated 17.870 -> 17.960, model saved\n",
      "523015: done 294 games, mean reward 17.950, eps 0.02, speed 8.15 f/s\n",
      "524992: done 295 games, mean reward 17.920, eps 0.02, speed 7.96 f/s\n",
      "526791: done 296 games, mean reward 17.910, eps 0.02, speed 8.11 f/s\n",
      "528621: done 297 games, mean reward 17.890, eps 0.02, speed 8.18 f/s\n",
      "530421: done 298 games, mean reward 17.900, eps 0.02, speed 8.14 f/s\n",
      "532226: done 299 games, mean reward 17.920, eps 0.02, speed 8.35 f/s\n",
      "534018: done 300 games, mean reward 17.980, eps 0.02, speed 8.45 f/s\n",
      "Best mean reward updated 17.960 -> 17.980, model saved\n",
      "536080: done 301 games, mean reward 17.970, eps 0.02, speed 8.10 f/s\n",
      "537809: done 302 games, mean reward 17.960, eps 0.02, speed 8.10 f/s\n",
      "539803: done 303 games, mean reward 17.940, eps 0.02, speed 7.91 f/s\n",
      "541647: done 304 games, mean reward 17.920, eps 0.02, speed 7.78 f/s\n",
      "543456: done 305 games, mean reward 17.920, eps 0.02, speed 7.92 f/s\n",
      "545393: done 306 games, mean reward 17.930, eps 0.02, speed 7.80 f/s\n",
      "547096: done 307 games, mean reward 17.920, eps 0.02, speed 7.87 f/s\n",
      "548838: done 308 games, mean reward 17.920, eps 0.02, speed 7.84 f/s\n",
      "551001: done 309 games, mean reward 17.990, eps 0.02, speed 7.88 f/s\n",
      "Best mean reward updated 17.980 -> 17.990, model saved\n",
      "552887: done 310 games, mean reward 17.990, eps 0.02, speed 7.90 f/s\n",
      "554736: done 311 games, mean reward 17.980, eps 0.02, speed 8.02 f/s\n",
      "556636: done 312 games, mean reward 17.970, eps 0.02, speed 8.01 f/s\n",
      "558503: done 313 games, mean reward 17.950, eps 0.02, speed 8.05 f/s\n",
      "560437: done 314 games, mean reward 17.920, eps 0.02, speed 7.92 f/s\n",
      "562247: done 315 games, mean reward 17.940, eps 0.02, speed 7.95 f/s\n",
      "564399: done 316 games, mean reward 18.170, eps 0.02, speed 8.03 f/s\n",
      "Best mean reward updated 17.990 -> 18.170, model saved\n",
      "566179: done 317 games, mean reward 18.220, eps 0.02, speed 7.93 f/s\n",
      "Best mean reward updated 18.170 -> 18.220, model saved\n",
      "567964: done 318 games, mean reward 18.200, eps 0.02, speed 7.72 f/s\n",
      "570040: done 319 games, mean reward 18.170, eps 0.02, speed 7.62 f/s\n",
      "571911: done 320 games, mean reward 18.160, eps 0.02, speed 7.75 f/s\n",
      "573901: done 321 games, mean reward 18.180, eps 0.02, speed 7.57 f/s\n",
      "575957: done 322 games, mean reward 18.140, eps 0.02, speed 7.58 f/s\n",
      "578578: done 323 games, mean reward 18.090, eps 0.02, speed 7.74 f/s\n",
      "580490: done 324 games, mean reward 18.130, eps 0.02, speed 8.17 f/s\n",
      "582375: done 325 games, mean reward 18.160, eps 0.02, speed 8.41 f/s\n",
      "584310: done 326 games, mean reward 18.140, eps 0.02, speed 8.31 f/s\n",
      "586091: done 327 games, mean reward 18.120, eps 0.02, speed 8.33 f/s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #we create argument parser here and activate cuda environment\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME, help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "    parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
    "                        help=\"Mean reward boundary for stop of training,default=%.2f\" %MEAN_REWARD_BOUND)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    #we construct environment with environment wrappers, and construct online and target network\n",
    "    #although they have random initial weights, but we will synchronize every 1000 frames, which is 1 episode, so it \n",
    "    #is not very important\n",
    "    env = wrappers.make_env(args.env)\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    \n",
    "    #we make replay buffer here, then send it to agent. Epsilon is set to 1 at start, but will decrease much after each\n",
    "    #iteration\n",
    "    writer = SummaryWriter(comment=\"-\" + args.env)\n",
    "    print(net)\n",
    "    \n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "    epsilon = EPSILON_START\n",
    "    \n",
    "    #create optimizer, one episode reward buffer, frame counter, some speed counter and best mean reward recorder.\n",
    "    #we will record the best reward if it get new record.\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None\n",
    "    \n",
    "    while True:\n",
    "        #we will calculate the number of iteration finished and decrease epsilon accordingly, it will decrease in \n",
    "        #EPSILON_DECAY_LAST_FRAME, which is 100000 frames, when it reach epsilon 0.02, it will hold at that value\n",
    "        frame_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "        \n",
    "        #agent will play one step in environment, if we go to the last step, it will return not None and we will record \n",
    "        #the following values:\n",
    "        #current epsilon value\n",
    "        #speed, which is the number of frames processed each second\n",
    "        #last 100 episodes mean reward\n",
    "        #episode number\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" \n",
    "                  %(frame_idx, len(total_rewards), mean_reward, epsilon, speed))\n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        \n",
    "            #if last 100 episodes mean reward reached largest value, we will report and save model\n",
    "            #if mean reward larger than reward boundary, we will stop training. \n",
    "            #Our mean reward bound in pong is 19.5 for 21 games\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), args.env + \"-best.dat\")\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" %(best_mean_reward, mean_reward))\n",
    "                best_mean_reward = mean_reward\n",
    "            if mean_reward > args.reward:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "        \n",
    "        #we will wait until buffer accumulates enough data to start\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "        \n",
    "        #we will synchronize the online network to target network in SYNC_TARGET_FRAMES frames, which is 1000 frames\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "        \n",
    "        #we set gradient to zero, get batch sample from replay buffer, calculate loss and minimize the loss\n",
    "        #this part used the most time\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
