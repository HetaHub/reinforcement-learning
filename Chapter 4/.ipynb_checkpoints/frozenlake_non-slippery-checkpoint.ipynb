{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b98e30",
   "metadata": {},
   "source": [
    "1. Batch must be large enough(In CartPole, we just need 16 episodes is enough, but in FrozenLake, we need at least 100 episode to get some success cases.)\n",
    "2. Use discount factor on reward, we use 0.9 or 0.95, therefore a longer episode will have a smaller reward than a shorter episode.\n",
    "3. Extend the time of keeping elite episode: In CartPole, we sample from episodes and get the elite episode, using those for training and drop them. But in FrozenLake, it is difficult to see a success episode, therefore we must keep them longer for training.\n",
    "4. Decrease learning rate to get more samples\n",
    "5. Longer training time, we need at least 5000 times training for 50% success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153879a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.spaces, gym.wrappers, gym.envs.toy_text.frozen_lake\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54234f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#other parameters are randomize and won't do customization\n",
    "#hidden layers neurons number, number of episodes for every loop\n",
    "#filter percentage for the best episodes(we will take the best 30%)\n",
    "HIDDEN_SIZE = 128\n",
    "#batch size set to 100 instead of 16\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "#Use for calculate discounted reward\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299ec49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8431d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define 2 helper class to create 2 namedtuple.\n",
    "#EpisodeStep store 1 step in a episode, it also stored observation from environment and the action performed\n",
    "#Episode is the set of EpisodeStep, it store the non-discounted reward in one whole episode\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c254f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    #batch to save result of process, set a reward counter for the current episode\n",
    "    #reset environment and construct softmax layer, it transfer the output to action probabilites\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    #every loop will convert observation to pytorch tensor and send to network to get the action probability\n",
    "    #nn.Module will get observation value from the CartPole 1 * 4 Tensor\n",
    "    #Because we didn't use softmax function at final layer, it will output original value\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        #we use tensor.data to uncompress tensor and convert to Numpy array, this has same 2 dimension as input data\n",
    "        #we want to get the first batch from the batch array, therefore we use [0], which is action probabilities\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        \n",
    "        #we use random.choice() to sample from action probabilities, and put the action to environment to get next \n",
    "        #observation, reward and episode is done or not\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        #we add the reward to total reward, we store the observation and action pair into the episode_steps, the \n",
    "        #observation is before action, not after\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        \n",
    "        #when gameover, episode is done, we will append the reward to total reward and reset environment and episode rewards\n",
    "        #if batch accumulates enough episodes,we yield the result to caller for further process\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        \n",
    "        #get observation value from environment to current observation variable\n",
    "        #repeat everything after: pass observation result to network, sample action and take action, let environment\n",
    "        #handle action, save result\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fbec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "#we return elite batch here\n",
    "def filter_batch(batch, percentile):\n",
    "    #we use numpy percentile to caluclate the reward_bound with the batch reward we got\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "    \n",
    "    #if reward is larger than reward boundary, we keep it and store the observation and action\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "            \n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2993dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885ba79",
   "metadata": {},
   "source": [
    "# The reward cannot converge because the environment and reward mechanism is very different, the reward will mark as success(1) or fail(0), and we only have very little chance by random walking will go to the goal. So if we use >50-70% as elite episode for training, probably there are many failed episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff53bd",
   "metadata": {},
   "source": [
    "# To conclude, if using cross entropy, the episode is shorter, the result is better. The total reward should be able to divide the good and bad episode. Also before success, there won't be any indication of whether the target is going to achieve as good episode or bad episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c19fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kelvin\\reinforcement\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.309, reward_mean=0.0, reward_bound=0.0\n",
      "1: loss=1.310, reward_mean=0.0, reward_bound=0.0\n",
      "2: loss=1.309, reward_mean=0.0, reward_bound=0.0\n",
      "3: loss=1.314, reward_mean=0.0, reward_bound=0.0\n",
      "4: loss=1.327, reward_mean=0.0, reward_bound=0.0\n",
      "5: loss=1.322, reward_mean=0.0, reward_bound=0.0\n",
      "6: loss=1.317, reward_mean=0.0, reward_bound=0.0\n",
      "7: loss=1.318, reward_mean=0.0, reward_bound=0.0\n",
      "8: loss=1.316, reward_mean=0.0, reward_bound=0.0\n",
      "9: loss=1.315, reward_mean=0.0, reward_bound=0.0\n",
      "10: loss=1.309, reward_mean=0.0, reward_bound=0.0\n",
      "11: loss=1.303, reward_mean=0.0, reward_bound=0.0\n",
      "12: loss=1.300, reward_mean=0.0, reward_bound=0.0\n",
      "13: loss=1.293, reward_mean=0.0, reward_bound=0.0\n",
      "14: loss=1.288, reward_mean=0.0, reward_bound=0.0\n",
      "15: loss=1.289, reward_mean=0.1, reward_bound=0.0\n",
      "16: loss=1.285, reward_mean=0.0, reward_bound=0.0\n",
      "17: loss=1.281, reward_mean=0.0, reward_bound=0.0\n",
      "18: loss=1.277, reward_mean=0.0, reward_bound=0.0\n",
      "19: loss=1.271, reward_mean=0.0, reward_bound=0.0\n",
      "20: loss=1.269, reward_mean=0.1, reward_bound=0.0\n",
      "21: loss=1.264, reward_mean=0.0, reward_bound=0.0\n",
      "22: loss=1.257, reward_mean=0.0, reward_bound=0.0\n",
      "23: loss=1.253, reward_mean=0.1, reward_bound=0.0\n",
      "24: loss=1.250, reward_mean=0.0, reward_bound=0.0\n",
      "25: loss=1.247, reward_mean=0.1, reward_bound=0.0\n",
      "26: loss=1.244, reward_mean=0.1, reward_bound=0.0\n",
      "27: loss=1.242, reward_mean=0.1, reward_bound=0.0\n",
      "28: loss=1.234, reward_mean=0.1, reward_bound=0.0\n",
      "29: loss=1.230, reward_mean=0.1, reward_bound=0.0\n",
      "30: loss=1.232, reward_mean=0.1, reward_bound=0.0\n",
      "31: loss=1.227, reward_mean=0.1, reward_bound=0.0\n",
      "32: loss=1.233, reward_mean=0.1, reward_bound=0.0\n",
      "33: loss=1.227, reward_mean=0.1, reward_bound=0.0\n",
      "34: loss=1.224, reward_mean=0.1, reward_bound=0.0\n",
      "35: loss=1.219, reward_mean=0.1, reward_bound=0.0\n",
      "36: loss=1.214, reward_mean=0.1, reward_bound=0.0\n",
      "37: loss=1.212, reward_mean=0.1, reward_bound=0.0\n",
      "38: loss=1.207, reward_mean=0.0, reward_bound=0.0\n",
      "39: loss=1.202, reward_mean=0.1, reward_bound=0.0\n",
      "40: loss=1.198, reward_mean=0.1, reward_bound=0.0\n",
      "41: loss=1.194, reward_mean=0.1, reward_bound=0.0\n",
      "42: loss=1.191, reward_mean=0.1, reward_bound=0.0\n",
      "43: loss=1.187, reward_mean=0.1, reward_bound=0.0\n",
      "44: loss=1.182, reward_mean=0.1, reward_bound=0.0\n",
      "45: loss=1.159, reward_mean=0.1, reward_bound=0.1\n",
      "46: loss=1.149, reward_mean=0.1, reward_bound=0.1\n",
      "47: loss=1.134, reward_mean=0.1, reward_bound=0.1\n",
      "48: loss=1.107, reward_mean=0.1, reward_bound=0.2\n",
      "49: loss=1.091, reward_mean=0.2, reward_bound=0.2\n",
      "50: loss=1.085, reward_mean=0.1, reward_bound=0.2\n",
      "51: loss=1.081, reward_mean=0.1, reward_bound=0.2\n",
      "52: loss=1.057, reward_mean=0.1, reward_bound=0.3\n",
      "53: loss=1.043, reward_mean=0.2, reward_bound=0.3\n",
      "54: loss=1.019, reward_mean=0.2, reward_bound=0.3\n",
      "55: loss=1.020, reward_mean=0.1, reward_bound=0.1\n",
      "56: loss=1.011, reward_mean=0.1, reward_bound=0.2\n",
      "57: loss=0.998, reward_mean=0.1, reward_bound=0.3\n",
      "58: loss=0.987, reward_mean=0.1, reward_bound=0.3\n",
      "59: loss=0.949, reward_mean=0.2, reward_bound=0.3\n",
      "60: loss=0.941, reward_mean=0.2, reward_bound=0.3\n",
      "61: loss=0.894, reward_mean=0.2, reward_bound=0.4\n",
      "62: loss=0.914, reward_mean=0.2, reward_bound=0.0\n",
      "63: loss=0.913, reward_mean=0.2, reward_bound=0.1\n",
      "64: loss=0.883, reward_mean=0.2, reward_bound=0.3\n",
      "65: loss=0.867, reward_mean=0.3, reward_bound=0.3\n",
      "66: loss=0.842, reward_mean=0.3, reward_bound=0.4\n",
      "67: loss=0.768, reward_mean=0.2, reward_bound=0.4\n",
      "68: loss=0.802, reward_mean=0.2, reward_bound=0.0\n",
      "69: loss=0.787, reward_mean=0.3, reward_bound=0.3\n",
      "70: loss=0.765, reward_mean=0.3, reward_bound=0.3\n",
      "71: loss=0.737, reward_mean=0.3, reward_bound=0.4\n",
      "72: loss=0.707, reward_mean=0.3, reward_bound=0.4\n",
      "73: loss=0.606, reward_mean=0.3, reward_bound=0.5\n",
      "74: loss=0.630, reward_mean=0.4, reward_bound=0.4\n",
      "75: loss=0.647, reward_mean=0.3, reward_bound=0.3\n",
      "76: loss=0.607, reward_mean=0.4, reward_bound=0.4\n",
      "77: loss=0.601, reward_mean=0.3, reward_bound=0.4\n",
      "78: loss=0.546, reward_mean=0.4, reward_bound=0.5\n",
      "79: loss=0.534, reward_mean=0.4, reward_bound=0.5\n",
      "80: loss=0.528, reward_mean=0.3, reward_bound=0.4\n",
      "81: loss=0.509, reward_mean=0.4, reward_bound=0.5\n",
      "83: loss=0.768, reward_mean=0.5, reward_bound=0.0\n",
      "84: loss=0.758, reward_mean=0.5, reward_bound=0.0\n",
      "85: loss=0.751, reward_mean=0.3, reward_bound=0.0\n",
      "86: loss=0.698, reward_mean=0.4, reward_bound=0.3\n",
      "87: loss=0.640, reward_mean=0.4, reward_bound=0.4\n",
      "88: loss=0.553, reward_mean=0.5, reward_bound=0.4\n",
      "89: loss=0.437, reward_mean=0.5, reward_bound=0.5\n",
      "90: loss=0.456, reward_mean=0.4, reward_bound=0.4\n",
      "91: loss=0.422, reward_mean=0.5, reward_bound=0.5\n",
      "92: loss=0.412, reward_mean=0.4, reward_bound=0.5\n",
      "94: loss=0.688, reward_mean=0.5, reward_bound=0.0\n",
      "95: loss=0.637, reward_mean=0.5, reward_bound=0.0\n",
      "96: loss=0.515, reward_mean=0.6, reward_bound=0.4\n",
      "97: loss=0.464, reward_mean=0.4, reward_bound=0.4\n",
      "98: loss=0.459, reward_mean=0.6, reward_bound=0.5\n",
      "99: loss=0.375, reward_mean=0.5, reward_bound=0.5\n",
      "101: loss=0.623, reward_mean=0.6, reward_bound=0.0\n",
      "102: loss=0.582, reward_mean=0.5, reward_bound=0.0\n",
      "103: loss=0.434, reward_mean=0.6, reward_bound=0.4\n",
      "104: loss=0.350, reward_mean=0.6, reward_bound=0.5\n",
      "105: loss=0.372, reward_mean=0.6, reward_bound=0.5\n",
      "107: loss=0.556, reward_mean=0.7, reward_bound=0.0\n",
      "108: loss=0.491, reward_mean=0.6, reward_bound=0.4\n",
      "109: loss=0.436, reward_mean=0.6, reward_bound=0.4\n",
      "110: loss=0.336, reward_mean=0.7, reward_bound=0.5\n",
      "112: loss=0.521, reward_mean=0.7, reward_bound=0.3\n",
      "113: loss=0.398, reward_mean=0.7, reward_bound=0.4\n",
      "114: loss=0.403, reward_mean=0.7, reward_bound=0.5\n",
      "115: loss=0.303, reward_mean=0.7, reward_bound=0.5\n",
      "117: loss=0.489, reward_mean=0.7, reward_bound=0.4\n",
      "118: loss=0.393, reward_mean=0.7, reward_bound=0.4\n",
      "119: loss=0.296, reward_mean=0.7, reward_bound=0.5\n",
      "121: loss=0.394, reward_mean=0.8, reward_bound=0.4\n",
      "122: loss=0.275, reward_mean=0.7, reward_bound=0.5\n",
      "124: loss=0.380, reward_mean=0.7, reward_bound=0.4\n",
      "125: loss=0.255, reward_mean=0.8, reward_bound=0.5\n",
      "127: loss=0.404, reward_mean=0.7, reward_bound=0.0\n",
      "128: loss=0.252, reward_mean=0.8, reward_bound=0.5\n",
      "130: loss=0.415, reward_mean=0.8, reward_bound=0.4\n",
      "131: loss=0.227, reward_mean=0.7, reward_bound=0.5\n",
      "133: loss=0.307, reward_mean=0.8, reward_bound=0.4\n",
      "134: loss=0.234, reward_mean=0.8, reward_bound=0.5\n",
      "136: loss=0.311, reward_mean=0.8, reward_bound=0.5\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #we create all necessary objects: environment, network, target function, optimizer and TensorBoard writer\n",
    "    ####### We change the environemnt as non-slippery #########\n",
    "    env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "    env = DiscreteOneHotWrapper(env)\n",
    "    #The line below will create a monitor to save the agent action as video\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon4\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    #learning set to 0.001 instead of 0.01, 1/10 of original\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-nonslippery\")\n",
    "    \n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_m = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        full_batch, obs, acts, reward_b = filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #For monitoring, show iterate number, loss, batch reward mean, reward boundary, all these values will be \n",
    "        #written in TensorBoard\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" %(iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        \n",
    "        #if mean reward > 199, we stop training, it is because in Gym, when 100 episodes > 195,\n",
    "        #the cartpole problem is said to be successfully solved, it can balance infinitely long,\n",
    "        #but in CartPole environment, it used TimeLimit to limit the episodes within 200, so it is forced to stop after\n",
    "        #200 steps. Therefore we use > 199 steps as to indicate the problem solved\n",
    "        if reward_m > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
