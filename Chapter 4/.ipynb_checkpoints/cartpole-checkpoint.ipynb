{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a27c03",
   "metadata": {},
   "source": [
    "1. Use our current module and environment to play N episodes\n",
    "2. Calculate the reward for each episode and set the reward boundary, we use the percentile of total reward as boundary, such as 50-70%\n",
    "3. Drop all episodes that has total reward less than reward boundary.\n",
    "4. Use observation as input and train the remaining episodes, and the decided action as output.\n",
    "5. Return to step 1 until we are satisfied with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18045919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9a5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other parameters are randomize and won't do customization\n",
    "#hidden layers neurons number, number of episodes for every loop\n",
    "#filter percentage for the best episodes(we will take the best 30%)\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b94b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299e5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define 2 helper class to create 2 namedtuple.\n",
    "#EpisodeStep store 1 step in a episode, it also stored observation from environment and the action performed\n",
    "#Episode is the set of EpisodeStep, it store the non-discounted reward in one whole episode\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6684f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    #batch to save result of process, set a reward counter for the current episode\n",
    "    #reset environment and construct softmax layer, it transfer the output to action probabilites\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    #every loop will convert observation to pytorch tensor and send to network to get the action probability\n",
    "    #nn.Module will get observation value from the CartPole 1 * 4 Tensor\n",
    "    #Because we didn't use softmax function at final layer, it will output original value\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        #we use tensor.data to uncompress tensor and convert to Numpy array, this has same 2 dimension as input data\n",
    "        #we want to get the first batch from the batch array, therefore we use [0], which is action probabilities\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        \n",
    "        #we use random.choice() to sample from action probabilities, and put the action to environment to get next \n",
    "        #observation, reward and episode is done or not\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        #we add the reward to total reward, we store the observation and action pair into the episode_steps, the \n",
    "        #observation is before action, not after\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        \n",
    "        #when gameover, episode is done, we will append the reward to total reward and reset environment and episode rewards\n",
    "        #if batch accumulates enough episodes,we yield the result to caller for further process\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        \n",
    "        #get observation value from environment to current observation variable\n",
    "        #repeat everything after: pass observation result to network, sample action and take action, let environment\n",
    "        #handle action, save result\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74981b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "def filter_batch(batch, percentile):\n",
    "    #we use numpy percentile to caluclate the reward_bound with the batch reward we got\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    #reward_mean for monitoring\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    \n",
    "    #if reward is larger than reward boundary, we keep it and store the observation and action\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "        \n",
    "    #transform observation and action to vector and put in array, last 2 will just put in TensorBoard for monitoring but \n",
    "    # no real use\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c93420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.698, reward_mean=15.7, reward_bound=16.0\n",
      "1: loss=0.684, reward_mean=20.5, reward_bound=23.0\n",
      "2: loss=0.667, reward_mean=30.4, reward_bound=33.5\n",
      "3: loss=0.652, reward_mean=36.4, reward_bound=44.0\n",
      "4: loss=0.637, reward_mean=34.4, reward_bound=41.0\n",
      "5: loss=0.627, reward_mean=36.9, reward_bound=44.5\n",
      "6: loss=0.629, reward_mean=42.1, reward_bound=52.0\n",
      "7: loss=0.593, reward_mean=44.5, reward_bound=49.0\n",
      "8: loss=0.608, reward_mean=53.8, reward_bound=60.5\n",
      "9: loss=0.589, reward_mean=44.8, reward_bound=48.5\n",
      "10: loss=0.581, reward_mean=57.0, reward_bound=64.5\n",
      "11: loss=0.571, reward_mean=56.6, reward_bound=52.0\n",
      "12: loss=0.552, reward_mean=59.7, reward_bound=71.0\n",
      "13: loss=0.546, reward_mean=64.9, reward_bound=77.0\n",
      "14: loss=0.552, reward_mean=63.1, reward_bound=69.5\n",
      "15: loss=0.551, reward_mean=70.9, reward_bound=86.0\n",
      "16: loss=0.521, reward_mean=70.8, reward_bound=76.0\n",
      "17: loss=0.524, reward_mean=62.1, reward_bound=68.0\n",
      "18: loss=0.533, reward_mean=54.1, reward_bound=58.5\n",
      "19: loss=0.530, reward_mean=59.4, reward_bound=67.5\n",
      "20: loss=0.489, reward_mean=59.4, reward_bound=66.0\n",
      "21: loss=0.499, reward_mean=68.1, reward_bound=72.0\n",
      "22: loss=0.513, reward_mean=58.9, reward_bound=67.0\n",
      "23: loss=0.500, reward_mean=66.4, reward_bound=76.0\n",
      "24: loss=0.523, reward_mean=62.0, reward_bound=66.0\n",
      "25: loss=0.496, reward_mean=68.9, reward_bound=76.0\n",
      "26: loss=0.487, reward_mean=77.3, reward_bound=84.0\n",
      "27: loss=0.473, reward_mean=84.6, reward_bound=81.0\n",
      "28: loss=0.489, reward_mean=69.6, reward_bound=70.0\n",
      "29: loss=0.493, reward_mean=72.6, reward_bound=79.0\n",
      "30: loss=0.484, reward_mean=67.3, reward_bound=69.5\n",
      "31: loss=0.433, reward_mean=65.4, reward_bound=74.5\n",
      "32: loss=0.485, reward_mean=74.2, reward_bound=102.0\n",
      "33: loss=0.495, reward_mean=68.7, reward_bound=71.5\n",
      "34: loss=0.486, reward_mean=74.2, reward_bound=71.0\n",
      "35: loss=0.461, reward_mean=62.5, reward_bound=69.5\n",
      "36: loss=0.433, reward_mean=75.1, reward_bound=71.5\n",
      "37: loss=0.468, reward_mean=87.7, reward_bound=93.0\n",
      "38: loss=0.454, reward_mean=90.9, reward_bound=101.0\n",
      "39: loss=0.454, reward_mean=110.8, reward_bound=132.5\n",
      "40: loss=0.447, reward_mean=104.7, reward_bound=118.5\n",
      "41: loss=0.461, reward_mean=98.9, reward_bound=112.0\n",
      "42: loss=0.461, reward_mean=93.8, reward_bound=112.5\n",
      "43: loss=0.448, reward_mean=111.9, reward_bound=119.5\n",
      "44: loss=0.434, reward_mean=105.6, reward_bound=121.5\n",
      "45: loss=0.441, reward_mean=103.8, reward_bound=115.0\n",
      "46: loss=0.422, reward_mean=101.4, reward_bound=106.0\n",
      "47: loss=0.428, reward_mean=110.7, reward_bound=122.5\n",
      "48: loss=0.422, reward_mean=110.5, reward_bound=111.0\n",
      "49: loss=0.419, reward_mean=112.7, reward_bound=131.5\n",
      "50: loss=0.429, reward_mean=108.8, reward_bound=112.0\n",
      "51: loss=0.423, reward_mean=127.3, reward_bound=139.0\n",
      "52: loss=0.417, reward_mean=119.4, reward_bound=126.5\n",
      "53: loss=0.443, reward_mean=118.4, reward_bound=148.0\n",
      "54: loss=0.417, reward_mean=107.3, reward_bound=116.5\n",
      "55: loss=0.411, reward_mean=120.2, reward_bound=121.5\n",
      "56: loss=0.412, reward_mean=124.6, reward_bound=143.0\n",
      "57: loss=0.421, reward_mean=112.1, reward_bound=125.5\n",
      "58: loss=0.405, reward_mean=113.9, reward_bound=123.5\n",
      "59: loss=0.430, reward_mean=125.0, reward_bound=155.0\n",
      "60: loss=0.429, reward_mean=105.0, reward_bound=114.0\n",
      "61: loss=0.418, reward_mean=132.1, reward_bound=161.5\n",
      "62: loss=0.432, reward_mean=110.0, reward_bound=117.5\n",
      "63: loss=0.402, reward_mean=112.1, reward_bound=127.0\n",
      "64: loss=0.438, reward_mean=112.1, reward_bound=126.0\n",
      "65: loss=0.404, reward_mean=98.6, reward_bound=112.5\n",
      "66: loss=0.414, reward_mean=130.4, reward_bound=147.5\n",
      "67: loss=0.419, reward_mean=118.0, reward_bound=121.5\n",
      "68: loss=0.396, reward_mean=124.1, reward_bound=139.0\n",
      "69: loss=0.425, reward_mean=124.8, reward_bound=146.5\n",
      "70: loss=0.421, reward_mean=140.4, reward_bound=165.5\n",
      "71: loss=0.389, reward_mean=117.9, reward_bound=123.5\n",
      "72: loss=0.399, reward_mean=126.8, reward_bound=134.0\n",
      "73: loss=0.409, reward_mean=138.4, reward_bound=171.5\n",
      "74: loss=0.420, reward_mean=114.9, reward_bound=128.0\n",
      "75: loss=0.403, reward_mean=142.1, reward_bound=176.0\n",
      "76: loss=0.422, reward_mean=133.8, reward_bound=155.0\n",
      "77: loss=0.390, reward_mean=137.9, reward_bound=138.0\n",
      "78: loss=0.403, reward_mean=154.4, reward_bound=179.5\n",
      "79: loss=0.403, reward_mean=147.3, reward_bound=159.5\n",
      "80: loss=0.404, reward_mean=153.8, reward_bound=176.5\n",
      "81: loss=0.413, reward_mean=174.3, reward_bound=200.0\n",
      "82: loss=0.416, reward_mean=176.4, reward_bound=200.0\n",
      "83: loss=0.423, reward_mean=174.4, reward_bound=200.0\n",
      "84: loss=0.415, reward_mean=186.6, reward_bound=200.0\n",
      "85: loss=0.417, reward_mean=186.4, reward_bound=200.0\n",
      "86: loss=0.415, reward_mean=185.9, reward_bound=200.0\n",
      "87: loss=0.412, reward_mean=190.1, reward_bound=200.0\n",
      "88: loss=0.414, reward_mean=196.5, reward_bound=200.0\n",
      "89: loss=0.412, reward_mean=200.0, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #we create all necessary objects: environment, network, target function, optimizer and TensorBoard writer\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #The line below will create a monitor to save the agent action as video\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    #we get the batch in loop, filter the batch, get observation and action vector, reward boundary and mean\n",
    "    #we make the gradient zero and give the observation to network and get the action score\n",
    "    #action score will put to target function to calculate the difference between network output and agent chosen action\n",
    "    #so the agent will choose the action will higher network output value\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #For monitoring, show iterate number, loss, batch reward mean, reward boundary, all these values will be \n",
    "        #written in TensorBoard\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" %(iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        \n",
    "        #if mean reward > 199, we stop training, it is because in Gym, when 100 episodes > 195,\n",
    "        #the cartpole problem is said to be successfully solved, it can balance infinitely long,\n",
    "        #but in CartPole environment, it used TimeLimit to limit the episodes within 200, so it is forced to stop after\n",
    "        #200 steps. Therefore we use > 199 steps as to indicate the problem solved\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
